
[{"content":"","date":"2 January 2026","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"2 January 2026","externalUrl":null,"permalink":"/draft/","section":"Drafts","summary":"","title":"Drafts","type":"draft"},{"content":"","date":"2 January 2026","externalUrl":null,"permalink":"/tags/eli5/","section":"Tags","summary":"","title":"ELI5","type":"tags"},{"content":" This is part 3 of the ELI5 transformers series, however you do not need to read Part 1 in order to follow the article. Link to the series can be found here\nI\u0026rsquo;m pretty sure you\u0026rsquo;ve already heard about something like this before: \u0026lsquo;generative LLM is just slightly advanced auto complete\u0026rsquo;. Or, something like \u0026lsquo;predicting next word given all the previous words\u0026rsquo;. Today I would like to invite you think of text generation from a slightly different perspective: Text generation is sequence classification in a for-loop.\nalways has been Introduction: the blackbox # Let\u0026rsquo;s start with a simple example. Let\u0026rsquo;s say, you want to a model that converts digits like [1, 2, 3,..] into arabic numerals [Ù£ , Ù¢ , Ù¡, \u0026hellip;]:\nInputs 0 1 2 ... Desired outputs Ù  Ù¡ Ù¢ ... This model could be as simple as an if-else check or a lookup-table that gives you specific output when seeing specific input: flowchart LR ipt([\"Input\"]) ipt --\u003e if1[\"Is input 0?\"] if1 --\u003e o1[\"Output: Ù \"] ipt --\u003e if2[\"Is input 1?\"] if2 --\u003e o2[\"Output: Ù¡\"] ipt --\u003e if3[\"...\"] if3 --\u003e o3[\"...\"] classDef default fill: transparent, bg-color: transparent Or, this could be a very complicated LLM that does lots of unnecessary thinking:\nFirst, the user said: \u0026#34;Convert digits to arabic numerals:\u0026#34; and then listed \u0026#34;- 0\u0026#34;, \u0026#34;- 1\u0026#34;, \u0026#34;- 2\u0026#34;. I need to understand what is being asked here. The term \u0026#34;arabic numerals\u0026#34; might be confusing because in common usage, the digits 0, 1, 2 are already Arabic numerals. But perhaps the user is referring to the numerals used in Arabic script, which are different from the Western Arabic numerals we use. In Arabic, numerals are written differently. For example, the Western Arabic numeral \u0026#34;0\u0026#34; is represented as \u0026#34;Ù \u0026#34; in Arabic script, \u0026#34;1\u0026#34; as \u0026#34;Ù¡\u0026#34;, \u0026#34;2\u0026#34; as \u0026#34;Ù¢\u0026#34;, and so on. So, probably, the user wants the conversion from Western digits to Eastern Arabic numerals. Let me confirm: In many Arabic-speaking countries, they use these numerals: Ù , Ù¡, Ù¢, Ù£, Ù¤, Ù¥, Ù¦, Ù§, Ù¨, Ù©. So for the given digits: 0 should be converted to Ù  1 should be converted to Ù¡ 2 should be converted to Ù¢ The user only provided three digits: 0, 1, and 2. So I should only convert these. The instruction is \u0026#34;Convert digits to arabic numerals\u0026#34;, and since it\u0026#39;s listed with hyphens, I should probably respond in a similar format. So, for each digit, I\u0026#39;ll provide the corresponding Arabic numeral. My response should be clear and direct. And finally:\nThe digits 0, 1, and 2 in Western Arabic numerals correspond to the following Eastern Arabic numerals used in Arabic script: 0 â†’ Ù  1 â†’ Ù¡ 2 â†’ Ù¢ AI is neither new nor revolutionary as people might think \u0026ndash; at the end of the day they are nothing more than mathematical formulas. They are complicated, yes, but from a functional perspective, there is no difference between GPT-OSS and \\(y = ax + b\\). We can even write our model as a mathematical formular like this:\n\\(y = F(x)\\)\nWhere \\(x\\) is our input text, and \\(y\\) is the output text.\nIn real life scenarios, we often don\u0026rsquo;t have the formula for our target function \\(F\\), but, if we were only interested in getting an acceptable \\(y\\), then an approximation of \\(F\\) would be decent enough. Think about the digital-arabic converter in our example: both lookup table and the state-of-art AI give us satisfying answer, they provided the same output with the same input. All LLMs, from big names like GPT or Gemini, to domain-specific models like Helium, shares this one key aspect in common: they are set up to produce specfic outputs with specific inputs. Most of the time we aren\u0026rsquo;t interested in how these outputs are produced, we only need to the output to be accurate.\nHere, \u0026lsquo;most of the time\u0026rsquo; applies for both model inference (when running the model) and model training. There is this idea of universal approximation theorem. Suppose we have a target function, say, \\(y = F(x)\\), and we have our transformer \\(y = T(x)\\). With sufficient training and sufficient model size, taking the same input \\(x\\), our transformer \\(T\\) would be able to produce similar output \\(y\\) as the target function \\(F\\). Since we are only interested in function inputs and function outputs, transformers comes very handly for creating AI models that suits our specific to our need: we only need to set up appropriate training data, feed the data to the model, and we would expect model behave in ways we want without doing too much considerations to the model itself.\nThe rest of this post will using text generation as an example to further explore this idea of transformer as a black box. This post will mainly explore:\nHow model generates texts, and text generation resembles sentence classification. How text generation model is trained through parallelism \u0026ndash;\u0026gt; it\u0026rsquo;s such a simple trick but people overcomplicated it so much! ..And how parallelism used for other training tasks, like reinforcement learning. Prep # Setup environment # First, install dependencies and start an interactive Python session. In the terminal, create a virtual environment so the work in this post doesn\u0026rsquo;t affect other projects:\npython -m venv transformers-pt2 If you were using windows, type these in your terminal:\ntransformers-pt2\\Scripts\\activate For mac and linux, run:\nsource transformers-pt2/bin/activate Then install dependencies and start Python:\npip install torch tokenizers -U pip install transformers --pre python All set.\nGetting model and tokenizer # Same as previous posts, we\u0026rsquo;ll be using the mini-version of Qwen3 model for today\u0026rsquo;s demo:\nQwen/Qwen3-0.6B text-generation 802 7.059495e\u0026#43;06 In python, copy and paste these codes to initialise our model\nimport pprint import torch from transformers import AutoModelForCausalLM, AutoTokenizer torch.set_default_device(\u0026#39;cpu\u0026#39;) # or \u0026#39;cuda\u0026#39; if GPU is available device = torch.get_default_device() checkpoint = r\u0026#34;Qwen/Qwen3-0.6B\u0026#34; # link to the model tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device) model.generation_config.max_new_tokens = 256 # to make things runs slightly quicker,, You would see a status bar whilst downloading \u0026amp; loading our model:\n... Loading weights: 100%|â–ˆâ–ˆ| 311/311 [00:00\u0026lt;00:00, 4366.64it/s, Materializing param=model.norm.weight] The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning You can safely ignore the warning messages since we aren\u0026rsquo;t going to modify the model, but you would need to handle these warning messages when it comes to actual model development!\nPreparing example data # We\u0026rsquo;ll be working with an example message for our text generation demo:\nexample = \u0026#34;how are you?\u0026#34; In order for model to perform optimally, input data needs to be formatted similar to what model was originally trained on. For our QWEN3 model, it\u0026rsquo;s this specific chat format:\nmessage = [{\u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: example}] formatted_example = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True, enable_thinking=False) print(formatted_example) Our formatted input texts looks like this:\n... \u0026lt;|im_start|\u0026gt;assistant \u0026lt;think\u0026gt; \u0026lt;/think\u0026gt; Models from transformers library do not accepts raw texts as inputs, only integers of token indicies. We need to convert our formatted texts to tokens before passing to our model:\ntokens = tokenizer(formatted_example, return_tensors=\u0026#34;pt\u0026#34;) # or via tokenizer.apply_chat_template with tokenize=True: tokens = tokenizer.apply_chat_template( message, add_generation_prompt=True, enable_thinking=False, tokenize=True, return_dict=True, return_tensors=\u0026#34;pt\u0026#34;, ) print(tokens) # {\u0026#39;input_ids\u0026#39;: tensor([[151644, 77091, 198, 151667, 271, 151668, 271]]), \u0026#39;attention_mask\u0026#39;: tensor([[1, 1, 1, 1, 1, 1, 1]])} Let\u0026rsquo;s test with text generation:\n\u0026gt;\u0026gt;\u0026gt; result = model.generate(**tokens) ... generated = result[0, tokens[\u0026#39;input_ids\u0026#39;].shape[1]:] # skip input tokens ... print(tokenizer.decode(generated)) ... Hello! I\u0026#39;m here to help. How can I assist you today?\u0026lt;|im_end|\u0026gt; Text generation, one token at a time # We\u0026rsquo;ll be doing lots of model output decoding, so here\u0026rsquo;s a function to quickly decode model outputs and print out the decoded text:\nfrom functools import partial from transformers import PreTrainedTokenizer def parse_output(tokenizer:PreTrainedTokenizer, logits:torch.Tensor) -\u0026gt; None: top1 = logits.topk(1, -1)[-1].view(-1) # get indicies for the highest scores decoded_messages = tokenizer.batch_decode(top1) for message in decoded_messages: print(message, end=\u0026#34;\\n======\\n\u0026#34;) decode = partial(parse_output, tokenizer) Understand model outputs # Before starting off, first we\u0026rsquo;ll have a look at what model outputs:\noutput = model(**tokens) pprint.pprint(output, ) Outputs from our model contains a lot of things, as you can see here:\nCausalLMOutputWithPast( loss=None, logits=tensor([[[ 3.5938, 3.7812, 3.6562, ..., 1.6484, 1.6484, 1.6484], [ 7.2812, 8.2500, 6.3125, ..., 0.4141, 0.4141, 0.4141], [ 4.7188, 9.6250, 11.5625, ..., 3.8281, 3.8281, 3.8281], ..., [ 5.7812, 14.7500, 9.5000, ..., 1.4531, 1.4531, 1.4531], [-4.2500, -3.8750, -7.4688, ..., -1.1719, -1.1719, -1.1719], [14.1250, 12.9375, 7.6875, ..., 4.3750, 4.3750, 4.3750]]], dtype=torch.bfloat16, grad_fn=\u0026lt;UnsafeViewBackward0\u0026gt;), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None ) Here\u0026rsquo;s a quick explaination:\nname explain loss loss of model performance, used in model training logits the main outputs of our model, this is the main focus of current post past_key_values caching, this is used to speed up text generation hidden_states raw output of the base model (more detail), returned when output_hidden_states = True attentions raw output of each attention layer, returned when output_attentions = True ","date":"2 January 2026","externalUrl":null,"permalink":"/draft/transformers-pt-3/","section":"Drafts","summary":"(it\u0026rsquo;s glorified sequence classification)","title":"ELI5 Transformers (Part 3) - Demystify Text Generation","type":"draft"},{"content":"","date":"2 January 2026","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","date":"2 January 2026","externalUrl":null,"permalink":"/","section":"Myxiniformes Moment","summary":"","title":"Myxiniformes Moment","type":"page"},{"content":"","date":"2 January 2026","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" ","date":"2 January 2026","externalUrl":null,"permalink":"/topics/","section":"Topics","summary":"","title":"Topics","type":"topics"},{"content":"As someone without much backrounds in neither physics nor computer science, I find lots of available introductions on transformers very confusing. However, most of the articles on transformers focuses on attention mechanisms, using either the OG transformer or the classic BERT as examples. There are a lot of very good learning materials out there, for example the amazing interactive transofmer explainer.\nFinally, I\u0026rsquo;ve decided to bite the bullet and spend some time have a read through the HuggingFace\u0026rsquo;s source code for many of these models, like this one. I took lots of notes here and there during the process of studying transformers.\nThis series of blogposts collects my notes and ELI5s of what I\u0026rsquo;ve learned, hope these notes can help others alongside their studying, or being interesting little pieces of articles to read through.\n","date":"2 January 2026","externalUrl":null,"permalink":"/topics/transformers/","section":"Topics","summary":"","title":"Transformer","type":"topics"},{"content":" ","date":"27 December 2025","externalUrl":null,"permalink":"/posts/","section":"Blog","summary":"","title":"Blog","type":"posts"},{"content":" This is part 2 of the ELI5 transformers series, however you do not need to read Part 1 in order to follow the article. Link to the previous article can be found in this link\nHaving a trained transformer model alone is not enough to practically solve most tasks: you also need infrastructure that converts inputs and outputs between human- and model-readable formats. From experience, much of the work in AI is actually on building a maintainable, production-ready ecosystem that manages the model rather than on the model itself. The majority of the cost (both in time and resources) for running and maintaining AI comes from this supporting infrastructure.\nDifferent tasks (i.e., text-to-voice, audio transcription, text-generation, paragraph summrisation, etc.,) would require different infrastructures, of cource, but the general idea behind running these tasks are the same.\nThis post mainly focuses on a general introduction on running a already trained model, with main focus on the follow 3 sections:\nHow to get a pretrained model\nsetup virtual environment for testing ideas download model from checkpoint How to run the model\nconverting inputs into model-readable format passing model-readable inputs to the model and converting model outputs back to human-readable format Understand what happens when running the model\nbasic model structure: base model and task head A step-by-step walkthrough of matrix transformation during the text generation Getting a trained model # relevant XKCD Preparing the environment # First, install dependencies and start an interactive Python session. In the terminal, create a virtual environment so the work in this post doesn\u0026rsquo;t affect other projects:\npython -m venv transformers-pt2 If you were using windows, type these in your terminal:\ntransformers-pt2\\Scripts\\activate For mac and linux, run:\nsource transformers-pt2/bin/activate Then install dependencies and start Python:\npip install torch tokenizers -U pip install transformers --pre python All set.\nDownload the model # The model in this demo is Qwen3, a small text-generation model that performs surprisingly well. Qwen/Qwen3-0.6B text-generation 802 7.059495e\u0026#43;06 Import basic dependencies:\nimport pprint import torch from transformers import AutoModelForCausalLM Sidenote: As of November 2025, Pytorch/ Apple still haven\u0026rsquo;t fully fixed the memory leak issue for Apple Silicon devices. As a result, running models with pytorch may gets slower and slower over time, or freeze compeletly. If you were running a Macbook purchased after 2020, I\u0026rsquo;d recomment manually set pytorch device as \u0026lsquo;cpu\u0026rsquo;.\nSkip this if you were confident that this memory leak won\u0026rsquo;t happen:\ntorch.set_default_device(\u0026#39;cpu\u0026#39;) We then download our model from the huggingface hub:\ndevice = torch.get_default_device() checkpoint = r\u0026#34;Qwen/Qwen3-0.6B\u0026#34; # link to the model model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device) model.requires_grad_(False) # setting our model in inference mode Running the model # Processors # Now we try some inputs:\n\u0026gt;\u0026gt;\u0026gt; model.generate(\u0026#34;this is a test input\u0026#34;) ... 2546 if \u0026#34;inputs_tensor\u0026#34; in inspect.signature(decoding_method).parameters.keys(): 2547 generation_mode_kwargs[\u0026#34;inputs_tensor\u0026#34;] = inputs_tensor -\u0026gt; 2548 batch_size = inputs_tensor.shape[0] 2550 device = inputs_tensor.device 2551 self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=device) AttributeError: \u0026#39;str\u0026#39; object has no attribute \u0026#39;shape\u0026#39; help(model.generate) explains that our model expectstorch.Tensor, instead of str:\nHelp on method generate in module transformers.generation.utils: ... Parameters: inputs (`torch.Tensor` of varying shape depending on the modality, *optional*): The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs` should be in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of `input_ids`, `input_values`, `input_features`, or `pixel_values`. ... In order to run the model, we need to convert text into torch.Tensor objects.\nAlmost all transformer models would require one to first convert inputs into model-readable formats. Ways of data preprocessing changes greatly depending on our desired inputs and desired output. Taking image generation for example, our inputs would be texts (\u0026lsquo;generate me a cat pictrure\u0026rsquo;) and our outputs would be images (\u0026lsquo;cat-picture.png\u0026rsquo;). There are a lot of different processors, depending on your task. Here are some examples:\nType Example Processor text huggingface tokenizers, nltk, spacy, tiktoken image torch vision, huggingface, scikit-image video torchcodec, llava, whisper audio torchaudio reinforcement learning torchrl, Gymnasium, brax mixed huggingface\u0026rsquo;s Processor, vllm Despite all these variations, all these processors are doing essentially the same task: converting inputs into model-readable forms. However, it is worth noting that:\nFor most cases, processors are model-specific, that is, a model would only work if it was paired with the processor that used in model training. For example, a GPT2 model trained with GPT2 tokenizer would not function propery with a BERT tokenizer. Modifying the processor usually means we would need to modify the model accordingly. This does not always mean we need to re-train the already-trained model, but one should always check if this was needed. Tokenizer # Let\u0026rsquo;s go back to our text generation example. Raw text data is processed via tokenisation. Technic details behind this process very complicated, you can check out a more detailed guide. When it comes to production, you can simply think tokenisers as some sort of very glorified look-up tables that converts texts into indices that our model recognises:\nfrom transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(checkpoint) example = \u0026#34;Hi!\u0026#34; token = tokenizer(example, return_tensors=\u0026#34;pt\u0026#34;) pprint.pprint(token) This converts our result to model-ready format:\n{\u0026#39;input_ids\u0026#39;: tensor([[13048, 0]]), \u0026#39;attention_mask\u0026#39;: tensor([[1, 1]])} Let\u0026rsquo;s now try with model generation:\nresult = model.generate(**token, max_new_tokens=20) # set max-length to stop model generating forever print(result) Output:\ntensor([[13048, 0, 358, 1184, 1492, 448, 419, 3491, 13, 576, 3491, 2727, 25, 362, 220, 16, 15, 15, 15, 20972, 3745, 374]]) And use our tokenizer to convert result back to human readable form:\nprint(tokenizer.decode(result)) Output:\n\u0026gt;\u0026gt;\u0026gt; print(tokenizer.decode(result)) ... [\u0026#39;Hi! I need help with this problem. The problem says: A 1000 kg box is\u0026#39;] Chat template # Although our model indeed generated something meaningful, this output isn\u0026rsquo;t really what we\u0026rsquo;ve expected! This is because our model, just like most of other chat-based text generation models, expects inputs in a very specific format: chats:\ntext = \u0026#34;Hi!\u0026#34; message = [{\u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: text}] # format our input as chat message message = tokenizer.apply_chat_template( message, tokenize=False, # to see our formatted text enable_thinking=False, # disable thinking mode add_generation_prompt=True ) print(message) As seen in the output, our input text is now formatted as chat that message that model would recognise:\n... \u0026lt;|im_start|\u0026gt;user Hi!\u0026lt;|im_end|\u0026gt; \u0026lt;|im_start|\u0026gt;assistant \u0026lt;think\u0026gt; \u0026lt;/think\u0026gt; \u0026gt;\u0026gt;\u0026gt; Now we tokenize the message and run the text generation\ntoken = tokenizer(message, return_tensors=\u0026#34;pt\u0026#34;) result = model.generate(**token, max_new_tokens=20) print(tokenizer.decode(result)[0]) Output:\n\u0026lt;|im_start|\u0026gt;user Hi!\u0026lt;|im_end|\u0026gt; \u0026lt;|im_start|\u0026gt;assistant \u0026lt;think\u0026gt; \u0026lt;/think\u0026gt; Hello! How can I assist you today?\u0026lt;|im_end|\u0026gt; \u0026gt;\u0026gt;\u0026gt; In addition to the text generated by the model, our raw output also contains input text. We can get rid of them:\ngenerated_tokens = result[0, token[\u0026#39;input_ids\u0026#39;].shape[1]:] generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True) print(generated_text) We finally get our decoded output:\nHello! How can I assist you today? \u0026gt;\u0026gt;\u0026gt; Running text generation # Putting everything together, here\u0026rsquo;s our text-generation function:\nfrom functools import partial def run_text_generation( model, tokenizer, input_text, enable_thinking=False, max_new_tokens=256 ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Run text generation and print out the generate text\u0026#34;\u0026#34;\u0026#34; # format as chat message = [{\u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: input_text}] # tokenize tokens = tokenizer.apply_chat_template( message, enable_thinking=enable_thinking, add_generation_prompt=True, return_tensors=\u0026#34;pt\u0026#34;, ) # run text generation result = model.generate(**tokens, max_new_tokens=max_new_tokens) # decode generated_tokens = result[0, tokens[\u0026#39;input_ids\u0026#39;].shape[1]:] generated_text = tokenizer.decode(generated_tokens) print(generated_text) generate = partial(run_text_generation, model, tokenizer) To run our function:\ngenerate(\u0026#34;how are you?\u0026#34;, enable_thinking=True) # Let\u0026#39;s try with thinking mode! Output:\n\u0026lt;think\u0026gt; Okay, the user asked, \u0026#34;how are you?\u0026#34; I need to respond appropriately. Let me start by acknowledging their question. I should be friendly and open. Maybe say something like, \u0026#34;Hi! I\u0026#39;m here to help. How are you feeling today?\u0026#34; I should keep the tone positive and offer assistance. Maybe mention that I\u0026#39;m available to help with anything. Also, make sure the response is concise but covers the key points. Let me check if there\u0026#39;s any specific context I should consider, but since the user didn\u0026#39;t mention anything else, I can proceed with a general response. Alright, that should work. \u0026lt;/think\u0026gt; Hi! I\u0026#39;m here to help. How are you feeling today? ðŸ˜Š\u0026lt;|im_end|\u0026gt; \u0026gt;\u0026gt;\u0026gt; Putting everything together # To recap what\u0026rsquo;s covered in the previous demo, the full workflow of running a transformer model would be:\nStage 1 Pre-process The input gets converted into model-readable form. Stage 2 Transformer The pre-processed inputs feed into the model. Stage 3 Post-process Model outputs is converted back to human readable form. On a side note, this workflow is suprising similar to the model architecture of a transformer model, as mentioned in the previous post:\nStage 1 converts inputs indices into vectors so that it\u0026rsquo;s model-readable (Embedding). Stage 2 computes the vector representation of the input. This is the main focus of the previous post (Transformer). Stage 3 converts raw inputs to task-specific output (Model output). Model Inference # Transformers are, just like all other neural networks, functions that handles matrix computations. Considering it\u0026rsquo;s size, it is basically impossible to trace every internal calculation steps. On the other hand, understanding how inputs are converted into the outputs can be crucial when it comes to model development and debugging. Instead of looking at how raw numbers gets changed over the calculation steps, looking at how the shape of input matrix gets converted during model inference would be much more helpful and much more informative.\nLet\u0026rsquo;s use text-generation as our example. We will be using the same QWEN3 model \u0026amp; tokenizer used in the previous section.\nGetting the model \u0026amp; the tokenizer:\nimport pprint import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = torch.get_default_device() checkpoint = r\u0026#34;Qwen/Qwen3-0.6B\u0026#34; # link to the model checkpoint model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device) model.requires_grad_(False) # setting our model in inference mode tokenizer = AutoTokenizer.from_pretrained(checkpoint) Task Head # All transformer models can be conceptually divided into two parts: a base_model that computs inputs and produces raw logits, and a task_head that process the raw logits into task-specific outputs:\n\u0026gt;\u0026gt;\u0026gt; base_model = model.model ... task_head = model.lm_head ... ... print(\u0026#34;base model:\u0026#34;, base_model.__class__.__name__) ... print(\u0026#34;task head:\u0026#34;, task_head) ... base model: Qwen3Model task head: Linear(in_features=1024, out_features=151936, bias=False) \u0026gt;\u0026gt;\u0026gt; Text generation, step-by-step # Let\u0026rsquo;s go back to our previous exmaple and have a look at how matrices are transformed throughout the entire process.\nWe first create an text input:\nmessage = [{\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;how are you?\u0026#39;}] tokens = tokenizer.apply_chat_template( message, add_generation_prompt=True, enable_thinking=False, return_tensors=\u0026#34;pt\u0026#34; ) inputs = tokens[\u0026#34;input_ids\u0026#34;] print(\u0026#34;length of formatted text:\u0026#34;, len(message)) print(\u0026#34;shape of converted tokens:\u0026#34;, inputs.shape) We can see that through the tokenizer, our text input was converted from a list of size \\(1\\) into a matrix of size \\(1 \\times 16\\), as:\nbatch_index \\(\\times\\) token_indices\nThe batch_index of 1 here means the model is processing 1 single text entry; where the token_indices of 16 means there are 16 tokens in our text input:\n... length of formatted text: 1 shape of converted tokens: torch.Size([1, 16]) \u0026gt;\u0026gt;\u0026gt; Let\u0026rsquo;s first take a look at what our text gets transformed into:\n\u0026gt;\u0026gt;\u0026gt; converted_tokens = tokenizer.convert_ids_to_tokens(inputs[0]) \u0026gt;\u0026gt;\u0026gt; pairs = list(zip(inputs[0].tolist(), converted_tokens)) \u0026gt;\u0026gt;\u0026gt; pprint.pprint(pairs) ... [(151644, \u0026#39;\u0026lt;|im_start|\u0026gt;\u0026#39;), (872, \u0026#39;user\u0026#39;), (198, \u0026#39;ÄŠ\u0026#39;), (5158, \u0026#39;how\u0026#39;), (525, \u0026#39;Ä are\u0026#39;), (498, \u0026#39;Ä you\u0026#39;), (30, \u0026#39;?\u0026#39;), (151645, \u0026#39;\u0026lt;|im_end|\u0026gt;\u0026#39;), (198, \u0026#39;ÄŠ\u0026#39;), (151644, \u0026#39;\u0026lt;|im_start|\u0026gt;\u0026#39;), (77091, \u0026#39;assistant\u0026#39;), (198, \u0026#39;ÄŠ\u0026#39;), (151667, \u0026#39;\u0026lt;think\u0026gt;\u0026#39;), (271, \u0026#39;ÄŠÄŠ\u0026#39;), (151668, \u0026#39;\u0026lt;/think\u0026gt;\u0026#39;), (271, \u0026#39;ÄŠÄŠ\u0026#39;)] \u0026gt;\u0026gt;\u0026gt; For our tokenizer, white spaces are represented by symbol Ä  and ÄŠ.\nAs we can see from the output, our input gets formatted into a chats, and then conveted into a matrix of integers, with each entry represent the token_id of the corresponding text string. If you print(inputs), you would see the actual inputs that model receives:\n\u0026gt;\u0026gt;\u0026gt; print(inputs) tensor([[151644, 872, 198, 5158, 525, 498, 30, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271]]) \u0026gt;\u0026gt;\u0026gt; We then pass our the tokens to the base model:\noutput = base_model(**tokens) last_hidden_state = output.last_hidden_state print(\u0026#34;shape of base_model output:\u0026#34;, last_hidden_state.shape) Result:\n... shape of base_model output: torch.Size([1, 16, 1024]) Through the base_model, our input tokens of size \\(1 \\times 16\\) gets transformed into a matrix of size \\(1 \\times 16 \\times 1024 \\), formatted as:\nbatch_index \\(\\times\\) token_id \\(\\times\\) token_representation\nThis output from our base_model can be seen as the final transformed matrix representation of our original input data:\n\u0026gt;\u0026gt;\u0026gt; print(last_hidden_state) tensor([[[ 5.0938, 18.7500, -0.2129, ..., -0.9297, 0.7852, 0.8203], [ 1.7891, 26.6250, -1.1484, ..., -3.8906, -3.3906, -0.8125], [ -1.0000, 7.3125, -1.4531, ..., -0.8672, -1.6797, -2.7969], ..., [ 3.4062, -32.2500, 0.1260, ..., 1.3906, -3.0938, 1.4844], [ 3.7656, -7.3750, 0.2070, ..., 3.7500, -1.4375, -1.3984], [ -1.4531, 34.0000, -0.6484, ..., -0.7188, 1.3047, 1.0625]]], dtype=torch.bfloat16) \u0026gt;\u0026gt;\u0026gt; We then pass the output of base_model to the task_head:\nmodel_output = task_head(last_hidden_state) print(\u0026#34;shape of the final model output:\u0026#34;, model_output.shape) print(\u0026#34;vocabulary size:\u0026#34;, len(tokenizer.vocab)) Result:\n... shape of the final model output: torch.Size([1, 16, 151936]) vocabulary size: 151669 This output matrix of \\( 1 \\times 16 \\times 151936\\) is:\nbatch_size \\( \\times \\) token_index \\( \\times \\) token_score\nWhere token_score refers to the score for the next token index (next text) given all previous text. For example, token_score at [ 0, 7, : ] in our output of \\( 1 \\times 16 \\times 151936\\) matrix refers to the score of 7th token given previous token 0-6.\nTo get our 17th token, we only need the last sequence of our model_output:\nnext_token_score = model_output[:, -1, :] print(\u0026#34;shape of next_token_score:\u0026#34;, next_token_score.shape) print(\u0026#34;value of next_token_score:\u0026#34;, next_token_score) Result:\n... shape of next_token_score: torch.Size([1, 151936]) value of next_token_score: tensor([[14.1250, 12.9375, 7.6875, ..., 4.3750, 4.3750, 4.3750]], dtype=torch.bfloat16) To convert score to probablities, although not necessary, use softmax:\n\u0026gt;\u0026gt;\u0026gt; next_token_probability = torch.nn.functional.softmax(next_token_score, dim=-1) \u0026gt;\u0026gt;\u0026gt; print(next_token_probability) tensor([[9.7603e-07, 2.9802e-07, 1.5643e-09, ..., 5.7071e-11, 5.7071e-11, 5.7071e-11]], dtype=torch.bfloat16) Finally, we need to use tokenizer to convert model output to human-redable format. We use tokens with highest next_token_score as the next token for text generation:\ntop_score, top_token_index = next_token_score.topk(1, dim=-1) print(\u0026#34;top 1 score:\u0026#34;, top_score) print(\u0026#34;top 1 next token index:\u0026#34;, top_token_index) print(\u0026#34;shape of output:\u0026#34;, top_token_index.shape) Our top_token_index matrix has shape \\( 1 \\times 1 \\):\n... top 1 score: tensor([[27.7500]], dtype=torch.bfloat16) top 1 next token index: tensor([[9707]]) shape of output: torch.Size([1, 1]) That is batch_index \\( \\times \\) top1_token_index\nWe use tokenizer, the processor for text transformers, to decode top_token_index back to text:\ndecoded_next_token = tokenizer.decode(top_token_index) print(decoded_next_token) And here it is:\n... [\u0026#39;Hello\u0026#39;] Recap: transformation journey # The demo in the section above shows how matrix is transformed step-by-step through the whole model inference process, here\u0026rsquo;s a recap of how the shape of our input data changes throughout the computation process:\n--- config: layout: dagre theme: redux look: neo --- flowchart TB subgraph Model[\"\n\"] n4[\"last_hidden_state\nsize: [1, 16, 1024]\"] n5[\"next_token_score\nsize: [1, 16, 151936]\"] end n1@{ label: \"input: 'how are you?'\" } --\u003e n2([\"chat message\nsize: [1]\"]) n2 -- tokenizer --\u003e n3[\"tokens\nsize: [1, 16]\"] n3 -- base model --\u003e n4 n4 -- task head --\u003e n5 n5 -- top 1 --\u003e n6([\"top_token_index\nsize:[1, 1]\"]) n6 -- tokenizer --\u003e n7([\"decoded_next_token\nsize: [1]\"]) n7 --\u003e n8@{ label: \"next token: 'Hello'\" } n4@{ shape: braces} n5@{ shape: braces} n1@{ shape: rect} n3@{ shape: terminal} n8@{ shape: rect} classDef default fill: transparent, bg-color: transparent style n1 stroke-width:4px,stroke-dasharray: 0 style n8 stroke-width:4px,stroke-dasharray: 0 style Model fill:transparent Conclusion # Running transformers boils down to three things: preprocess inputs into model-readable formats, pass them through the model, and post-process outputs back to human-readable forms. Much of the work in machine learning isn\u0026rsquo;t actually about the modelâ€”it\u0026rsquo;s about the supporting infrastructure. Processors are as important as the model when it comes to using them. I would say understanding data flow and how to format inputs and interpret outputs would be more than enough to cover 80% of using transformers as productivity tools.\nThe next post will take a more in-depth look at text-generation, and starting to divert the main focus from inference to model training.\nSome further reading # Here are some external reading materials that I found very useful when it comes to learning model inference:\npytorch\u0026rsquo;s intro on neural networks. pytorch\u0026rsquo;s guide on torch.nn library, this covers 90% of the basics you need to know for pytorch models from the transformers library. transformer\u0026rsquo;s tutorial on running model pipelines, instead of writing your own functions of parsing the model, transformers library provides Pipelines that runs the entire model inference from human-readable inputs to human-readable outputs. The linked tutorial covers the basics for how to use them. The basic structure behind Pipelines are exactly the same as what described in this post: preprocess, model forward, post process. ","date":"27 December 2025","externalUrl":null,"permalink":"/posts/transformers-pt-2/","section":"Blog","summary":"(it\u0026rsquo;s just learning the API)","title":"ELI5 Transformers (Part 2) - Running the model","type":"posts"},{"content":" In this particular post, I would like to do a very brief overview of the transformer model architecture, specifically on the attention mechanism. I won\u0026rsquo;t go into too much math and there won\u0026rsquo;t be any mathematical formulas. However, I would assume readers of this silly little post already have some okay-ish background in math/datascience. (i.e., matrix computations, embeddings, tokens, model fitting etc.). I am not going to list out all the implementation details for transformers, since there are a lot of very good materials out there and they are doing fantastic jobs. Instead, in this (maybe series of?) post, I would like to draw out a general framework on transformers to help one understand the detailed math behind.\nToday, I\u0026rsquo;ll very quickly go through some very basics on neural network model, just enough to cover what needed for this post, accompied by demo of transformer model as a proof of concept. In this section, there will be some codes that you can copy and paste into an interactive python session to fiddle around for a bit. And lastly, I\u0026rsquo;ll do a quick sketch on the general architecture of transformers, and a overview of the attention mechasm.\nModel only needs to be useful # In order to make things easier to understand, I would wish to start with an inaccuate premise: we can view neural network models as functions that takes some sort of matrix as inputs, do some sort of matrix computations, and output another matrix as the final result. What makes one neural network different from others is how the computation is carried out. It\u0026rsquo;s like \\(y=a \\times x^2\\) is a different function from \\(y=a \\times sin(x)\\), only that in the case of neural network, both x and y are matrices, and the math is much complicated. When it comes to model training, we are essentially trying to find values gives best fit to the data.\nThere\u0026rsquo;s an important assumption here: just because a model fits the data well does not mean the model describes the mechanisms behind the data. For example, we definitely can fit \\(y=a \\times sin(x) + b\\) to a normal distribution data (like distribution of customer spendings in McDonald\u0026rsquo;s), and it\u0026rsquo;s probably going to be a pretty good fit, but this does not mean the sine function has anything to do with explaining the normal distribution. A good model does not always need to be a description; a good model just needs to be useful for its purpose.\nTransformers are precisely these kinds of models: they are, surprisingly good at fitting into all sorts of data whilst the math behind the model probably doesn\u0026rsquo;t have much to do with the mechanisms behind. We don\u0026rsquo;t know how transformers work so well for text-based tasks. At least not yet. Originally, transformer was designed as an add-on to text-processing neural network models in order to tackle some tricky problems (these problems are not the main focus of the current blogpost so I\u0026rsquo;m skipping them, but here\u0026rsquo;s a good article if you were interested). We just happened to discover that transformers alone is good enough to solve these problems; we just need to make the transformers much bigger. So that\u0026rsquo;s where the AI boom started: GPT2 solved issues in GPT1 by simply being 10 times bigger; the most-recently open-sourced pretrained GPT-OSS is 200 times bigger than the previous open-sourced model, GPT2 (note: GPT-OSS is structurally different from the original GPT2 but the fundamental ideas are the same.) There are even speculations suggesting transformer neural network models can be seen as some sort of universal function approximator. That is, it\u0026rsquo;s capable of \u0026lsquo;approximating\u0026rsquo; other formulas/functions with a certain degree of accuracy, providing the model itself is big enough (\u0026lsquo;universal approximation theorem\u0026rsquo;). Transformer Model Architecture # Overview # At conceptual level, the general idea behind transformer models is actually pretty intuitive. We can roughly divide the model calculations into three stages: Stage 1 Embedding The input gets converted into vectors or matrices. The first step starts with creating a mathematical representation of our input data. This process can vary based on different types of inputs. It can simply be some sort of look-up tables (text embedding), some matrix transformations of the raw inputs (convolution) etc. Stage 2 Transformer The raw output from Stage 1 feeds into the multiple different attention layers. Mathematically, each attention layer is doing very much the same mathematical operation, with each layer having its own sets of parameters. Each layer takes a matrix as an input, and outputs another matrix to pass onto the next layer. This process is repeated multiple times. Stage 2 is the core of a transformer model, it *transforms* our inputs into something else. Stage 3 Output We convert the matrix output from Stage 2 into task-specific results. This is usually done by another set of simple matrix operations, depending on the task. For example, if we are doing text sentimental analysis task, this operation could be a simple matrix multiplication, resulting in a final score of 0-10. In other words, you can conceptually see Stage 1 as a conversion stage in order to initiate the model, Stage 2 being the core of a transformer model, and Stage 3 as a \u0026lsquo;decoding\u0026rsquo; step to convert the output back into human-readable form. When we talk about transformer models, we are mostly referring to Stage 2, which is the focus of the current post. I\u0026rsquo;ll elaborate a lot more on what\u0026rsquo;s happening in Stage 3 in the next post.\nThe Transformer Itself # The transformer itself is pretty straightforward: it consists of stacks of multiple attention layers that often share exactly the same, or very similar structure: --- config: layout: dagre --- flowchart LR n1[\"Input\"] --\u003e n2[\"Attention\nLayer\"] n2 --\u003e n3[\"Attention\nLayer\"] n3 --\u003e n6[\"Repeat\"] n6 --\u003e n5[\"Output\"] n2@{ shape: rounded} n3@{ shape: rounded} n6@{ shape: text} classDef default fill: transparent, bg-color: transparent Asttention layer consists of multiple \u0026lsquo;attention heads\u0026rsquo; that works in parallel: each attention head processes the inputs independently, and the output of all attention heads are merged back together. The joined matrix is the final output of the current layer: flowchart LR subgraph s1[\"Attention Heads\"] direction LR n14[\"Attention Head\"] n16[\"Attention Head\"] n17[\"Attention Head\"] end s1 --\u003e n10[\"Merge\"] n10 --\u003e n18[\"Next Layer\"] n19[\"Previous Layer\"] --\u003e s1 n18@{ shape: text} n19@{ shape: text} classDef default fill: transparent style s1 fill:transparent The attention head # You can think each of the attention head as a mini neural network. A typical attention head works like this:\nflowchart TB IN[\"Previous Layer\"] --\u003e Q[\"Matrix 1\"] \u0026 K[\"Matrix 2\"] \u0026 V[\"Matrix 3\"] Q ---\u003e SDPA[\"Matrix 1 \u0026 2\"] K ---\u003e SDPA SDPA --\u003e n1[\"Output\"] V ---\u003e n1 classDef default fill: transparent, bg-color: transparent Step 1: The input matrix gets converted into multiple matices through matrix multiplication. Most current transformers converts input matrix into three smaller matrices. Step 2: Two of the matrix from step (1) gets combined together using some matrix operation, usually dot products. Step 3: The third matrix from step (1) combines with output from step (2), using some other matrix operation. Example: Qwen3 # We\u0026rsquo;ll now take a look at an actual transformer model and see how it works in action. It\u0026rsquo;s very suprising is that, transformers are able to produce pretty impressive results for tasks model that are not specifically trained for. Here, we\u0026rsquo;ll use Qwen3, a small-sized text generation model as a demo. It\u0026rsquo;s tiny (~1.5GB) but the performance is VERY impressive for its size.\nHere\u0026rsquo;s huggingface\u0026rsquo;s link to the model: Qwen/Qwen3-0.6B text-generation 802 7.059495e\u0026#43;06 Prep # Make sure you have python pre-installed. If you were using windows, python can be downloaded from the Microsoft Store. If you were Mac/ Linux Mint/ Ubuntu/ Debian etc., your computer should already come with python by default. For this demo, we would need python version \\( \\geqslant \\) 3.12.\nWe first need to install some dependencies. Open terminal/ command prompt, type this command to install required dependencies:\npip install torch transformers And then type:\npython To open python and start an interactive python session.\nAlternatively, if you installed ipython, which should already be installed if you have installed jupyter notebook previously, you can open ipython instead:\nipython Once python was opened, copy and paste these lines into python to import required packages:\nfrom pprint import pprint import re from transformers import AutoModelForCausalLM, AutoTokenizer, BatchEncoding Get the model # Models from transformers library takes matrices as inputs, and outputs matrices. Thus, text generation models needs to be paired with tokenizer in order to convert inputs into model-readable formant, and convert model outputs into human-readble format. transformers library has a special class called TextGenerationPipeline to handle this conversion class, but would be a bit too complicated for our purpose. Here, I modified demo code from Qwen3\u0026rsquo;s demo code that simply combines tokenizers and models together. Here\u0026rsquo;s the code for you to copy and paste into the currently running python session:\nclass DemoChatbot: \u0026#34;\u0026#34;\u0026#34; A simple demo chatbot, code modified from https://huggingface.co/Qwen/Qwen3-0.6B \u0026#34;\u0026#34;\u0026#34; def __init__(self, model_name:str=\u0026#34;Qwen/Qwen3-0.6B\u0026#34;): self.tokenizer = AutoTokenizer.from_pretrained(model_name) self.enable_thinking = False self._thiniking_regex = r\u0026#34;\u0026lt;think\u0026gt;(.*?)\u0026lt;/think\u0026gt;\u0026#34; self.model = AutoModelForCausalLM.from_pretrained( model_name, ) self.history = [] def clear_history(self) -\u0026gt; None: self.history = [] def tokenise(self, user_input:str, enable_thinking=None) -\u0026gt; BatchEncoding: if enable_thinking is None: enable_thinking = self.enable_thinking # default value messages = self.history + [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}] tokens = self.tokenizer.apply_chat_template( messages, tokenize=True, enable_thinking=enable_thinking, add_generation_prompt=True, return_tensors=\u0026#34;pt\u0026#34; ) return tokens def __call__(self, user_input, enable_thinking=None) -\u0026gt; None: inputs = self.tokenise(user_input, enable_thinking=enable_thinking) response_ids = self.model.generate( **inputs, max_new_tokens=2048, ) response_ids = response_ids[0][len(inputs.input_ids[0]):].tolist() response = self.tokenizer.decode(response_ids, skip_special_tokens=True) # since we aren\u0026#39;t going to do anything with the output, # just prints out the response and save it to the chat history. print(response) # Update history response_history = re.sub(self._thiniking_regex, \u0026#34;\u0026#34;, response, flags=re.DOTALL).strip() self.history.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}) self.history.append({\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: response_history}) Then, create a new chat instance with the code below:\nchat = DemoChatbot() You will see a progress bar showing download status. Once completed, type the following to have a look at the QWen3 model structure:\npprint(chat.model) â€¦which in term will give you this output:\nQwen3ForCausalLM( (model): Qwen3Model( (embed_tokens): Embedding(151936, 1024) (layers): ModuleList( (0-27): 28 x Qwen3DecoderLayer( (self_attn): Qwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) (mlp): Qwen3MLP( (gate_proj): Linear(in_features=1024, out_features=3072, bias=False) (up_proj): Linear(in_features=1024, out_features=3072, bias=False) (down_proj): Linear(in_features=3072, out_features=1024, bias=False) (act_fn): SiLUActivation() ) (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) ) ) (norm): Qwen3RMSNorm((1024,), eps=1e-06) (rotary_emb): Qwen3RotaryEmbedding() ) (lm_head): Linear(in_features=1024, out_features=151936, bias=False) ) Model Architecture # The very core of a transformer model can be checked by:\npprint(chat.model.model) And you will get an output similar to the previous output, but without lm_head at the end:\nQwen3Model( (embed_tokens): Embedding(151936, 1024) (layers): ModuleList( (0-27): 28 x Qwen3DecoderLayer( (self_attn): Qwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) (mlp): Qwen3MLP( (gate_proj): Linear(in_features=1024, out_features=3072, bias=False) (up_proj): Linear(in_features=1024, out_features=3072, bias=False) (down_proj): Linear(in_features=3072, out_features=1024, bias=False) (act_fn): SiLUActivation() ) (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) ) ) (norm): Qwen3RMSNorm((1024,), eps=1e-06) (rotary_emb): Qwen3RotaryEmbedding() ) The embed_tokens is the Stage 1 mentioned earlier, and the lm_head is the Stage 3. The core of a transformer, is what we\u0026rsquo;ve been discussing today.\nLet\u0026rsquo;s have a look at one of the layers inside the transformer:\nlayer = chat.model.model.layers[0] pprint(layer) You will see something like this:\nQwen3DecoderLayer( (self_attn): Qwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) (mlp): Qwen3MLP( (gate_proj): Linear(in_features=1024, out_features=3072, bias=False) (up_proj): Linear(in_features=1024, out_features=3072, bias=False) (down_proj): Linear(in_features=3072, out_features=1024, bias=False) (act_fn): SiLUActivation() ) (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) ) Layer is divided into 2 major sections, just like we discussed previously:\nself_attn is where attention gets calculated. All the rest (i.e., mlp, input_layernorm and post_attention_layernorm) are step-by-step computations of combining and averaging the attention heads. Run the following code to see the structure of an attention head:\nattention = layer.self_attn pprint(attention) The q_proj, k_proj and v_proj are the three matrices mentioned earlier:\nQwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) The actual impelmentations of attention heads are different from model to model, but the general principle behind should be roughly the same. I\u0026rsquo;ll dive deeper into it in the future.\n// \u0026hellip;existing code\u0026hellip;\nHave some fun! # Meanwhile, since we\u0026rsquo;ve got a generative model already, we might as well test out some text generations:\nchat(\u0026#34;Why is the content, which was held to be true in perceiving, in fact only belongs to the form, and it dissolves into the formâ€™s unity ðŸ¥ºðŸ¥ºðŸ¥ºðŸ¥ºðŸ¥º??\u0026#34;) Or, you can turn on the \u0026rsquo;thinking\u0026rsquo; mode to enable chain-of-thought:\nchat.enable_thinking = True chat(\u0026#34;But...but the phenomenology Î¦147 says the inner, essential is essentially the truth of appearanceðŸ˜ ðŸ˜ ðŸ˜  I\u0026#39;m absolutely fewming\u0026#34;) If it takes too long to run, you can clear chat history to remove cached chats:\nchat.clear_history() \u0026lsquo;LLM is magic\u0026rsquo; I wanted to point out the (maybe) obvious thing here: almost all operations mentioned contain learnable parameters. Inside individual attention heads, the three matrices converted from inputs are typically created by multiplying (\u0026lsquo;dot product\u0026rsquo;) inputs with three separate matrices. These matrices are part of the learnable parameters for the attention head. When we combine matrices, the combination operation also has their own learnable parameters. Furthermore, when we combine outputs from each \u0026lsquo;attention head\u0026rsquo;, this combining operation also has its own set of trainable parameters, so on and so on\u0026hellip; Almost every stage of the matrix computations are parameterized, resulting in the unbelievably massive AI models as of today. However, the model used in the demo today, despite its size, is still able to produce long and very coherent responses. Yes, in a sense, transformers are just a huge stack of matrix calculations. However, we know very little about the reason behind it. We gave names to these matrices, but we don\u0026rsquo;t know much about their behavior.\nFinal thoughts # Transformer neural network models aren\u0026rsquo;t as mysterious as one might think. It has no difference compared with any other functions. At the end of the day it\u0026rsquo;s just another mathematical function, but takes matrices as inputs, does matrix calculations, and outputs matrices. Just like any other neural network models, it takes multiple steps to do the calculation. Within each computational step, the inputs get processed by three mini-neural networks, and outputs are re-combined together as the output of the current step. In this sense, transformers are like nested neural networks: they are bigger neural networks that contain lots of mini neural networks.\nWhat\u0026rsquo;s up next?\nThere are still a bit more stuffs that I\u0026rsquo;d like to share, like how text-generation works and what\u0026rsquo;s really happenning when we are training a generative model. We\u0026rsquo;ve heard of the same old things over and over: \u0026lsquo;generative LLM is just a very massive auto-complete!\u0026rsquo;. Whilst I do aggree with it, I also find it not helpful if one wants to understand text generation, for both model training and model inference. In the next post, we\u0026rsquo;ll have a look at the Stage 3 for text generation.\nSome Other Resources # I hope whoever comes across this post would find it useful. Here are some extra reading materials that I found particularly useful:\nPytorch\u0026rsquo;s step-byp-step guide on creating a generative LLM is prob one of the best out there that teaches you all the fundenmentals. BertViz, a very good visualisation tool for looking at attention heads layer by layer. You can run it interactively in a jupyter notebook. Huggingface\u0026rsquo;s LLM cources. Although they tends to focus on the side of programming \u0026amp; practical applications, I found many of their conceptual guides very good for a beginner. ","date":"31 October 2025","externalUrl":null,"permalink":"/posts/transformers-pt-1/","section":"Blog","summary":"(it\u0026rsquo;s glorified linear algebra)","title":"ELI5 Transformers (Part 1): Attention Mechanism ","type":"posts"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/status/","section":"Status","summary":"","title":"Status","type":"status"}]
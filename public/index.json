
[{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/","section":"Myxiniformes Moment","summary":"","title":"Myxiniformes Moment","type":"page"},{"content":"","date":"31 October 2025","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":" ","date":"31 October 2025","externalUrl":null,"permalink":"/posts/","section":"Blog","summary":"","title":"Blog","type":"posts"},{"content":"","date":"31 October 2025","externalUrl":null,"permalink":"/tags/eli5/","section":"Tags","summary":"","title":"ELI5","type":"tags"},{"content":"In this particular post, I would like to do a very brief overview of the transformer model architecture, specifically on the attention mechanism. I won\u0026rsquo;t go into too much math and there won\u0026rsquo;t be any mathematical formulas. However, I would assume readers of this silly little post already have some okay-ish background in math/datascience. (i.e., matrix computations, embeddings, tokens, model fitting etc.). I am not going to list out all the implementation details for transformers, since there are a lot of very good materials out there and they are doing fantastic jobs. Instead, in this (maybe series of?) post, I would like to draw out a general framework on transformers to help one understand the detailed math behind.\nToday, I\u0026rsquo;ll very quickly go through some very basics on neural network model, just enough to cover what needed for this post, accompied by demo of transformer model as a proof of concept. In this section, there will be some codes that you can copy and paste into an interactive python session to fiddle around for a bit. And lastly, I\u0026rsquo;ll do a quick sketch on the general architecture of transformers, and a overview of the attention mechasm.\nModel only needs to be useful # In order to make things easier to understand, I would wish to start with an inaccuate premise: we can view neural network models as functions that takes some sort of matrix as inputs, do some sort of matrix computations, and output another matrix as the final result. What makes one neural network different from others is how the computation is carried out. It\u0026rsquo;s like \\(y=a \\times x^2\\) is a different function from \\(y=a \\times sin(x)\\), only that in the case of neural network, both x and y are matrices, and the math is much complicated. When it comes to model training, we are essentially trying to find values gives best fit to the data.\nThere\u0026rsquo;s an important assumption here: just because a model fits the data well does not mean the model describes the mechanisms behind the data. For example, we definitely can fit \\(y=a \\times sin(x) + b\\) to a normal distribution data (like distribution of customer spendings in McDonald\u0026rsquo;s), and it\u0026rsquo;s probably going to be a pretty good fit, but this does not mean the sine function has anything to do with explaining the normal distribution. A good model does not always need to be a description; a good model just needs to be useful for its purpose.\nTransformers are precisely these kinds of models: they are, surprisingly good at fitting into all sorts of data whilst the math behind the model probably doesn\u0026rsquo;t have much to do with the mechanisms behind. We don\u0026rsquo;t know how transformers work so well for text-based tasks. At least not yet. Originally, transformer was designed as an add-on to text-processing neural network models in order to tackle some tricky problems (these problems are not the main focus of the current blogpost so I\u0026rsquo;m skipping them, but here\u0026rsquo;s a good article if you were interested). We just happened to discover that transformers alone is good enough to solve these problems; we just need to make the transformers much bigger. So that\u0026rsquo;s where the AI boom started: GPT2 solved issues in GPT1 by simply being 10 times bigger; the most-recently open-sourced pretrained GPT-OSS is 200 times bigger than the previous open-sourced model, GPT2 (note: GPT-OSS is structurally different from the original GPT2 but the fundamental ideas are the same.) There are even speculations suggesting transformer neural network models can be seen as some sort of universal function approximator. That is, it\u0026rsquo;s capable of \u0026lsquo;approximating\u0026rsquo; other formulas/functions with a certain degree of accuracy, providing the model itself is big enough (\u0026lsquo;universal approximation theorem\u0026rsquo;). Transformer Model Architecture # Overview # At conceptual level, the general idea behind transformer models is actually pretty intuitive. We can roughly divide the model calculations into three stages: Stage 1 Embedding The input gets converted into vectors or matrices. The first step starts with creating a mathematical representation of our input data. This process can vary based on different types of inputs. It can simply be some sort of look-up tables (text embedding), some matrix transformations of the raw inputs (convolution) etc. Stage 2 Transformer The raw output from Stage 1 feeds into the multiple different attention layers. Mathematically, each attention layer is doing very much the same mathematical operation, with each layer having its own sets of parameters. Each layer takes a matrix as an input, and outputs another matrix to pass onto the next layer. This process is repeated multiple times. Stage 2 is the core of a transformer model, it *transforms* our inputs into something else. Stage 3 Output We convert the matrix output from Stage 2 into task-specific results. This is usually done by another set of simple matrix operations, depending on the task. For example, if we are doing text sentimental analysis task, this operation could be a simple matrix multiplication, resulting in a final score of 0-10. In other words, you can conceptually see Stage 1 as a conversion stage in order to initiate the model, Stage 2 being the core of a transformer model, and Stage 3 as a \u0026lsquo;decoding\u0026rsquo; step to convert the output back into human-readable form. When we talk about transformer models, we are mostly referring to Stage 2, which is the focus of the current post. I\u0026rsquo;ll elaborate a lot more on what\u0026rsquo;s happening in Stage 3 in the next post.\nThe Transformer Itself # The transformer itself is pretty straightforward: it consists of stacks of multiple attention layers that often share exactly the same, or very similar structure: --- config: layout: dagre --- flowchart LR n1[\"Input\"] --\u003e n2[\"Attention\nLayer\"] n2 --\u003e n3[\"Attention\nLayer\"] n3 --\u003e n6[\"Repeat\"] n6 --\u003e n5[\"Output\"] n2@{ shape: rounded} n3@{ shape: rounded} n6@{ shape: text} classDef default fill: transparent, bg-color: transparent Asttention layer consists of multiple \u0026lsquo;attention heads\u0026rsquo; that works in parallel: each attention head processes the inputs independently, and the output of all attention heads are merged back together. The joined matrix is the final output of the current layer: flowchart LR subgraph s1[\"Attention Heads\"] direction LR n14[\"Attention Head\"] n16[\"Attention Head\"] n17[\"Attention Head\"] end s1 --\u003e n10[\"Merge\"] n10 --\u003e n18[\"Next Layer\"] n19[\"Previous Layer\"] --\u003e s1 n18@{ shape: text} n19@{ shape: text} classDef default fill: transparent style s1 fill:transparent The attention head # You can think each of the attention head as a mini neural network. A typical attention head works like this:\nflowchart TB IN[\"Previous Layer\"] --\u003e Q[\"Matrix 1\"] \u0026 K[\"Matrix 2\"] \u0026 V[\"Matrix 3\"] Q ---\u003e SDPA[\"Matrix 1 \u0026 2\"] K ---\u003e SDPA SDPA --\u003e n1[\"Output\"] V ---\u003e n1 classDef default fill: transparent, bg-color: transparent Step 1: The input matrix gets converted into multiple matices through matrix multiplication. Most current transformers converts input matrix into three smaller matrices. Step 2: Two of the matrix from step (1) gets combined together using some matrix operation, usually dot products. Step 3: The third matrix from step (1) combines with output from step (2), using some other matrix operation. Example: Qwen3 # We\u0026rsquo;ll now take a look at an actual transformer model and see how it works in action. It\u0026rsquo;s very suprising is that, transformers are able to produce pretty impressive results for tasks model that are not specifically trained for. Here, we\u0026rsquo;ll use Qwen3, a small-sized text generation model as a demo. It\u0026rsquo;s tiny (~1.5GB) but the performance is VERY impressive for its size.\nHere\u0026rsquo;s huggingface\u0026rsquo;s link to the model: Qwen/Qwen3-0.6B text-generation 802 7.059495e\u0026#43;06 Prep # Make sure you have python pre-installed. If you were using windows, python can be downloaded from the Microsoft Store. If you were Mac/ Linux Mint/ Ubuntu/ Debian etc., your computer should already come with python by default. For this demo, we would need python version \\( \\geqslant \\) 3.12.\nWe first need to install some dependencies. Open terminal/ command prompt, type this command to install required dependencies:\npip install torch transformers And then type:\npython To open python and start an interactive python session.\nAlternatively, if you installed ipython, which should already be installed if you have installed jupyter notebook previously, you can open ipython instead:\nipython Once python was opened, copy and paste these lines into python to import required packages:\nfrom pprint import pprint import re from transformers import AutoModelForCausalLM, AutoTokenizer, BatchEncoding Get the model # Models from transformers library takes matrices as inputs, and outputs matrices. Thus, text generation models needs to be paired with tokenizer in order to convert inputs into model-readable formant, and convert model outputs into human-readble format. transformers library has a special class called TextGenerationPipeline to handle this conversion class, but would be a bit too complicated for our purpose. Here, I modified demo code from Qwen3\u0026rsquo;s demo code that simply combines tokenizers and models together. Here\u0026rsquo;s the code for you to copy and paste into the currently running python session:\nclass DemoChatbot: \u0026#34;\u0026#34;\u0026#34; A simple demo chatbot, code modified from https://huggingface.co/Qwen/Qwen3-0.6B \u0026#34;\u0026#34;\u0026#34; def __init__(self, model_name:str=\u0026#34;Qwen/Qwen3-0.6B\u0026#34;): self.tokenizer = AutoTokenizer.from_pretrained(model_name) self.enable_thinking = False self._thiniking_regex = r\u0026#34;\u0026lt;think\u0026gt;(.*?)\u0026lt;/think\u0026gt;\u0026#34; self.model = AutoModelForCausalLM.from_pretrained( model_name, ) self.history = [] def clear_history(self) -\u0026gt; None: self.history = [] def tokenise(self, user_input:str, enable_thinking=None) -\u0026gt; BatchEncoding: if enable_thinking is None: enable_thinking = self.enable_thinking # default value messages = self.history + [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}] tokens = self.tokenizer.apply_chat_template( messages, tokenize=True, enable_thinking=enable_thinking, add_generation_prompt=True, return_tensors=\u0026#34;pt\u0026#34; ) return tokens def __call__(self, user_input, enable_thinking=None) -\u0026gt; None: inputs = self.tokenise(user_input, enable_thinking=enable_thinking) response_ids = self.model.generate( **inputs, max_new_tokens=2048, ) response_ids = response_ids[0][len(inputs.input_ids[0]):].tolist() response = self.tokenizer.decode(response_ids, skip_special_tokens=True) # since we aren\u0026#39;t going to do anything with the output, # just prints out the response and save it to the chat history. print(response) # Update history response_history = re.sub(self._thiniking_regex, \u0026#34;\u0026#34;, response, flags=re.DOTALL).strip() self.history.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}) self.history.append({\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: response_history}) Then, create a new chat instance with the code below:\nchat = DemoChatbot() You will see a progress bar showing download status. Once completed, type the following to have a look at the QWen3 model structure:\npprint(chat.model) â€¦which in term will give you this output:\nQwen3ForCausalLM( (model): Qwen3Model( (embed_tokens): Embedding(151936, 1024) (layers): ModuleList( (0-27): 28 x Qwen3DecoderLayer( (self_attn): Qwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) (mlp): Qwen3MLP( (gate_proj): Linear(in_features=1024, out_features=3072, bias=False) (up_proj): Linear(in_features=1024, out_features=3072, bias=False) (down_proj): Linear(in_features=3072, out_features=1024, bias=False) (act_fn): SiLUActivation() ) (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) ) ) (norm): Qwen3RMSNorm((1024,), eps=1e-06) (rotary_emb): Qwen3RotaryEmbedding() ) (lm_head): Linear(in_features=1024, out_features=151936, bias=False) ) Model Architecture # The very core of a transformer model can be checked by:\npprint(chat.model.model) And you will get an output similar to the previous output, but without lm_head at the end:\nQwen3Model( (embed_tokens): Embedding(151936, 1024) (layers): ModuleList( (0-27): 28 x Qwen3DecoderLayer( (self_attn): Qwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) (mlp): Qwen3MLP( (gate_proj): Linear(in_features=1024, out_features=3072, bias=False) (up_proj): Linear(in_features=1024, out_features=3072, bias=False) (down_proj): Linear(in_features=3072, out_features=1024, bias=False) (act_fn): SiLUActivation() ) (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) ) ) (norm): Qwen3RMSNorm((1024,), eps=1e-06) (rotary_emb): Qwen3RotaryEmbedding() ) The embed_tokens is the Stage 1 mentioned earlier, and the lm_head is the Stage 3. The core of a transformer, is what we\u0026rsquo;ve been discussing today.\nLet\u0026rsquo;s have a look at one of the layers inside the transformer:\nlayer = chat.model.model.layers[0] pprint(layer) You will see something like this:\nQwen3DecoderLayer( (self_attn): Qwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) (mlp): Qwen3MLP( (gate_proj): Linear(in_features=1024, out_features=3072, bias=False) (up_proj): Linear(in_features=1024, out_features=3072, bias=False) (down_proj): Linear(in_features=3072, out_features=1024, bias=False) (act_fn): SiLUActivation() ) (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) ) Layer is divided into 2 major sections, just like we discussed previously:\nself_attn is where attention gets calculated. All the rest (i.e., mlp, input_layernorm and post_attention_layernorm) are step-by-step computations of combining and averaging the attention heads. Run the following code to see the structure of an attention head:\nattention = layer.self_attn pprint(attention) The q_proj, k_proj and v_proj are the three matrices mentioned earlier:\nQwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) The actual impelmentations of attention heads are different from model to model, but the general principle behind should be roughly the same. I\u0026rsquo;ll dive deeper into it in the future.\n// \u0026hellip;existing code\u0026hellip;\nHave some fun! # Meanwhile, since we\u0026rsquo;ve got a generative model already, we might as well test out some text generations:\nchat(\u0026#34;Why is the content, which was held to be true in perceiving, in fact only belongs to the form, and it dissolves into the formâ€™s unity ðŸ¥ºðŸ¥ºðŸ¥ºðŸ¥ºðŸ¥º??\u0026#34;) Or, you can turn on the \u0026rsquo;thinking\u0026rsquo; mode to enable chain-of-thought:\nchat.enable_thinking = True chat(\u0026#34;But...but the phenomenology Î¦147 says the inner, essential is essentially the truth of appearanceðŸ˜ ðŸ˜ ðŸ˜  I\u0026#39;m absolutely fewming\u0026#34;) If it takes too long to run, you can clear chat history to remove cached chats:\nchat.clear_history() \u0026lsquo;LLM is magic\u0026rsquo; I wanted to point out the (maybe) obvious thing here: almost all operations mentioned contain learnable parameters. Inside individual attention heads, the three matrices converted from inputs are typically created by multiplying (\u0026lsquo;dot product\u0026rsquo;) inputs with three separate matrices. These matrices are part of the learnable parameters for the attention head. When we combine matrices, the combination operation also has their own learnable parameters. Furthermore, when we combine outputs from each \u0026lsquo;attention head\u0026rsquo;, this combining operation also has its own set of trainable parameters, so on and so on\u0026hellip; Almost every stage of the matrix computations are parameterized, resulting in the unbelievably massive AI models as of today. However, the model used in the demo today, despite its size, is still able to produce long and very coherent responses. Yes, in a sense, transformers are just a huge stack of matrix calculations. However, we know very little about the reason behind it. We gave names to these matrices, but we don\u0026rsquo;t know much about their behavior.\nFinal thoughts # Transformer neural network models aren\u0026rsquo;t as mysterious as one might think. It has no difference compared with any other functions. At the end of the day it\u0026rsquo;s just another mathematical function, but takes matrices as inputs, does matrix calculations, and outputs matrices. Just like any other neural network models, it takes multiple steps to do the calculation. Within each computational step, the inputs get processed by three mini-neural networks, and outputs are re-combined together as the output of the current step. In this sense, transformers are like nested neural networks: they are bigger neural networks that contain lots of mini neural networks.\nWhat\u0026rsquo;s up next?\nThere are still a bit more stuffs that I\u0026rsquo;d like to share, like how text-generation works and what\u0026rsquo;s really happenning when we are training a generative model. We\u0026rsquo;ve heard of the same old things over and over: \u0026lsquo;generative LLM is just a very massive auto-complete!\u0026rsquo;. Whilst I do aggree with it, I also find it not helpful if one wants to understand text generation, for both model training and model inference. In the next post, we\u0026rsquo;ll have a look at the Stage 3 for text generation.\nSome Other Resources # I hope whoever comes across this post would find it useful. Here are some extra reading materials that I found particularly useful:\nPytorch\u0026rsquo;s step-byp-step guide on creating a generative LLM is prob one of the best out there that teaches you all the fundenmentals. BertViz, a very good visualisation tool for looking at attention heads layer by layer. You can run it interactively in a jupyter notebook. Huggingface\u0026rsquo;s LLM cources. Although they tends to focus on the side of programming \u0026amp; practical applications, I found many of their conceptual guides very good for a beginner. ","date":"31 October 2025","externalUrl":null,"permalink":"/posts/transformers-pt-1/","section":"Blog","summary":"(it\u0026rsquo;s glorified linear algebra)","title":"ELI5 Transformers (Part 1): Attention Mechanism ","type":"posts"},{"content":"","date":"31 October 2025","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","date":"31 October 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" ","date":"31 October 2025","externalUrl":null,"permalink":"/topics/","section":"Topics","summary":"","title":"Topics","type":"topics"},{"content":"As someone without much backrounds in neither physics nor computer science, I find lots of available introductions on transformers very confusing. However, most of the articles on transformers focuses on attention mechanisms, using either the OG transformer or the classic BERT as examples. There are a lot of very good learning materials out there, for example the amazing interactive transofmer explainer.\nFinally, I\u0026rsquo;ve decided to bite the bullet and spend some time have a read through the HuggingFace\u0026rsquo;s source code for many of these models, like this one. I took lots of notes here and there during the process of studying transformers.\nThis series of blogposts collects my notes and ELI5s of what I\u0026rsquo;ve learned, hope these notes can help others alongside their studying, or being interesting little pieces of articles to read through.\n","date":"31 October 2025","externalUrl":null,"permalink":"/topics/transformers/","section":"Topics","summary":"","title":"Transformer","type":"topics"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/status/","section":"Status","summary":"","title":"Status","type":"status"}]
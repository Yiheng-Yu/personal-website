
[{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":" ","date":"11 November 2025","externalUrl":null,"permalink":"/posts/","section":"Blog","summary":"","title":"Blog","type":"posts"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/tags/eli5/","section":"Tags","summary":"","title":"ELI5","type":"tags"},{"content":" always has been This is part 2 of the ELI5 transformers series, however you do not need to read Part 1 in order to follow the article. Link to the previous article can be found here\nI\u0026rsquo;m pretty sure you\u0026rsquo;ve already heard about someting like this before: \u0026lsquo;generative LLM is just slightly advanced auto complete\u0026rsquo;. Or, something like \u0026lsquo;all it does is predicting what\u0026rsquo;s the most likely next word with all the previous words given\u0026rsquo;.\nToday I would like to invite you think of text generation in a very different perspective, at least it\u0026rsquo;s the persceptive I found helped me the most: text generation is glorified sequence classification. I\u0026rsquo;m going to walk your way through the text generation process, step-by-step, one word at a time, showing you how text generation actually works.\nFINISH THIS LATER!!\nRecap on transformer architecture # As mentioned in the previous post, AI models as we know today computes data in roughly three stages:\nflowchart LR n1([\"Raw Input\"]) --\u003e n2[\"Embedding\"] n2 --\u003e n3[\"Transformer\"] n3 --\u003e n4[\"Model output\"] n4 --\u003e n5([\"Final output\"]) classDef default fill: transparent, bg-color: transparent Stage 1 converts inputs into vectors so that it\u0026rsquo;s model-readable (Embedding). Stage 2 computes the vector representation of the input. This is the main focus of the previous post (Transformer). Stage 3 converts outputs from the transformer into human-readable, task-specific format (Model output). Stage 1 and 3 are very context-dependent as they are dependent on the type of inputs (text, image, audio etc.,) For image data, this can simply be the RGBA values for each pixel; for text data, this can be a look up table of converting sub-words into matrices. The model that\u0026rsquo;s going to be used in this demo is Qwen3, same as the previous post. It\u0026rsquo;s a very tiny text generation model that I feel performs very well. Here\u0026rsquo;s huggingface\u0026rsquo;s link to the model: Qwen/Qwen3-0.6B text-generation 802 7.059495e\u0026#43;06 Setup # Before we start, make sure you installed all the dependencies. Open termianl and type these codes to install depencies:\npip install torch transformers tokenizers -U Then in terminal, to start python:\npython ..Or ipython if you installed jupyter notebook previously:\nipython You should see something like this in your terminal, indicating the beginning of a python session:\nPython 3.13.7 | packaged by conda-forge | (main, Sep 3 2025, 14:24:46) [Clang 19.1.7 ] on darwin Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; depencies # Now import dependencies:\nimport torch from transformers import AutoTokenizer, Qwen3ForCausalLM Sidenote:\nAs of November 2025, Pytorch/ Apple still haven\u0026rsquo;t fully fixed the memory leak issue for Apple Silicon devices. As a result, running models with pytorch may gets slower and slower over time, or freeze compeletly. If you were running a Macbook purchased after 2020, I\u0026rsquo;d recomment manually set pytorch device as \u0026lsquo;cpu\u0026rsquo;. Skip this code if you were confident that this memory leak won\u0026rsquo;t happen:\ntorch.set_default_device(\u0026#39;cpu\u0026#39;) model # Finally, load the model:\nDEVICE = torch.get_default_device() CHECKPOINT = \u0026#34;Qwen/Qwen3-0.6B\u0026#34; print(\u0026#34;Using device:\u0026#34;, DEVICE) model = Qwen3ForCausalLM.from_pretrained(CHECKPOINT) model.requires_grad_(False) You are now all set. example input text # Let\u0026rsquo;s start with an example text input:\nexample = \u0026#34;Hi\u0026#34; Step-by-step Text Generation # Tokenisation # Our firtst problem is that our input (example), is str. However, our model does not accept str as input. If you just pass our input to the model, you would get this TypeError:\n\u0026gt;\u0026gt;\u0026gt; model(example) TypeError: embedding(): argument \u0026#39;indices\u0026#39; (position 2) must be Tensor, not str In order to do text-generation, one first need to to convert inputs into machine-readable formats. This conversion process is called tokenisation. Tokenisation is not the main focus of today\u0026rsquo;s post, but as a quick ELI5, you can functionally see tokenisers as glorified look-up tables that converts texts into indices. Model reads these indicies, through it\u0026rsquo;s own internal lookup table, convert them into text matrices.\nIt works like this: flowchart LR ipt([\"input\"]) \u003c--\u003e tkn[\"Tokenizer\"] tkn \u003c--\u003e mdl[\"Model\"] classDef default fill: transparent, bg-color: transparent For example: flowchart LR ipt([\"text: 'cat sits'\"]) \u003c--tokenizer--\u003e tkn[\"tokens: [[8, 10]]\"] tkn --\u003e mdl[\"Model\"] classDef default fill: transparent, bg-color: transparent For huggingface\u0026rsquo;s transformers library, tokenisation is handeled by Tokenizer objects. Every pre-trained text models in transformers library all come with their own paired tokenizers. This is the one used by our model:\ntokenizer = AutoTokenizer.from_pretrained(CHECKPOINT) To tokenize our text:\ntokens = tokenizer(example, return_tensors=\u0026#39;pt\u0026#39;) print(tokens) print(tokens[\u0026#39;input_ids\u0026#39;].shape) You would see our tokenized text input:\n{\u0026#39;input_ids\u0026#39;: tensor([[13048]]), \u0026#39;attention_mask\u0026#39;: tensor([[1]])} torch.Size([1, 1]) The Transformer # Base model \u0026amp; the task head\nModels in the transformers library are assembed by two parts: the transformer itself, task-specific \u0026lsquo;head\u0026rsquo;. Let\u0026rsquo;s have a look at our model architecture by running print(model):\n\u0026gt;\u0026gt;\u0026gt; print(model) Qwen3ForCausalLM( (model): Qwen3Model( (embed_tokens): Embedding(151936, 1024) (layers): ModuleList( (0-27): 28 x Qwen3DecoderLayer( (self_attn): Qwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) (mlp): Qwen3MLP( (gate_proj): Linear(in_features=1024, out_features=3072, bias=False) (up_proj): Linear(in_features=1024, out_features=3072, bias=False) (down_proj): Linear(in_features=3072, out_features=1024, bias=False) (act_fn): SiLUActivation() ) (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) ) ) (norm): Qwen3RMSNorm((1024,), eps=1e-06) (rotary_emb): Qwen3RotaryEmbedding() ) (lm_head): Linear(in_features=1024, out_features=151936, bias=False) ) In output above, our transformer base model is the one called (model): Qwen3Model at the very beginning, the task-head is the lm_head: Linear at the very end. Let\u0026rsquo;s separate them apart:\nbase_model = model.model task_head = model.lm_head print(base_model) shall give you the output save as above except the \u0026rsquo;lm_head\u0026rsquo; bits.\nRunning the transformer\nTo do text generation, we first pass the inputs to the base_model:\noutput = base_model(**tokens) print(output) Which would give you this output:\nBaseModelOutputWithPast(last_hidden_state=tensor([[[ 7.4006, 29.0470, -0.1732, ..., -1.2643, 1.1580, 1.1260]]]), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None) Output of the transformer base_model is the last_hidden_state attribute of the BaseModelOutputWithPast object:\nlast_hidden_state = output.last_hidden_state print(last_hidden_state.shape) Output:\ntorch.Size([1, 1, 1024]) The Task-Head # Tokenisation # Quick sidenote on tokenizer\nOn top of the transformer models themselves, we also need some helper objects in order to convert inputs/ outputs between human-readable and machine-readable formats: our inputs and desirerd outputs are both texts, however models only accepts matrices as their inputs and would only output matrices. This conversation process is called tokenisation. For huggingface transformers library, tokenisation is handeled by obejcts called Tokenizer. Tokenisation is very important process for linguistic research, and there are a lot of different methods to tokenise texts. Tokenisation is not the main focus of today\u0026rsquo;s post, but as a quick ELI5, you can functionally see tokenisers as glorified look-up tables that converts texts into indices. Model reads these indicies, through it\u0026rsquo;s own internal lookup table, convert them into text matrices. Pre-trained text models from transformers library all come with their own paired tokenizers.\nIt works like this: flowchart LR ipt([\"input\"]) \u003c--\u003e tkn[\"Tokenizer\"] tkn \u003c--\u003e mdl[\"Model\"] classDef default fill: transparent, bg-color: transparent For example: flowchart LR ipt([\"text: 'cat sits'\"]) \u003c--tokenizer--\u003e tkn[\"index: [[8, 10]]\"] tkn \u003c--model--\u003e mdl[\"matrix: [[0.3, 0.2, 0.5], [0.1, 0.7, 0.3]]\"] classDef default fill: transparent, bg-color: transparent converter\nIn order to test out the model, we\u0026rsquo;ll first need to prepare a function that converts inputs \u0026amp; outputs back and forth. Copy this function and paste into python:\ndef model_converter(tokenizer, inputs: Union[str, ModelOutput, torch.Tensor]) -\u0026gt; Union[str, BatchEncoding]: \u0026#34;\u0026#34;\u0026#34;Converts between text inputs and token indicies Args: tokenizer (PreTrainedTokenizerFast): the tokenizer to use inputs (Union[str, ModelOutput, torch.Tensor]): text inputs or model outputs \u0026#34;\u0026#34;\u0026#34; # text -\u0026gt; model inputs if isinstance(inputs, str): message = [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: inputs} ] tokens = tokenizer.apply_chat_template( message, tokenize=True, return_dict=True, enable_thinking=False, add_generation_prompt=True, return_tensors=\u0026#39;pt\u0026#39; ) return tokens # in this demo, we will enconter several different types of model outputs... # outputs from model.generate() elif isinstance(inputs, torch.Tensor): if inputs.ndim==1: return tokenizer.decode(inputs) elif inputs.ndim \u0026gt; 1 and inputs.shape[0]==1: return model_converter(tokenizer, inputs[0]) else: return tokenizer.batch_decode(inputs) # outputs from model.forward elif isinstance(inputs, ModelOutput): if \u0026#39;logits\u0026#39; in inputs: sequences = inputs.logits.topk(1, dim=-1).view(1, -1) return model_converter(tokenizer, sequences) raise TypeError(type(inputs)) Copy these codes to initialise the tokenizer: tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT) convert:Callable = partial(model_converter, tokenizer) Model # Copy these codes to initialise the model:\nmodel = AutoModelForCausalLM.from_pretrained(CHECKPOINT) model.generation_config.max_new_tokens = 128 model.requires_grad_(False) You are now all set.\nRunning the model # Before dive into the text generation mechaism, let\u0026rsquo;s try some text generation first. Copy this example text or use whatever other texts you\u0026rsquo;d like to have a try on:\nexample = \u0026#34;Why are you running?\u0026#34; Tokenisation\nFirst, we\u0026rsquo;ll need to convert input text into model-redable form:\ntokens = convert(example) Running print(tokens) would print out something looks like a dictionary with keys like input_ids and attention_mask. input_ids are our main interest, these are our formatted input that gets converted into model-redable matrix. Using convert, we can convert the tokens back into text:\nprint(convert(tokens[\u0026#39;input_ids\u0026#39;])) \u0026hellip;which would give you something like:\n\u0026lt;|im_start|\u0026gt;user Why are you running?\u0026lt;|im_end|\u0026gt; \u0026lt;|im_start|\u0026gt;assistant \u0026lt;think\u0026gt; \u0026lt;/think\u0026gt; \u0026ldquo;\u0026lt;|im_start|\u0026gt;user\u0026rdquo;, \u0026ldquo;\u0026lt;|im_start|\u0026gt;assistant\u0026rdquo;, \u0026ldquo;\u0026lt;think\u0026gt;\u0026rdquo; and \u0026ldquo;\u0026lt;/think\u0026gt;\u0026rdquo; are extra special tokens that manually amended by the model_converter in order to format our inputs into a format that the model was orinially trained with. Formatting input this way would help the model generating better results. Different tokenizers have different set of special tokens. For Qwen3 used here, you can view all special tokens by calling tokenizer.get_added_vocab() to see a list of all theset tokens.\nText Generation\nUse model.generate to run text generation:\noutputs = model.generate(**tokens) outputs You would see that our model returned antoher matrix of token indicies:\ntensor([[151644, 872, 198, 10234, 525, 498, 4303, 30, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271, 40, 2776, 537, 4303, 13, 358, 2776, 1101, 1588, 311, 1492, 498, 448, 697, 4755, 13, 6771, 752, 1414, 1246, 358, 646, 7789, 498, 0, 151645]]) Use convert to convert it back to text:\nprint(convert(outputs)) text generation is a randomised process, therefore your result may be different from mine:\n\u0026lt;|im_start|\u0026gt;user Why are you running?\u0026lt;|im_end|\u0026gt; \u0026lt;|im_start|\u0026gt;assistant \u0026lt;think\u0026gt; \u0026lt;/think\u0026gt; I\u0026#39;m not running. I\u0026#39;m just here to help you with your questions. Let me know how I can assist you!\u0026lt;|im_end|\u0026gt; Text generation, one word at a time # Instead of generating the entire sentence at once, model generates texts one token at a time. We\u0026rsquo;ll first take a look at how model generates one word from the input text, and look at the entire text generation process in the next section. For a quick recap on the model architecture, please refer to the previous post: here\u0026rsquo;s the link. Text models in the transformers library are assembed by two parts, the XXXModel itself and task-specific xx_head. The XXXModel refers to the transformer model itself, and xx_head is a converter that converts transformer outputs according to different tasks, usually called task head.\nLet\u0026rsquo;s have a look at our model by running print(model):\n\u0026gt;\u0026gt;\u0026gt; print(model) Qwen3ForCausalLM( (model): Qwen3Model( (embed_tokens): Embedding(151936, 1024) (layers): ModuleList( (0-27): 28 x Qwen3DecoderLayer( (self_attn): Qwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) (mlp): Qwen3MLP( (gate_proj): Linear(in_features=1024, out_features=3072, bias=False) (up_proj): Linear(in_features=1024, out_features=3072, bias=False) (down_proj): Linear(in_features=3072, out_features=1024, bias=False) (act_fn): SiLU() ) (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) ) ) (norm): Qwen3RMSNorm((1024,), eps=1e-06) (rotary_emb): Qwen3RotaryEmbedding() ) (lm_head): Linear(in_features=1024, out_features=151936, bias=False) ) In the outputs shown above, (model) is our transformer, and (lm_head) is our text-generation head. To demonstrate, let\u0026rsquo;s use the example mentioned in the previous section:\n# convert texts into tokens example = \u0026#34;Why are you running?\u0026#34; tokens = convert(example) # pass tokens to the transformer transformed = model.model.forward(**tokens) The raw output from transformers models contains not only the final output, but also computation results inbetween. These inbetween data are very useful for anyone who\u0026rsquo;s interested in figuring out behaviours of their model. For now we are only interested in the final output, last_hidden_state. Let\u0026rsquo;s take a look:\nprint(\u0026#39;Shape of the input matrix:\u0026#39;, tokens[\u0026#39;input_ids\u0026#39;].shape) print(\u0026#39;Shape of the output matrix:\u0026#39;, transformed.last_hidden_state.shape) print(\u0026#39;Output:\\n\u0026#39;, transformed.last_hidden_state) Output:\nShape of the input matrix: torch.Size([1, 17]) Shape of the output matrix: torch.Size([1, 17, 1024]) Output: tensor([[[ 5.1596, 19.8593, -0.2142, ..., -0.9417, 0.8061, 0.8258], [ 1.6308, 26.9123, -1.1482, ..., -3.7286, -3.3529, -0.7485], [ -0.9393, 7.0810, -1.4507, ..., -0.8947, -1.6471, -2.8123], ..., [ 4.9579, -29.7081, 0.0470, ..., 0.9277, -2.9034, 0.9229], [ 3.7464, -7.0461, 0.1408, ..., 4.0884, -1.4918, -1.3324], [ -0.5219, 12.8126, -1.1671, ..., 2.8096, 0.8872, -0.3728]]]) We then pass the outputs of our transformer to the task head, lm_hed:\nlogits = model.lm_head(transformed.last_hidden_state) print(\u0026#34;Shape of the final output matrix:\u0026#34;, logits.shape) Output:\nShape of the final output: torch.Size([1, 17, 151936]) As you can see, through the computation, our text inputs gets converted into a matrix of indicies of size 1 x 17, then gets transformed into a matrix of size 1 x 17 x 1024, and then gets converted into a (very big!) matrix of size 1 x 17 x 151,936:\nflowchart TB text([\"text\"])--tokenizer--\u003eipt[\"matrix: 1 x 17\"] --\"model\"--\u003e tfm[\"matrix: 1 x 17 x 1024\"] --\"lm_head\"--\u003eoutput[\"matrix: 1 x 17 x 151,936\"] classDef default fill: transparent, bg-color: transparent TODO: MERGE PREVIOUS SECTION (THE DEMO TEXT GENERATION) WITH THE CURRENT SECTION TODO: RE-WRITE THE convert FUNCTION TO FIT MY CURRENT NARRATIVE!!!\n","date":"11 November 2025","externalUrl":null,"permalink":"/posts/transformers-pt-2-prev/","section":"Blog","summary":"(it\u0026rsquo;s glorified sequence classification)","title":"ELI5 Transformers (Part 2) - Generation","type":"posts"},{"content":" always has been This is part 2 of the ELI5 transformers series, however you do not need to read Part 1 in order to follow the article. Link to the previous article can be found here\nI\u0026rsquo;m pretty sure you\u0026rsquo;ve already heard about someting like this before: \u0026lsquo;generative LLM is just slightly advanced auto complete\u0026rsquo;. Or, something like \u0026lsquo;all it does is predicting what\u0026rsquo;s the most likely next word with all the previous words given\u0026rsquo;.\nToday I would like to invite you think of text generation in a very different perspective, at least it\u0026rsquo;s the persceptive I found helped me the most: text generation is glorified sequence classification. I\u0026rsquo;m going to walk your way through the text generation process, step-by-step, one word at a time, showing you how text generation actually works.\nFINISH THIS LATER!!\nRecap on transformer architecture # As mentioned in the previous post, AI models as we know today computes data in roughly three stages:\nflowchart LR n1([\"Raw Input\"]) --\u003e n2[\"Embedding\"] n2 --\u003e n3[\"Transformer\"] n3 --\u003e n4[\"Model output\"] n4 --\u003e n5([\"Final output\"]) classDef default fill: transparent, bg-color: transparent Stage 1 converts inputs into vectors so that it\u0026rsquo;s model-readable (Embedding). Stage 2 computes the vector representation of the input. This is the main focus of the previous post (Transformer). Stage 3 converts outputs from the transformer into human-readable, task-specific format (Model output). Stage 1 and 3 are very context-dependent as they are dependent on the type of inputs (text, image, audio etc.,) For image data, this can simply be the RGBA values for each pixel; for text data, this can be a look up table of converting sub-words into matrices. The model that\u0026rsquo;s going to be used in this demo is Qwen3, same as the previous post. It\u0026rsquo;s a very tiny text generation model that I feel performs very well. Here\u0026rsquo;s huggingface\u0026rsquo;s link to the model: Qwen/Qwen3-0.6B text-generation 802 7.059495e\u0026#43;06 Setup # Before we start, make sure you installed all the dependencies. Open termianl and type these codes to install depencies:\npip install torch transformers tokenizers -U Then in terminal, to start python:\npython ..Or ipython if you installed jupyter notebook previously:\nipython You should see something like this in your terminal, indicating the beginning of a python session:\nPython 3.13.7 | packaged by conda-forge | (main, Sep 3 2025, 14:24:46) [Clang 19.1.7 ] on darwin Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; depencies # Now import dependencies:\nimport torch from transformers import AutoTokenizer, Qwen3ForCausalLM Sidenote:\nAs of November 2025, Pytorch/ Apple still haven\u0026rsquo;t fully fixed the memory leak issue for Apple Silicon devices. As a result, running models with pytorch may gets slower and slower over time, or freeze compeletly. If you were running a Macbook purchased after 2020, I\u0026rsquo;d recomment manually set pytorch device as \u0026lsquo;cpu\u0026rsquo;. Skip this code if you were confident that this memory leak won\u0026rsquo;t happen:\ntorch.set_default_device(\u0026#39;cpu\u0026#39;) model # Finally, load the model:\nDEVICE = torch.get_default_device() CHECKPOINT = \u0026#34;Qwen/Qwen3-0.6B\u0026#34; print(\u0026#34;Using device:\u0026#34;, DEVICE) model = Qwen3ForCausalLM.from_pretrained(CHECKPOINT) model.requires_grad_(False) You are now all set. example input text # Let\u0026rsquo;s start with an example text input:\nexample = \u0026#34;Hi\u0026#34; Step-by-step Text Generation # Tokenisation # Our firtst problem is that our input (example), is str. However, our model does not accept str as input. If you just pass our input to the model, you would get this TypeError:\n\u0026gt;\u0026gt;\u0026gt; model(example) TypeError: embedding(): argument \u0026#39;indices\u0026#39; (position 2) must be Tensor, not str In order to do text-generation, one first need to to convert inputs into machine-readable formats. This conversion process is called tokenisation. Tokenisation is not the main focus of today\u0026rsquo;s post, but as a quick ELI5, you can functionally see tokenisers as glorified look-up tables that converts texts into indices. Model reads these indicies, through it\u0026rsquo;s own internal lookup table, convert them into text matrices.\nIt works like this: flowchart LR ipt([\"input\"]) \u003c--\u003e tkn[\"Tokenizer\"] tkn \u003c--\u003e mdl[\"Model\"] classDef default fill: transparent, bg-color: transparent For example: flowchart LR ipt([\"text: 'cat sits'\"]) \u003c--tokenizer--\u003e tkn[\"tokens: [[8, 10]]\"] tkn --\u003e mdl[\"Model\"] classDef default fill: transparent, bg-color: transparent For huggingface\u0026rsquo;s transformers library, tokenisation is handeled by Tokenizer objects. Every pre-trained text models in transformers library all come with their own paired tokenizers. This is the one used by our model:\ntokenizer = AutoTokenizer.from_pretrained(CHECKPOINT) To tokenize our text:\ntokens = tokenizer(example, return_tensors=\u0026#39;pt\u0026#39;) print(tokens) print(tokens[\u0026#39;input_ids\u0026#39;].shape) You would see our tokenized text input:\n{\u0026#39;input_ids\u0026#39;: tensor([[13048]]), \u0026#39;attention_mask\u0026#39;: tensor([[1]])} torch.Size([1, 1]) The Transformer # Base model \u0026amp; the task head\nModels in the transformers library are assembed by two parts: the transformer itself, task-specific \u0026lsquo;head\u0026rsquo;. Let\u0026rsquo;s have a look at our model architecture by running print(model):\n\u0026gt;\u0026gt;\u0026gt; print(model) Qwen3ForCausalLM( (model): Qwen3Model( (embed_tokens): Embedding(151936, 1024) (layers): ModuleList( (0-27): 28 x Qwen3DecoderLayer( (self_attn): Qwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) (mlp): Qwen3MLP( (gate_proj): Linear(in_features=1024, out_features=3072, bias=False) (up_proj): Linear(in_features=1024, out_features=3072, bias=False) (down_proj): Linear(in_features=3072, out_features=1024, bias=False) (act_fn): SiLUActivation() ) (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) ) ) (norm): Qwen3RMSNorm((1024,), eps=1e-06) (rotary_emb): Qwen3RotaryEmbedding() ) (lm_head): Linear(in_features=1024, out_features=151936, bias=False) ) In output above, our transformer base model is the one called (model): Qwen3Model at the very beginning, the task-head is the lm_head: Linear at the very end. Let\u0026rsquo;s separate them apart:\nbase_model = model.model task_head = model.lm_head print(base_model) shall give you the output save as above except the \u0026rsquo;lm_head\u0026rsquo; bits.\nRunning the transformer\nTo do text generation, we first pass the inputs to the base_model:\noutput = base_model(**tokens) print(output) Which would give you this output:\nBaseModelOutputWithPast(last_hidden_state=tensor([[[ 7.4006, 29.0470, -0.1732, ..., -1.2643, 1.1580, 1.1260]]]), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None) Output of the transformer base_model is the last_hidden_state attribute of the BaseModelOutputWithPast object:\nlast_hidden_state = output.last_hidden_state print(last_hidden_state.shape) Output:\ntorch.Size([1, 1, 1024]) The Task-Head # Tokenisation # Quick sidenote on tokenizer\nOn top of the transformer models themselves, we also need some helper objects in order to convert inputs/ outputs between human-readable and machine-readable formats: our inputs and desirerd outputs are both texts, however models only accepts matrices as their inputs and would only output matrices. This conversation process is called tokenisation. For huggingface transformers library, tokenisation is handeled by obejcts called Tokenizer. Tokenisation is very important process for linguistic research, and there are a lot of different methods to tokenise texts. Tokenisation is not the main focus of today\u0026rsquo;s post, but as a quick ELI5, you can functionally see tokenisers as glorified look-up tables that converts texts into indices. Model reads these indicies, through it\u0026rsquo;s own internal lookup table, convert them into text matrices. Pre-trained text models from transformers library all come with their own paired tokenizers.\nIt works like this: flowchart LR ipt([\"input\"]) \u003c--\u003e tkn[\"Tokenizer\"] tkn \u003c--\u003e mdl[\"Model\"] classDef default fill: transparent, bg-color: transparent For example: flowchart LR ipt([\"text: 'cat sits'\"]) \u003c--tokenizer--\u003e tkn[\"index: [[8, 10]]\"] tkn \u003c--model--\u003e mdl[\"matrix: [[0.3, 0.2, 0.5], [0.1, 0.7, 0.3]]\"] classDef default fill: transparent, bg-color: transparent converter\nIn order to test out the model, we\u0026rsquo;ll first need to prepare a function that converts inputs \u0026amp; outputs back and forth. Copy this function and paste into python:\ndef model_converter(tokenizer, inputs: Union[str, ModelOutput, torch.Tensor]) -\u0026gt; Union[str, BatchEncoding]: \u0026#34;\u0026#34;\u0026#34;Converts between text inputs and token indicies Args: tokenizer (PreTrainedTokenizerFast): the tokenizer to use inputs (Union[str, ModelOutput, torch.Tensor]): text inputs or model outputs \u0026#34;\u0026#34;\u0026#34; # text -\u0026gt; model inputs if isinstance(inputs, str): message = [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: inputs} ] tokens = tokenizer.apply_chat_template( message, tokenize=True, return_dict=True, enable_thinking=False, add_generation_prompt=True, return_tensors=\u0026#39;pt\u0026#39; ) return tokens # in this demo, we will enconter several different types of model outputs... # outputs from model.generate() elif isinstance(inputs, torch.Tensor): if inputs.ndim==1: return tokenizer.decode(inputs) elif inputs.ndim \u0026gt; 1 and inputs.shape[0]==1: return model_converter(tokenizer, inputs[0]) else: return tokenizer.batch_decode(inputs) # outputs from model.forward elif isinstance(inputs, ModelOutput): if \u0026#39;logits\u0026#39; in inputs: sequences = inputs.logits.topk(1, dim=-1).view(1, -1) return model_converter(tokenizer, sequences) raise TypeError(type(inputs)) Copy these codes to initialise the tokenizer: tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT) convert:Callable = partial(model_converter, tokenizer) Model # Copy these codes to initialise the model:\nmodel = AutoModelForCausalLM.from_pretrained(CHECKPOINT) model.generation_config.max_new_tokens = 128 model.requires_grad_(False) You are now all set.\nRunning the model # Before dive into the text generation mechaism, let\u0026rsquo;s try some text generation first. Copy this example text or use whatever other texts you\u0026rsquo;d like to have a try on:\nexample = \u0026#34;Why are you running?\u0026#34; Tokenisation\nFirst, we\u0026rsquo;ll need to convert input text into model-redable form:\ntokens = convert(example) Running print(tokens) would print out something looks like a dictionary with keys like input_ids and attention_mask. input_ids are our main interest, these are our formatted input that gets converted into model-redable matrix. Using convert, we can convert the tokens back into text:\nprint(convert(tokens[\u0026#39;input_ids\u0026#39;])) \u0026hellip;which would give you something like:\n\u0026lt;|im_start|\u0026gt;user Why are you running?\u0026lt;|im_end|\u0026gt; \u0026lt;|im_start|\u0026gt;assistant \u0026lt;think\u0026gt; \u0026lt;/think\u0026gt; \u0026ldquo;\u0026lt;|im_start|\u0026gt;user\u0026rdquo;, \u0026ldquo;\u0026lt;|im_start|\u0026gt;assistant\u0026rdquo;, \u0026ldquo;\u0026lt;think\u0026gt;\u0026rdquo; and \u0026ldquo;\u0026lt;/think\u0026gt;\u0026rdquo; are extra special tokens that manually amended by the model_converter in order to format our inputs into a format that the model was orinially trained with. Formatting input this way would help the model generating better results. Different tokenizers have different set of special tokens. For Qwen3 used here, you can view all special tokens by calling tokenizer.get_added_vocab() to see a list of all theset tokens.\nText Generation\nUse model.generate to run text generation:\noutputs = model.generate(**tokens) outputs You would see that our model returned antoher matrix of token indicies:\ntensor([[151644, 872, 198, 10234, 525, 498, 4303, 30, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271, 40, 2776, 537, 4303, 13, 358, 2776, 1101, 1588, 311, 1492, 498, 448, 697, 4755, 13, 6771, 752, 1414, 1246, 358, 646, 7789, 498, 0, 151645]]) Use convert to convert it back to text:\nprint(convert(outputs)) text generation is a randomised process, therefore your result may be different from mine:\n\u0026lt;|im_start|\u0026gt;user Why are you running?\u0026lt;|im_end|\u0026gt; \u0026lt;|im_start|\u0026gt;assistant \u0026lt;think\u0026gt; \u0026lt;/think\u0026gt; I\u0026#39;m not running. I\u0026#39;m just here to help you with your questions. Let me know how I can assist you!\u0026lt;|im_end|\u0026gt; Text generation, one word at a time # Instead of generating the entire sentence at once, model generates texts one token at a time. We\u0026rsquo;ll first take a look at how model generates one word from the input text, and look at the entire text generation process in the next section. For a quick recap on the model architecture, please refer to the previous post: here\u0026rsquo;s the link. Text models in the transformers library are assembed by two parts, the XXXModel itself and task-specific xx_head. The XXXModel refers to the transformer model itself, and xx_head is a converter that converts transformer outputs according to different tasks, usually called task head.\nLet\u0026rsquo;s have a look at our model by running print(model):\n\u0026gt;\u0026gt;\u0026gt; print(model) Qwen3ForCausalLM( (model): Qwen3Model( (embed_tokens): Embedding(151936, 1024) (layers): ModuleList( (0-27): 28 x Qwen3DecoderLayer( (self_attn): Qwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) (mlp): Qwen3MLP( (gate_proj): Linear(in_features=1024, out_features=3072, bias=False) (up_proj): Linear(in_features=1024, out_features=3072, bias=False) (down_proj): Linear(in_features=3072, out_features=1024, bias=False) (act_fn): SiLU() ) (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) ) ) (norm): Qwen3RMSNorm((1024,), eps=1e-06) (rotary_emb): Qwen3RotaryEmbedding() ) (lm_head): Linear(in_features=1024, out_features=151936, bias=False) ) In the outputs shown above, (model) is our transformer, and (lm_head) is our text-generation head. To demonstrate, let\u0026rsquo;s use the example mentioned in the previous section:\n# convert texts into tokens example = \u0026#34;Why are you running?\u0026#34; tokens = convert(example) # pass tokens to the transformer transformed = model.model.forward(**tokens) The raw output from transformers models contains not only the final output, but also computation results inbetween. These inbetween data are very useful for anyone who\u0026rsquo;s interested in figuring out behaviours of their model. For now we are only interested in the final output, last_hidden_state. Let\u0026rsquo;s take a look:\nprint(\u0026#39;Shape of the input matrix:\u0026#39;, tokens[\u0026#39;input_ids\u0026#39;].shape) print(\u0026#39;Shape of the output matrix:\u0026#39;, transformed.last_hidden_state.shape) print(\u0026#39;Output:\\n\u0026#39;, transformed.last_hidden_state) Output:\nShape of the input matrix: torch.Size([1, 17]) Shape of the output matrix: torch.Size([1, 17, 1024]) Output: tensor([[[ 5.1596, 19.8593, -0.2142, ..., -0.9417, 0.8061, 0.8258], [ 1.6308, 26.9123, -1.1482, ..., -3.7286, -3.3529, -0.7485], [ -0.9393, 7.0810, -1.4507, ..., -0.8947, -1.6471, -2.8123], ..., [ 4.9579, -29.7081, 0.0470, ..., 0.9277, -2.9034, 0.9229], [ 3.7464, -7.0461, 0.1408, ..., 4.0884, -1.4918, -1.3324], [ -0.5219, 12.8126, -1.1671, ..., 2.8096, 0.8872, -0.3728]]]) We then pass the outputs of our transformer to the task head, lm_hed:\nlogits = model.lm_head(transformed.last_hidden_state) print(\u0026#34;Shape of the final output matrix:\u0026#34;, logits.shape) Output:\nShape of the final output: torch.Size([1, 17, 151936]) As you can see, through the computation, our text inputs gets converted into a matrix of indicies of size 1 x 17, then gets transformed into a matrix of size 1 x 17 x 1024, and then gets converted into a (very big!) matrix of size 1 x 17 x 151,936:\nflowchart TB text([\"text\"])--tokenizer--\u003eipt[\"matrix: 1 x 17\"] --\"model\"--\u003e tfm[\"matrix: 1 x 17 x 1024\"] --\"lm_head\"--\u003eoutput[\"matrix: 1 x 17 x 151,936\"] classDef default fill: transparent, bg-color: transparent TODO: MERGE PREVIOUS SECTION (THE DEMO TEXT GENERATION) WITH THE CURRENT SECTION TODO: RE-WRITE THE convert FUNCTION TO FIT MY CURRENT NARRATIVE!!!\n","date":"11 November 2025","externalUrl":null,"permalink":"/posts/transformers-pt-2/","section":"Blog","summary":"(it\u0026rsquo;s glorified sequence classification)","title":"ELI5 Transformers (Part 2) - Generation","type":"posts"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/","section":"Myxiniformes Moment","summary":"","title":"Myxiniformes Moment","type":"page"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" ","date":"11 November 2025","externalUrl":null,"permalink":"/topics/","section":"Topics","summary":"","title":"Topics","type":"topics"},{"content":"As someone without much backrounds in neither physics nor computer science, I find lots of available introductions on transformers very confusing. However, most of the articles on transformers focuses on attention mechanisms, using either the OG transformer or the classic BERT as examples. There are a lot of very good learning materials out there, for example the amazing interactive transofmer explainer.\nFinally, I\u0026rsquo;ve decided to bite the bullet and spend some time have a read through the HuggingFace\u0026rsquo;s source code for many of these models, like this one. I took lots of notes here and there during the process of studying transformers. This series of blogposts collects my notes and ELI5s of what I\u0026rsquo;ve learned, hope these notes can help others alongside their studying, or being interesting little pieces of articles to read through.\n","date":"11 November 2025","externalUrl":null,"permalink":"/topics/transformers/","section":"Topics","summary":"","title":"Transformer","type":"topics"},{"content":" In this particular post, I would like to do a very brief overview of the transformer model architecture, specifically on the attention mechanism. I won\u0026rsquo;t go into too much math and there won\u0026rsquo;t be any mathematical formulas. However, I would assume readers of this silly little post already have some okay-ish background in math/datascience. (i.e., matrix computations, embeddings, tokens, model fitting etc.). I am not going to list out all the implementation details for transformers, since there are a lot of very good materials out there and they are doing fantastic jobs. Instead, in this (maybe series of?) post, I would like to draw out a general framework on transformers to help one understand the detailed math behind.\nToday, I\u0026rsquo;ll very quickly go through some very basics on neural network model, just enough to cover what needed for this post, accompied by demo of transformer model as a proof of concept. In this section, there will be some codes that you can copy and paste into an interactive python session to fiddle around for a bit. And lastly, I\u0026rsquo;ll do a quick sketch on the general architecture of transformers, and a overview of the attention mechasm.\nModel only needs to be useful # In order to make things easier to understand, I would wish to start with an inaccuate premise: we can view neural network models as functions that takes some sort of matrix as inputs, do some sort of matrix computations, and output another matrix as the final result. What makes one neural network different from others is how the computation is carried out. It\u0026rsquo;s like \\(y=a \\times x^2\\) is a different function from \\(y=a \\times sin(x)\\), only that in the case of neural network, both x and y are matrices, and the math is much complicated. When it comes to model training, we are essentially trying to find values gives best fit to the data.\nThere\u0026rsquo;s an important assumption here: just because a model fits the data well does not mean the model describes the mechanisms behind the data. For example, we definitely can fit \\(y=a \\times sin(x) + b\\) to a normal distribution data (like distribution of customer spendings in McDonald\u0026rsquo;s), and it\u0026rsquo;s probably going to be a pretty good fit, but this does not mean the sine function has anything to do with explaining the normal distribution. A good model does not always need to be a description; a good model just needs to be useful for its purpose.\nTransformers are precisely these kinds of models: they are, surprisingly good at fitting into all sorts of data whilst the math behind the model probably doesn\u0026rsquo;t have much to do with the mechanisms behind. We don\u0026rsquo;t know how transformers work so well for text-based tasks. At least not yet. Originally, transformer was designed as an add-on to text-processing neural network models in order to tackle some tricky problems (these problems are not the main focus of the current blogpost so I\u0026rsquo;m skipping them, but here\u0026rsquo;s a good article if you were interested). We just happened to discover that transformers alone is good enough to solve these problems; we just need to make the transformers much bigger. So that\u0026rsquo;s where the AI boom started: GPT2 solved issues in GPT1 by simply being 10 times bigger; the most-recently open-sourced pretrained GPT-OSS is 200 times bigger than the previous open-sourced model, GPT2 (note: GPT-OSS is structurally different from the original GPT2 but the fundamental ideas are the same.) There are even speculations suggesting transformer neural network models can be seen as some sort of universal function approximator. That is, it\u0026rsquo;s capable of \u0026lsquo;approximating\u0026rsquo; other formulas/functions with a certain degree of accuracy, providing the model itself is big enough (\u0026lsquo;universal approximation theorem\u0026rsquo;). Transformer Model Architecture # Overview # At conceptual level, the general idea behind transformer models is actually pretty intuitive. We can roughly divide the model calculations into three stages: Stage 1 Embedding The input gets converted into vectors or matrices. The first step starts with creating a mathematical representation of our input data. This process can vary based on different types of inputs. It can simply be some sort of look-up tables (text embedding), some matrix transformations of the raw inputs (convolution) etc. Stage 2 Transformer The raw output from Stage 1 feeds into the multiple different attention layers. Mathematically, each attention layer is doing very much the same mathematical operation, with each layer having its own sets of parameters. Each layer takes a matrix as an input, and outputs another matrix to pass onto the next layer. This process is repeated multiple times. Stage 2 is the core of a transformer model, it *transforms* our inputs into something else. Stage 3 Output We convert the matrix output from Stage 2 into task-specific results. This is usually done by another set of simple matrix operations, depending on the task. For example, if we are doing text sentimental analysis task, this operation could be a simple matrix multiplication, resulting in a final score of 0-10. In other words, you can conceptually see Stage 1 as a conversion stage in order to initiate the model, Stage 2 being the core of a transformer model, and Stage 3 as a \u0026lsquo;decoding\u0026rsquo; step to convert the output back into human-readable form. When we talk about transformer models, we are mostly referring to Stage 2, which is the focus of the current post. I\u0026rsquo;ll elaborate a lot more on what\u0026rsquo;s happening in Stage 3 in the next post.\nThe Transformer Itself # The transformer itself is pretty straightforward: it consists of stacks of multiple attention layers that often share exactly the same, or very similar structure: --- config: layout: dagre --- flowchart LR n1[\"Input\"] --\u003e n2[\"Attention\nLayer\"] n2 --\u003e n3[\"Attention\nLayer\"] n3 --\u003e n6[\"Repeat\"] n6 --\u003e n5[\"Output\"] n2@{ shape: rounded} n3@{ shape: rounded} n6@{ shape: text} classDef default fill: transparent, bg-color: transparent Asttention layer consists of multiple \u0026lsquo;attention heads\u0026rsquo; that works in parallel: each attention head processes the inputs independently, and the output of all attention heads are merged back together. The joined matrix is the final output of the current layer: flowchart LR subgraph s1[\"Attention Heads\"] direction LR n14[\"Attention Head\"] n16[\"Attention Head\"] n17[\"Attention Head\"] end s1 --\u003e n10[\"Merge\"] n10 --\u003e n18[\"Next Layer\"] n19[\"Previous Layer\"] --\u003e s1 n18@{ shape: text} n19@{ shape: text} classDef default fill: transparent style s1 fill:transparent The attention head # You can think each of the attention head as a mini neural network. A typical attention head works like this:\nflowchart TB IN[\"Previous Layer\"] --\u003e Q[\"Matrix 1\"] \u0026 K[\"Matrix 2\"] \u0026 V[\"Matrix 3\"] Q ---\u003e SDPA[\"Matrix 1 \u0026 2\"] K ---\u003e SDPA SDPA --\u003e n1[\"Output\"] V ---\u003e n1 classDef default fill: transparent, bg-color: transparent Step 1: The input matrix gets converted into multiple matices through matrix multiplication. Most current transformers converts input matrix into three smaller matrices. Step 2: Two of the matrix from step (1) gets combined together using some matrix operation, usually dot products. Step 3: The third matrix from step (1) combines with output from step (2), using some other matrix operation. Example: Qwen3 # We\u0026rsquo;ll now take a look at an actual transformer model and see how it works in action. It\u0026rsquo;s very suprising is that, transformers are able to produce pretty impressive results for tasks model that are not specifically trained for. Here, we\u0026rsquo;ll use Qwen3, a small-sized text generation model as a demo. It\u0026rsquo;s tiny (~1.5GB) but the performance is VERY impressive for its size.\nHere\u0026rsquo;s huggingface\u0026rsquo;s link to the model: Qwen/Qwen3-0.6B text-generation 802 7.059495e\u0026#43;06 Prep # Make sure you have python pre-installed. If you were using windows, python can be downloaded from the Microsoft Store. If you were Mac/ Linux Mint/ Ubuntu/ Debian etc., your computer should already come with python by default. For this demo, we would need python version \\( \\geqslant \\) 3.12.\nWe first need to install some dependencies. Open terminal/ command prompt, type this command to install required dependencies:\npip install torch transformers And then type:\npython To open python and start an interactive python session.\nAlternatively, if you installed ipython, which should already be installed if you have installed jupyter notebook previously, you can open ipython instead:\nipython Once python was opened, copy and paste these lines into python to import required packages:\nfrom pprint import pprint import re from transformers import AutoModelForCausalLM, AutoTokenizer, BatchEncoding Get the model # Models from transformers library takes matrices as inputs, and outputs matrices. Thus, text generation models needs to be paired with tokenizer in order to convert inputs into model-readable formant, and convert model outputs into human-readble format. transformers library has a special class called TextGenerationPipeline to handle this conversion class, but would be a bit too complicated for our purpose. Here, I modified demo code from Qwen3\u0026rsquo;s demo code that simply combines tokenizers and models together. Here\u0026rsquo;s the code for you to copy and paste into the currently running python session:\nclass DemoChatbot: \u0026#34;\u0026#34;\u0026#34; A simple demo chatbot, code modified from https://huggingface.co/Qwen/Qwen3-0.6B \u0026#34;\u0026#34;\u0026#34; def __init__(self, model_name:str=\u0026#34;Qwen/Qwen3-0.6B\u0026#34;): self.tokenizer = AutoTokenizer.from_pretrained(model_name) self.enable_thinking = False self._thiniking_regex = r\u0026#34;\u0026lt;think\u0026gt;(.*?)\u0026lt;/think\u0026gt;\u0026#34; self.model = AutoModelForCausalLM.from_pretrained( model_name, ) self.history = [] def clear_history(self) -\u0026gt; None: self.history = [] def tokenise(self, user_input:str, enable_thinking=None) -\u0026gt; BatchEncoding: if enable_thinking is None: enable_thinking = self.enable_thinking # default value messages = self.history + [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}] tokens = self.tokenizer.apply_chat_template( messages, tokenize=True, enable_thinking=enable_thinking, add_generation_prompt=True, return_tensors=\u0026#34;pt\u0026#34; ) return tokens def __call__(self, user_input, enable_thinking=None) -\u0026gt; None: inputs = self.tokenise(user_input, enable_thinking=enable_thinking) response_ids = self.model.generate( **inputs, max_new_tokens=2048, ) response_ids = response_ids[0][len(inputs.input_ids[0]):].tolist() response = self.tokenizer.decode(response_ids, skip_special_tokens=True) # since we aren\u0026#39;t going to do anything with the output, # just prints out the response and save it to the chat history. print(response) # Update history response_history = re.sub(self._thiniking_regex, \u0026#34;\u0026#34;, response, flags=re.DOTALL).strip() self.history.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}) self.history.append({\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: response_history}) Then, create a new chat instance with the code below:\nchat = DemoChatbot() You will see a progress bar showing download status. Once completed, type the following to have a look at the QWen3 model structure:\npprint(chat.model) which in term will give you this output:\nQwen3ForCausalLM( (model): Qwen3Model( (embed_tokens): Embedding(151936, 1024) (layers): ModuleList( (0-27): 28 x Qwen3DecoderLayer( (self_attn): Qwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) (mlp): Qwen3MLP( (gate_proj): Linear(in_features=1024, out_features=3072, bias=False) (up_proj): Linear(in_features=1024, out_features=3072, bias=False) (down_proj): Linear(in_features=3072, out_features=1024, bias=False) (act_fn): SiLUActivation() ) (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) ) ) (norm): Qwen3RMSNorm((1024,), eps=1e-06) (rotary_emb): Qwen3RotaryEmbedding() ) (lm_head): Linear(in_features=1024, out_features=151936, bias=False) ) Model Architecture # The very core of a transformer model can be checked by:\npprint(chat.model.model) And you will get an output similar to the previous output, but without lm_head at the end:\nQwen3Model( (embed_tokens): Embedding(151936, 1024) (layers): ModuleList( (0-27): 28 x Qwen3DecoderLayer( (self_attn): Qwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) (mlp): Qwen3MLP( (gate_proj): Linear(in_features=1024, out_features=3072, bias=False) (up_proj): Linear(in_features=1024, out_features=3072, bias=False) (down_proj): Linear(in_features=3072, out_features=1024, bias=False) (act_fn): SiLUActivation() ) (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) ) ) (norm): Qwen3RMSNorm((1024,), eps=1e-06) (rotary_emb): Qwen3RotaryEmbedding() ) The embed_tokens is the Stage 1 mentioned earlier, and the lm_head is the Stage 3. The core of a transformer, is what we\u0026rsquo;ve been discussing today.\nLet\u0026rsquo;s have a look at one of the layers inside the transformer:\nlayer = chat.model.model.layers[0] pprint(layer) You will see something like this:\nQwen3DecoderLayer( (self_attn): Qwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) (mlp): Qwen3MLP( (gate_proj): Linear(in_features=1024, out_features=3072, bias=False) (up_proj): Linear(in_features=1024, out_features=3072, bias=False) (down_proj): Linear(in_features=3072, out_features=1024, bias=False) (act_fn): SiLUActivation() ) (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) ) Layer is divided into 2 major sections, just like we discussed previously:\nself_attn is where attention gets calculated. All the rest (i.e., mlp, input_layernorm and post_attention_layernorm) are step-by-step computations of combining and averaging the attention heads. Run the following code to see the structure of an attention head:\nattention = layer.self_attn pprint(attention) The q_proj, k_proj and v_proj are the three matrices mentioned earlier:\nQwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) The actual impelmentations of attention heads are different from model to model, but the general principle behind should be roughly the same. I\u0026rsquo;ll dive deeper into it in the future.\n// \u0026hellip;existing code\u0026hellip;\nHave some fun! # Meanwhile, since we\u0026rsquo;ve got a generative model already, we might as well test out some text generations:\nchat(\u0026#34;Why is the content, which was held to be true in perceiving, in fact only belongs to the form, and it dissolves into the forms unity ??\u0026#34;) Or, you can turn on the \u0026rsquo;thinking\u0026rsquo; mode to enable chain-of-thought:\nchat.enable_thinking = True chat(\u0026#34;But...but the phenomenology 147 says the inner, essential is essentially the truth of appearance I\u0026#39;m absolutely fewming\u0026#34;) If it takes too long to run, you can clear chat history to remove cached chats:\nchat.clear_history() \u0026lsquo;LLM is magic\u0026rsquo; I wanted to point out the (maybe) obvious thing here: almost all operations mentioned contain learnable parameters. Inside individual attention heads, the three matrices converted from inputs are typically created by multiplying (\u0026lsquo;dot product\u0026rsquo;) inputs with three separate matrices. These matrices are part of the learnable parameters for the attention head. When we combine matrices, the combination operation also has their own learnable parameters. Furthermore, when we combine outputs from each \u0026lsquo;attention head\u0026rsquo;, this combining operation also has its own set of trainable parameters, so on and so on\u0026hellip; Almost every stage of the matrix computations are parameterized, resulting in the unbelievably massive AI models as of today. However, the model used in the demo today, despite its size, is still able to produce long and very coherent responses. Yes, in a sense, transformers are just a huge stack of matrix calculations. However, we know very little about the reason behind it. We gave names to these matrices, but we don\u0026rsquo;t know much about their behavior.\nFinal thoughts # Transformer neural network models aren\u0026rsquo;t as mysterious as one might think. It has no difference compared with any other functions. At the end of the day it\u0026rsquo;s just another mathematical function, but takes matrices as inputs, does matrix calculations, and outputs matrices. Just like any other neural network models, it takes multiple steps to do the calculation. Within each computational step, the inputs get processed by three mini-neural networks, and outputs are re-combined together as the output of the current step. In this sense, transformers are like nested neural networks: they are bigger neural networks that contain lots of mini neural networks.\nWhat\u0026rsquo;s up next?\nThere are still a bit more stuffs that I\u0026rsquo;d like to share, like how text-generation works and what\u0026rsquo;s really happenning when we are training a generative model. We\u0026rsquo;ve heard of the same old things over and over: \u0026lsquo;generative LLM is just a very massive auto-complete!\u0026rsquo;. Whilst I do aggree with it, I also find it not helpful if one wants to understand text generation, for both model training and model inference. In the next post, we\u0026rsquo;ll have a look at the Stage 3 for text generation.\nSome Other Resources # I hope whoever comes across this post would find it useful. Here are some extra reading materials that I found particularly useful:\nPytorch\u0026rsquo;s step-byp-step guide on creating a generative LLM is prob one of the best out there that teaches you all the fundenmentals. BertViz, a very good visualisation tool for looking at attention heads layer by layer. You can run it interactively in a jupyter notebook. Huggingface\u0026rsquo;s LLM cources. Although they tends to focus on the side of programming \u0026amp; practical applications, I found many of their conceptual guides very good for a beginner. ","date":"31 October 2025","externalUrl":null,"permalink":"/posts/transformers-pt-1/","section":"Blog","summary":"(it\u0026rsquo;s glorified linear algebra)","title":"ELI5 Transformers (Part 1): Attention Mechanism ","type":"posts"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]
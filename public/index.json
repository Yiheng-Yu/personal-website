
[{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/posts/","section":"Blog","summary":"","title":"Blog","type":"posts"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/tags/eli5/","section":"Tags","summary":"","title":"ELI5","type":"tags"},{"content":" always has been This is part 2 of the ELI5 transformers series, however you do not need to read Part 1 in order to follow the article. Link to the previous article can be found here\nI\u0026rsquo;m pretty sure you\u0026rsquo;ve already heard about someting like this before: \u0026lsquo;generative LLM is just slightly advanced auto complete\u0026rsquo;. Or, something like \u0026lsquo;all it does is predicting what\u0026rsquo;s the most likely next word with all the previous words given\u0026rsquo;.\nToday I would like to invite you think of text generation in a very different perspective, at least it\u0026rsquo;s the persceptive I found helped me the most: text generation is glorified sequence classification. I\u0026rsquo;m going to walk you through the mechanism of text generation with the source code, and show you how text generation actually works.\nIn this post, you will:\nsee the step-by-step process of how texts are generated Recap on transformer architecture # As mentioned in the previous post, AI models as we know today computes data in roughly three stages:\nflowchart n1([\"Raw Input\"]) --\u003e n2[\"Embedding\"] n2 --\u003e n3[\"Transformer\"] n3 --\u003e n4[\"Model output\"] n4 --\u003e n5([\"Final output\"]) Stage 1 converts inputs into vectors so that it\u0026rsquo;s model-readable (Embedding). Stage 2 computes the vector representation of the input. The previous post gave a rough overview of this stage(Transformer). Stage 3 converts outputs from the transformer into human-readable, task-specific format (Model output). Stage 1 and 3 are very context-dependent as they are dependent on the type of inputs (text, image, audio etc.,) For image data, this can simply be the RGBA values for each pixel; for text data, this can be a look up table of converting sub-words into matrices. Preparation # Download the model # Open termianl and type-in these codes to install depencies in case you haven\u0026rsquo;t done so:\npip install transformers Then in terminal, type \u0026lsquo;python\u0026rsquo; to start python interactive REPL:\npython You should see something like this in your terminal:\nPython 3.13.7 | packaged by conda-forge | (main, Sep 3 2025, 14:24:46) [Clang 19.1.7 ] on darwin Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; Import dependencies:\nimport torch from transformers import AutoTokenizer, AutoModelForCausalLM As of November 2025, Pytorch/ Apple still haven\u0026rsquo;t fixed the memory leak issue on Apple Silicon devices (i.e., post-2020 Macbooks). As a result, running models with pytorch for some period of time will gets slower and slower over time. Just for demonstration purpose, I\u0026rsquo;d recomment manually set pytorch device as \u0026lsquo;cpu\u0026rsquo; because of this. Skip this step if you were confident this won\u0026rsquo;t happen. Since we are just doing demonstrations, we can simly set torch device as \u0026lsquo;cpu\u0026rsquo;:\ntorch.set_default_device(\u0026#39;cpu\u0026#39;) # or \u0026#39;cuda\u0026#39; if you\u0026#39;d like to use GPU, would not recommend \u0026#39;mps\u0026#39; (at least for torch\u0026lt;=2.9.1) Download the model:\ncheckpoint = \u0026#34;Qwen/Qwen3-0.6B\u0026#34; device = torch.get_default_device() tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForCausalLM.from_pretrained(checkpoint, device=device) The openai GPT2 model released on Huggingface doesn\u0026rsquo;t come with some pretty import settings. We\u0026rsquo;ll need to manually amend them first:\npipe.generation_config.pad_token_id = pipe.tokenizer.eos_token_id pipe.generation_config.bos_token_id = pipe.tokenizer.eos_token_id pipe.generation_config.decoder_start_token_id = pipe.tokenizer.eos_token_id You are now all set. Optional: Testing text generation # We\u0026rsquo;ll first test the text-generation model. Here\u0026rsquo;s a little function to help with text generation. Basically, instead of returning the raw output (list of dictionaries), this function extracts the generated text and prints it directly, just for easier reading.\ndef generate(text:str, **generate_kwargs) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Run text generation pipeline and print out the generated text. \u0026#34;\u0026#34;\u0026#34; result = pipe(text, **generate_kwargs) print(result[0][\u0026#39;generated_text\u0026#39;]) \u0026hellip;and now you can use this function to try out text generation yourself:\ngenerate(\u0026#34;Who\u0026#39;s that Pokemon!?!?\u0026#34;, max_new_tokens=5) Since we\u0026rsquo;ve set do_sample=True in our pipeline, text generation is done through random-sampling. Model would generate slightly different answer everytime we run our generation function.\nLet\u0026rsquo;s run 5 of them:\nfor _ in range(5): generate(\u0026#34;Who\u0026#39;s that Pokemon!?!?\u0026#34;, max_new_tokens=5) Your output would be very different from mine:\nWho\u0026#39;s that Pokemon? Hahaâ€¦ This one Who\u0026#39;s that Pokemon? I can\u0026#39;t really recall Who\u0026#39;s that Pokemon? It\u0026#39;ll have a head Who\u0026#39;s that Pokemon? I\u0026#39;m going to catch Who\u0026#39;s that Pokemon? The first week has You can generate much longer text by setting \u0026lsquo;max_new_tokens\u0026rsquo; to a higher number, note that the model basically generates typical AÄ° slops when it\u0026rsquo;s too large:\ngenerate(\u0026#34;Who\u0026#39;s that Pokemon!?!?\u0026#34;, max_new_tokens=1024) 1 - Inputs \u0026lt;-\u0026gt; outputs # 2 - \u0026lsquo;Task Head\u0026rsquo; # Model generation vs text classification # (Here is where I write about LM head)\nPopular text generation strategies # é¢„å‘Šï¼Ÿï¼Ÿï¼Ÿparallel training # ","date":"11 November 2025","externalUrl":null,"permalink":"/posts/transformers-pt-2/","section":"Blog","summary":"(it\u0026rsquo;s glorified sequence classification)","title":"ELI5 Transformers (Part 2) - Generation","type":"posts"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/","section":"Myxiniformes Moment","summary":"","title":"Myxiniformes Moment","type":"page"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/topics/","section":"Topics","summary":"","title":"Topics","type":"topics"},{"content":"As someone without much backrounds in neither physics nor computer science, I find lots of available introductions on transformers very confusing. However, most of the articles on transformers focuses on attention mechanisms, using either the OG transformer or the classic BERT as examples. There are a lot of very good learning materials out there, for example the amazing interactive transofmer explainer.\nFinally, I\u0026rsquo;ve decided to bite the bullet and spend some time have a read through the HuggingFace\u0026rsquo;s source code for many of these models, like this one. I took lots of notes here and there during the process of studying transformers. This series of blogposts collects my notes and ELI5s of what I\u0026rsquo;ve learned, hope these notes can help others alongside their studying, or being interesting little pieces of articles to read through.\n","date":"11 November 2025","externalUrl":null,"permalink":"/topics/transformers/","section":"Topics","summary":"","title":"Transformer","type":"topics"},{"content":" In this particular post, I would like to do a very brief overview of the transformer model architecture, specifically on the attention mechanism. I won\u0026rsquo;t go metion too much math and there won\u0026rsquo;t be any mathenathical formulas. However, I would assume readers of this silly little post already have some okay-ish background of math/ datascience. (i.e., matrix computations embeddings, tokens, model fitting etc.). I am not going to list out all the implemention details for transformers, since there are a lot of very good materials out there and they are doing fantastic jobs. Instead, in this (maybe series of?) post, Ä° would like to draw out a general framework on transformers to help one understand the detailed math behind.\nToday, I\u0026rsquo;ll very quickly go through some very basics on neural network model, just enough to cover what needed for this post, accompied by demo of transformer model as a proof of concept. In this section, there will be some codes that you can copy and paste into an interactive python session to fiddle around for a bit. And lastly, I\u0026rsquo;ll do a quick sketch on the general architecture of transformers, and a overview of the attention mechasm.\nModel only needs to be useful # In order to make things easier to understand, I would wish to start with an inaccuate premise: we can view neural network models as functions that takes some sort of matrix as inputs, do some sort of matrix computations, and output another matrix as the final result. What makes one neural network different from others is how the computation is carried out. It\u0026rsquo;s like \\(y=a \\times x^2\\) is a different function from \\(y=a \\times sin(x)\\), only that in the case of neural network, both x and y are matrices, and the math is much complicated. When it comes to model training, we are essentially trying to find values gives best fit to the data.\nThere\u0026rsquo;s an important assumption here: just because model fits the data well does not mean the model describes mechanisms behind the data. For example, we definitely can fit \\(y=a \\times sin(x) + b\\) to a normal distribution data (like distribution of customer spendings in McDonald\u0026rsquo;s), and it\u0026rsquo;s prob going to be a pretty good fit, but this does not mean the sine function has anything to do with explaining the normal distribution. A good model does not always need to be description, a good model just needs to be useful for its purpose.\nTransformers are preciesly these kinds of models: they are, surprisingly good at fitting into all sorts of data whilst the math behind the model probably doesn\u0026rsquo;t have much to do with the mechanisms behind. We don\u0026rsquo;t know how transformers works so well for text-based tasks. At least not yet. Originally, transformer was designed as an add-on to the text-processing neural network models in order to tackle with some tricky problems (these problems are not the main forcus of the current blogpost so I\u0026rsquo;m skipping them, but here\u0026rsquo;s a good article if you were interested). We just happened to discover that transformers alone is good enough to solve these problems, we just need to make the transformers much bigger. So that\u0026rsquo;s where the AÄ° bloom started: GPT2 solved issues in GPT1 by simply being 10 times bigger; the most-recently open-sourced pretrained GPT-Oss, is 200 times bigger than the previous openpsourced model, GPT2 (note: GPT-OSS is structurlly different from the original GPT2 but the fundamental ideas are the same.). There are even speculations suggesting transformer neural network models can be seen as some sort of universal function approximator. That is, it\u0026rsquo;s capacable of \u0026lsquo;approximate\u0026rsquo; other formulas/ functions with certain degree of accuracy, providing the model itself is big enough (\u0026lsquo;universal approximation theorem\u0026rsquo;). Transformer Model Architecture # Overview # At conceptual level, the general idea behind transformer models are is actually pretty intuitive. We can roughly divide the model calclations into three stages: Stage 1 Embedding The input gets converted into vectors or matrices. The first step starts with creating a mathematical representation of our input data. This process can vary based on different types of inputs. It can simply be some sort of look-up tables (text embedding), some matrix transformations of the raw inputs (convolution) etc. Stage 2 Transformer The raw output from Stage 1 feeds into the multiple different attention layers. Mathematically, each attention layer is doing very much the same mathematical operation, with each layer having its own sets of parameters. Each layer takes a matrix as an input, and outputs another matrix to pass onto the next layer. This process is repeated multiple times. Stage 2 is the core of a transformer model, it *transforms* our inputs into something else. Stage 3 Output We convert the matrix output from Stage 2 into task-specific results. This is usually done by another set of simple matrix operations, depending on the task. For example, if we are doing text sentimental analysis task, this operation could be a simple matrix multiplication, resulting in a final score of 0-10. In other words, you can concepturally see Stage 1 as a conversion stage in order to initiate the model, Stage 2 being the core of a transformer model, and Stage 3 as a \u0026lsquo;decoding\u0026rsquo; step to convert the output back into human-readable form. When we talk about transformer models, we are mostly referring to Stage 2, which is the focus of the current post. I\u0026rsquo;ll elabrate a lot more on what\u0026rsquo;s hapenning in Stage 3 in the next post.\nThe Transformer Itself # The transformer itself is pretty straightforward: it consists of stacks of multiple attention layers that often share exactly the same, or very similar structure: --- config: layout: dagre --- flowchart LR n1[\"Input\"] --\u003e n2[\"Attention\nLayer\"] n2 --\u003e n3[\"Attention\nLayer\"] n3 --\u003e n6[\"Repeat\"] n6 --\u003e n5[\"Output\"] n2@{ shape: rounded} n3@{ shape: rounded} n6@{ shape: text} classDef default fill: transparent, bg-color: transparent Asttention layer consists of multiple \u0026lsquo;attention heads\u0026rsquo; that works in parallel: each attention head processes the inputs independently, and the output of all attention heads are merged back together. The joined matrix is the final output of the current layer: flowchart LR subgraph s1[\"Attention Heads\"] direction LR n14[\"Attention Head\"] n16[\"Attention Head\"] n17[\"Attention Head\"] end s1 --\u003e n10[\"Merge\"] n10 --\u003e n18[\"Next Layer\"] n19[\"Previous Layer\"] --\u003e s1 n18@{ shape: text} n19@{ shape: text} classDef default fill: transparent style s1 fill:transparent The attention head # You can think each of the attention head as a mini neural network. A typical attention head works like this:\nflowchart TB IN[\"Previous Layer\"] --\u003e Q[\"Matrix 1\"] \u0026 K[\"Matrix 2\"] \u0026 V[\"Matrix 3\"] Q ---\u003e SDPA[\"Matrix 1 \u0026 2\"] K ---\u003e SDPA SDPA --\u003e n1[\"Output\"] V ---\u003e n1 classDef default fill: transparent, bg-color: transparent Step 1: The input matrix gets converted into multiple matices through matrix multiplication. Most current transformers converts input matrix into three smaller matrices. Step 2: Two of the matrix from step (1) gets combined together using some matrix operation, usually dot products. Step 3: The third matrix from step (1) combines with output from step (2), using some other matrix operation. Example: Qwen3 # We\u0026rsquo;ll now take a look at an actual transformer model and see how it works in action. It\u0026rsquo;s very suprising is that, transformers are able to produce pretty impressive results for tasks model that are not specifically trained for. Here, we\u0026rsquo;ll use Qwen3, a small-sized text generation model as a demo. It\u0026rsquo;s tiny (~1.5GB) but the performance is VERY impressive for its size.\nHere\u0026rsquo;s huggingface\u0026rsquo;s link to the model: Qwen/Qwen3-0.6B text-generation 802 7.059495e\u0026#43;06 Prep # Make sure you have python pre-installed. If you were using windows, python can be downloaded from the Microsoft Store. If you were Mac/ Linux Mint/ Ubuntu/ Debian etc., your computer should already come with python by default. For this demo, we would need python version \\( \\geqslant \\) 3.12.\nWe first need to install some dependencies. Open terminal/ command prompt, type this command to install required dependencies:\npip install torch transformers And then type:\npython To open python and start an interactive python session.\nAlternatively, if you installed ipython, which should already be installed if you have installed jupyter notebook previously, you can open ipython instead:\nipython Once python was opened, copy and paste these lines into python to import required packages:\nfrom pprint import pprint import re from transformers import AutoModelForCausalLM, AutoTokenizer, BatchEncoding Get the model # Models from transformers library takes matrices as inputs, and outputs matrices. Thus, text generation models needs to be paired with tokenizer in order to convert inputs into model-readable formant, and convert model outputs into human-readble format. transformers library has a special class called TextGenerationPipeline to handle this conversion class, but would be a bit too complicated for our purpose. Here, I modified demo code from Qwen3\u0026rsquo;s demo code that simply combines tokenizers and models together. Here\u0026rsquo;s the code for you to copy and paste into the currently running python session:\nclass DemoChatbot: \u0026#34;\u0026#34;\u0026#34; A simple demo chatbot, code modified from https://huggingface.co/Qwen/Qwen3-0.6B \u0026#34;\u0026#34;\u0026#34; def __init__(self, model_name:str=\u0026#34;Qwen/Qwen3-0.6B\u0026#34;): self.tokenizer = AutoTokenizer.from_pretrained(model_name) self.enable_thinking = False self._thiniking_regex = r\u0026#34;\u0026lt;think\u0026gt;(.*?)\u0026lt;/think\u0026gt;\u0026#34; self.model = AutoModelForCausalLM.from_pretrained( model_name, ) self.history = [] def clear_history(self) -\u0026gt; None: self.history = [] def tokenise(self, user_input:str, enable_thinking=None) -\u0026gt; BatchEncoding: if enable_thinking is None: enable_thinking = self.enable_thinking # default value messages = self.history + [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}] tokens = self.tokenizer.apply_chat_template( messages, tokenize=True, enable_thinking=enable_thinking, add_generation_prompt=True, return_tensors=\u0026#34;pt\u0026#34; ) return tokens def __call__(self, user_input, enable_thinking=None) -\u0026gt; None: inputs = self.tokenise(user_input, enable_thinking=enable_thinking) response_ids = self.model.generate( **inputs, max_new_tokens=2048, ) response_ids = response_ids[0][len(inputs.input_ids[0]):].tolist() response = self.tokenizer.decode(response_ids, skip_special_tokens=True) # since we aren\u0026#39;t going to do anything with the output, # just prints out the response and save it to the chat history. print(response) # Update history response_history = re.sub(self._thiniking_regex, \u0026#34;\u0026#34;, response, flags=re.DOTALL).strip() self.history.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}) self.history.append({\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: response_history}) Then, create a new chat instance with the code below:\nchat = DemoChatbot() You will see a progress bar showing download status. Once completed, type the following to have a look at the QWen3 model structure:\npprint(chat.model) â€¦which in term will give you this output:\nQwen3ForCausalLM( (model): Qwen3Model( (embed_tokens): Embedding(151936, 1024) (layers): ModuleList( (0-27): 28 x Qwen3DecoderLayer( (self_attn): Qwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) (mlp): Qwen3MLP( (gate_proj): Linear(in_features=1024, out_features=3072, bias=False) (up_proj): Linear(in_features=1024, out_features=3072, bias=False) (down_proj): Linear(in_features=3072, out_features=1024, bias=False) (act_fn): SiLUActivation() ) (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) ) ) (norm): Qwen3RMSNorm((1024,), eps=1e-06) (rotary_emb): Qwen3RotaryEmbedding() ) (lm_head): Linear(in_features=1024, out_features=151936, bias=False) ) Model Architecture # The very core of a transformer model can be checked by:\npprint(chat.model.model) And you will get an output similar to the previous output, but without the embed_tokens at the beginning and the lm_head at the end:\nQwen3Model( (embed_tokens): Embedding(151936, 1024) (layers): ModuleList( (0-27): 28 x Qwen3DecoderLayer( (self_attn): Qwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) (mlp): Qwen3MLP( (gate_proj): Linear(in_features=1024, out_features=3072, bias=False) (up_proj): Linear(in_features=1024, out_features=3072, bias=False) (down_proj): Linear(in_features=3072, out_features=1024, bias=False) (act_fn): SiLUActivation() ) (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) ) ) (norm): Qwen3RMSNorm((1024,), eps=1e-06) (rotary_emb): Qwen3RotaryEmbedding() ) The embed_tokens is the Stage 1 mentioned earlier, and the lm_head is the Stage 3. The core of a transformer, is what we\u0026rsquo;ve been discussing today.\nLet\u0026rsquo;s have a look at one of the layers inside the transformer:\nlayer = chat.model.model.layers[0] pprint(layer) You will see something like this:\nQwen3DecoderLayer( (self_attn): Qwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) (mlp): Qwen3MLP( (gate_proj): Linear(in_features=1024, out_features=3072, bias=False) (up_proj): Linear(in_features=1024, out_features=3072, bias=False) (down_proj): Linear(in_features=3072, out_features=1024, bias=False) (act_fn): SiLUActivation() ) (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06) ) Layer is divided into 2 major sections, Just like like we discussed previously:\nself_attn is where attention gets calculatd. All the rest (i.e., mlp, input_layernorm and post_attention_layernorm) are step-by-step computations of combining and averaging the attention heads. Run the following code to see the structure of an attention head:\nattention = layer.self_attn pprint(attention) The q_proj, k_proj and v_proj are the three matrices mentioned earlier:\nQwen3Attention( (q_proj): Linear(in_features=1024, out_features=2048, bias=False) (k_proj): Linear(in_features=1024, out_features=1024, bias=False) (v_proj): Linear(in_features=1024, out_features=1024, bias=False) (o_proj): Linear(in_features=2048, out_features=1024, bias=False) (q_norm): Qwen3RMSNorm((128,), eps=1e-06) (k_norm): Qwen3RMSNorm((128,), eps=1e-06) ) The actual impelmentations of attention heads are different from model to model, but the general principle behind should be roughly the same. I\u0026rsquo;ll dive depper into it in the future.\nHave some fun! # Meanwhile, since we\u0026rsquo;ve got a generative model already, we might as well test out some text generations:\nchat(\u0026#34;Why is the content, which was held to be true in perceiving, in fact only belongs to the form, and it dissolves into the formâ€™s unity ðŸ¥ºðŸ¥ºðŸ¥ºðŸ¥ºðŸ¥º??\u0026#34;) Or, you can turn on the \u0026rsquo;thinking\u0026rsquo; mode to enable chain-of-thought:\nchat.enable_thinking = True chat(\u0026#34;But...but the phenomenology Î¦147 says the inner, essential is essentially the truth of appearanceðŸ˜ ðŸ˜ ðŸ˜  I\u0026#39;m absolutely fewming\u0026#34;) If it takes too long to run, you can clear chat history to remove cached chats:\nchat.clear_history() \u0026lsquo;LLM is magic\u0026rsquo; I wanted to point out the (maybe) obvious thing here: almost all operations mentioned contain learnable parameters. Inside individual attention heads, the three matrices convreted by inputs are typically converted by multiplying (\u0026lsquo;dot product\u0026rsquo;) inputs with three separate matrices. These matrices are part of the learnable parameters for the attention head. When we combine matrices, the combination operation also has their own learnable parameters. Furthermore, when we combining outputs from each \u0026lsquo;attention heads\u0026rsquo;, this combining opearation also has its own set of trainable parameters, so on and so on\u0026hellip; Almost every stage of the matrix computations are parameterised, resulting the unbelievably massive AÄ° models as of today. However, the model used in the demo today, despite it\u0026rsquo;s size, is still able to produce long and very coherent responses. Yes, in a sense, transformers are just a huge stack of matirx calculation? Howver, we know very little about the reason behind We gave names to these matrices, we don\u0026rsquo;t know much about their behaviour.\nFinal thoughts # Transformer neural network models aren\u0026rsquo;t as mysterious as one think it would be. It has no difference compared with any other functions. At the end of the day it\u0026rsquo;s just another mathematical function, but takes matrix as inputs, does matrix calculations, and outputs matrices. Just like any other neural network models, it takes multiple steps to do the calculation. Within each computational step, the inputs gets processed by three mini-neural networks, and outputs are re-combined together as the output of the current step. In this sense, transformers are like nested neural networks: they are bigger neural network that contains lots of mini neural networks.\nWhat\u0026rsquo;s up next?\nThere are still a bit more stuffs that I\u0026rsquo;d like to share, like how text-generation works and what\u0026rsquo;s really happenning when we are training a generative model. We\u0026rsquo;ve heard of the same old things over and over: \u0026lsquo;generative LLM is just a very massive auto-complete!\u0026rsquo;. Whilst I do aggree with it, I also find it not helpful if one wants to understand text generation, for both model training and model inference. In the next post, we\u0026rsquo;ll have a look at the Stage 3 for text generation.\nSome Other Resources # I hope whoever come accross this post would find it useful. Here are some extra reading materials that Ä° found particularly useful:\nPytorch\u0026rsquo;s step-byp-step guide on creating a generative LLM is prob one of the best out there that teaches you all the fundenmentals. BertViz, a very good visualisation tool for looking at attention heads layer by layer. You can run it interactively in a jupyter notebook. Huggingface\u0026rsquo;s LLM cources. Although they tends to focus on the side of programming \u0026amp; practical applications, I found many of their conceptual guides very good for a beginner. ","date":"31 October 2025","externalUrl":null,"permalink":"/posts/transformers-pt-1/","section":"Blog","summary":"(it\u0026rsquo;s glorified linear algebra)","title":"ELI5 Transformers (Part 1): Attention Mechanism ","type":"posts"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]
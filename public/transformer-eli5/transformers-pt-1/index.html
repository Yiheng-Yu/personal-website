<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="dark"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title>ELI5 Transformers (Part 1): Attention Mechanism  &middot; Myxiniformes Moment</title>
    <meta name="title" content="ELI5 Transformers (Part 1): Attention Mechanism  &middot; Myxiniformes Moment">
  

  
  
    <meta name="description" content="(it&#39;s glorified linear algebra)">
  
  
    <meta name="keywords" content="AI,Machine Learning,ELI5,">
  
  
  
  <link rel="canonical" href="http://localhost:1313/transformer-eli5/transformers-pt-1/">
  

  
  
    <meta name="author" content="@Yiheng Yu">
  
  
    
      
        
          <link href="https://github.com/Yiheng-Yu" rel="me">
        
      
    
  

  
  <meta property="og:url" content="http://localhost:1313/transformer-eli5/transformers-pt-1/">
  <meta property="og:site_name" content="Myxiniformes Moment">
  <meta property="og:title" content="ELI5 Transformers (Part 1): Attention Mechanism ">
  <meta property="og:description" content="(it’s glorified linear algebra)">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="transformer-eli5">
    <meta property="article:published_time" content="2025-10-31T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-31T00:00:00+00:00">
    <meta property="article:tag" content="AI">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="ELI5">
    <meta property="og:image" content="http://localhost:1313/transformer-eli5/transformers-pt-1/featured.jpg">

  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/transformer-eli5/transformers-pt-1/featured.jpg">
  <meta name="twitter:title" content="ELI5 Transformers (Part 1): Attention Mechanism ">
  <meta name="twitter:description" content="(it’s glorified linear algebra)">

  
  
  
  
    
      
    
  
    
  
    
  
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
    
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.e0d7aa938e611d632a6bcd77d7a653ca1f54eeca3a7e2bea616393ed5cbcbecfcff49199680b87d53eef76de12660619ce480158e6276f1e64b8ceb56d73ea30.css"
    integrity="sha512-4Neqk45hHWMqa81316ZTyh9U7so6fivqYWOT7Vy8vs/P9JGZaAuH1T7vdt4SZgYZzkgBWOYnbx5kuM61bXPqMA==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
    
    <script src="/js/a11y.min.7eeed9f7a6fe7c8aa3b999966484369eb1b5380455116fc15c2b407140f3c2e3b174fc00201797dba34e4d4013213736ae375c9bcf2d7ad1fadb224355d8d54d.js" integrity="sha512-fu7Z96b&#43;fIqjuZmWZIQ2nrG1OARVEW/BXCtAcUDzwuOxdPwAIBeX26NOTUATITc2rjdcm88tetH62yJDVdjVTQ=="></script>
  
  
  
  
    
    <script
      type="text/javascript"
      src="/js/zen-mode.min.9814dee9614d32aeb56239d118edfe20acd6231424f9b19c2dd48835038414130a5e946c0d1c9087d60e60e31840c9babfd217e3d4b95643dc8d651a71ccdf4a.js"
      integrity="sha512-mBTe6WFNMq61YjnRGO3&#43;IKzWIxQk&#43;bGcLdSINQOEFBMKXpRsDRyQh9YOYOMYQMm6v9IX49S5VkPcjWUacczfSg=="></script>
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
    
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9cc802d09f28c6af56ceee7bc6e320a39251fdae98243f2a9942f221ac57a9f49c51609699a91794a7b2580ee1deaa8e4d794a68ffa94aa317c66e893ce51e02.js"
      integrity="sha512-nMgC0J8oxq9Wzu57xuMgo5JR/a6YJD8qmULyIaxXqfScUWCWmakXlKeyWA7h3qqOTXlKaP&#43;pSqMXxm6JPOUeAg=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>



  
  
  
  
  <script
    defer
    type="text/javascript"
    src="/js/mermaid.bundle.952a37a4056669084b44489b8acec851a95825e3c34632c74a0d9916b951884204b8f720e6292cc85b51c6bac44ab6392877b98585ecc205b41ea076df927163.js"
    integrity="sha512-lSo3pAVmaQhLREibis7IUalYJePDRjLHSg2ZFrlRiEIEuPcg5iksyFtRxrrESrY5KHe5hYXswgW0HqB235JxYw=="></script>







  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/lib/katex/katex.min.9e578d7ac3b5270b37ae32edae7835a744cda7734036e6e2f1ccf16f382b056ccd51c0df6b7ef3470991357a026507840aa22c1740ea2fe6bbce9b4223c5c182.css"
    integrity="sha512-nleNesO1Jws3rjLtrng1p0TNp3NANubi8czxbzgrBWzNUcDfa37zRwmRNXoCZQeECqIsF0DqL&#43;a7zptCI8XBgg==">
  
  
  
  <script
    defer
    type="text/javascript"
    src="/js/katex.bundle.a10a4a0655682a978b0422e48a40f3515d4d96c8e600a48e3dad34029a55cc00b128b89a908855227d1c07897ce09cdfe9f0d57decb148cb73f17dbb8a0a715c.js"
    integrity="sha512-oQpKBlVoKpeLBCLkikDzUV1NlsjmAKSOPa00AppVzACxKLiakIhVIn0cB4l84Jzf6fDVfeyxSMtz8X27igpxXA=="
    id="katex-render"></script>
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  










  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/components/carousel.8e65f2c27285218feed8ca2a6e12a80ec2fb4be8f774b7c74d9642f1bc528a3352288a700a64a0e1e3c38a40b74c978b5dbbeb38346202b5f7613d8fc023e3c9.css"
    integrity="sha512-jmXywnKFIY/u2MoqbhKoDsL7S&#43;j3dLfHTZZC8bxSijNSKIpwCmSg4ePDikC3TJeLXbvrODRiArX3YT2PwCPjyQ==">
  
  
  <script
    defer
    type="text/javascript"
    src="/lib/tw-elements/index.min.5ffccf337b773ee831515cdde152c8039e972dd479f7dffb0532a79de9a6de5926154ae2587f181ed6c06a99aee25667096bcf6495da5aa86e235d98baee5943.js"
    integrity="sha512-X/zPM3t3PugxUVzd4VLIA56XLdR599/7BTKnnemm3lkmFUriWH8YHtbAapmu4lZnCWvPZJXaWqhuI12Yuu5ZQw=="></script>









  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "",
    "name": "ELI5 Transformers (Part 1): Attention Mechanism ",
    "headline": "ELI5 Transformers (Part 1): Attention Mechanism ",
    
    "abstract": "(it\u0026rsquo;s glorified linear algebra)",
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/transformer-eli5\/transformers-pt-1\/",
    "author" : {
      "@type": "Person",
      "name": "@Yiheng Yu"
    },
    "copyrightYear": "2025",
    "dateCreated": "2025-10-31T00:00:00\u002b00:00",
    "datePublished": "2025-10-31T00:00:00\u002b00:00",
    
    "dateModified": "2025-10-31T00:00:00\u002b00:00",
    
    "keywords": ["AI","Machine Learning","ELI5"],
    
    "mainEntityOfPage": "true",
    "wordCount": "3138"
  }]
  </script>



  
  

  
  

  
  

  
  
    
      <script src="https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js"></script>
      <script src="https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js"></script>
      <script src="https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js"></script>
      <script>

    const firebaseConfig = {
      apiKey: "AIzaSyC7c0UZ3xECttjMuZWh5AE41qOMLl5bAcg",
      authDomain: "yiheng-blog-stats.firebaseapp.com",
      projectId: "yiheng-blog-stats",
      storageBucket: "yiheng-blog-stats.firebasestorage.app",
      messagingSenderId: "674513555332",
      appId: "1:674513555332:web:fcc2cd0ba68d0adc0017d1",
      measurementId: "G-4X2W77Z3VD"
    };

        var app = firebase.initializeApp(firebaseConfig);
        var db = firebase.firestore();
        var auth = firebase.auth();

      </script>
    
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      <div class="min-h-[148px]"></div>
<div class="fixed inset-x-0 bg-neutral dark:bg-neutral-800 z-100">
  <div class="relative m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32">
    













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  
    
    

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Myxiniformes Moment
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/posts/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="Blog"
  title="">
  
  
    <p class="text-base font-medium">
      Blog
    </p>
  
</a>



      
    

    

    
      <div class="flex items-center">
    <button
      id="desktop-a11y-toggle"
      aria-label="Open accessibility panel"
      aria-expanded="false"
      type="button"
      class="text-base hover:text-primary-600 dark:hover:text-primary-400"
      role="button"
      aria-pressed="false">
      <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M0 256a256 256 0 1 1 512 0A256 256 0 1 1 0 256zm161.5-86.1c-12.2-5.2-26.3 .4-31.5 12.6s.4 26.3 12.6 31.5l11.9 5.1c17.3 7.4 35.2 12.9 53.6 16.3l0 50.1c0 4.3-.7 8.6-2.1 12.6l-28.7 86.1c-4.2 12.6 2.6 26.2 15.2 30.4s26.2-2.6 30.4-15.2l24.4-73.2c1.3-3.8 4.8-6.4 8.8-6.4s7.6 2.6 8.8 6.4l24.4 73.2c4.2 12.6 17.8 19.4 30.4 15.2s19.4-17.8 15.2-30.4l-28.7-86.1c-1.4-4.1-2.1-8.3-2.1-12.6l0-50.1c18.4-3.5 36.3-8.9 53.6-16.3l11.9-5.1c12.2-5.2 17.8-19.3 12.6-31.5s-19.3-17.8-31.5-12.6L338.7 175c-26.1 11.2-54.2 17-82.7 17s-56.5-5.8-82.7-17l-11.9-5.1zM256 160a40 40 0 1 0 0-80 40 40 0 1 0 0 80z"/></svg>
</span>
    </button>

    <div id="desktop-a11y-overlay" class="fixed inset-0 z-500 hidden"></div>

    <div
      id="desktop-a11y-panel"
      role="dialog"
      aria-labelledby="desktop-a11y-panel-title"
      class="a11y-panel-enter fixed hidden z-500 p-6 top-1/2 left-1/2 -translate-x-1/2 -translate-y-1/2 w-80 rounded-lg shadow-xl bg-neutral-50 dark:bg-neutral-800 border border-neutral-200 dark:border-neutral-700"
      style="min-width: 20rem;">
      <div class="flex items-center justify-between mb-6">
        <h3
          id="desktop-a11y-panel-title"
          class="text-lg font-semibold text-neutral-900 dark:text-neutral-100">
          Accessibility settings
        </h3>
        <button
          id="desktop-a11y-close"
          class="text-neutral-500 hover:text-neutral-700 dark:text-neutral-400 dark:hover:text-neutral-200"
          aria-label="Close a11y panel">
          <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
          </svg>
        </button>
      </div>

      <div class="space-y-5">
        
        
        
        
          
        
        
          <div class="flex items-center justify-between">
            <label for="desktop-disable-blur" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Disable blur
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="desktop-disable-blur">
            </div>
          </div>
          <div class="flex items-center justify-between">
            <label for="desktop-underline-links" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Show link underline
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="desktop-underline-links">
            </div>
          </div>
          <div class="flex items-center justify-between">
            <label for="desktop-zen-mode" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Enable zen mode
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="desktop-zen-mode">
            </div>
          </div>


        <div class="flex items-center justify-between">
          <label
            for="desktop-font-size-select"
            class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
            Font size
          </label>
          <select
            id="desktop-font-size-select"
            class="border rounded-lg px-3 py-1.5 pr-8 text-neutral-900 text-sm dark:bg-neutral-700 dark:text-neutral-200 focus:ring-primary-500 focus:border-primary-500">
            
            
              <option value="default">default</option>
            
              <option value="12px">12px</option>
            
              <option value="14px">14px</option>
            
              <option value="16px">16px</option>
            
              <option value="18px">18px</option>
            
              <option value="20px">20px</option>
            
              <option value="22px">22px</option>
            
              <option value="24px">24px</option>
            
          </select>
        </div>
      </div>
    </div>
  </div>

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    
      <div class="flex items-center">
    <button
      id="mobile-a11y-toggle"
      aria-label="Open accessibility panel"
      aria-expanded="false"
      type="button"
      class="text-base hover:text-primary-600 dark:hover:text-primary-400"
      role="button"
      aria-pressed="false">
      <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M0 256a256 256 0 1 1 512 0A256 256 0 1 1 0 256zm161.5-86.1c-12.2-5.2-26.3 .4-31.5 12.6s.4 26.3 12.6 31.5l11.9 5.1c17.3 7.4 35.2 12.9 53.6 16.3l0 50.1c0 4.3-.7 8.6-2.1 12.6l-28.7 86.1c-4.2 12.6 2.6 26.2 15.2 30.4s26.2-2.6 30.4-15.2l24.4-73.2c1.3-3.8 4.8-6.4 8.8-6.4s7.6 2.6 8.8 6.4l24.4 73.2c4.2 12.6 17.8 19.4 30.4 15.2s19.4-17.8 15.2-30.4l-28.7-86.1c-1.4-4.1-2.1-8.3-2.1-12.6l0-50.1c18.4-3.5 36.3-8.9 53.6-16.3l11.9-5.1c12.2-5.2 17.8-19.3 12.6-31.5s-19.3-17.8-31.5-12.6L338.7 175c-26.1 11.2-54.2 17-82.7 17s-56.5-5.8-82.7-17l-11.9-5.1zM256 160a40 40 0 1 0 0-80 40 40 0 1 0 0 80z"/></svg>
</span>
    </button>

    <div id="mobile-a11y-overlay" class="fixed inset-0 z-500 hidden"></div>

    <div
      id="mobile-a11y-panel"
      role="dialog"
      aria-labelledby="mobile-a11y-panel-title"
      class="a11y-panel-enter fixed hidden z-500 p-6 top-1/2 left-1/2 -translate-x-1/2 -translate-y-1/2 w-80 rounded-lg shadow-xl bg-neutral-50 dark:bg-neutral-800 border border-neutral-200 dark:border-neutral-700"
      style="min-width: 20rem;">
      <div class="flex items-center justify-between mb-6">
        <h3
          id="mobile-a11y-panel-title"
          class="text-lg font-semibold text-neutral-900 dark:text-neutral-100">
          Accessibility settings
        </h3>
        <button
          id="mobile-a11y-close"
          class="text-neutral-500 hover:text-neutral-700 dark:text-neutral-400 dark:hover:text-neutral-200"
          aria-label="Close a11y panel">
          <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
          </svg>
        </button>
      </div>

      <div class="space-y-5">
        
        
        
        
          
        
        
          <div class="flex items-center justify-between">
            <label for="mobile-disable-blur" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Disable blur
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="mobile-disable-blur">
            </div>
          </div>
          <div class="flex items-center justify-between">
            <label for="mobile-underline-links" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Show link underline
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="mobile-underline-links">
            </div>
          </div>
          <div class="flex items-center justify-between">
            <label for="mobile-zen-mode" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Enable zen mode
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="mobile-zen-mode">
            </div>
          </div>


        <div class="flex items-center justify-between">
          <label
            for="mobile-font-size-select"
            class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
            Font size
          </label>
          <select
            id="mobile-font-size-select"
            class="border rounded-lg px-3 py-1.5 pr-8 text-neutral-900 text-sm dark:bg-neutral-700 dark:text-neutral-200 focus:ring-primary-500 focus:border-primary-500">
            
            
              <option value="default">default</option>
            
              <option value="12px">12px</option>
            
              <option value="14px">14px</option>
            
              <option value="16px">16px</option>
            
              <option value="18px">18px</option>
            
              <option value="20px">20px</option>
            
              <option value="22px">22px</option>
            
              <option value="24px">24px</option>
            
          </select>
        </div>
      </div>
    </div>
  </div>

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/posts/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="Blog"
    title="">
    
    
      <p class="text-bg font-bg">
        Blog
      </p>
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>




  <script>
    (function () {
      var $mainmenu = $(".main-menu");
      var path = window.location.pathname;
      $mainmenu.find('a[href="' + path + '"]').each(function (i, e) {
        $(e).children("p").addClass("active");
      });
    })();
  </script>


  </div>
</div>

    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
        <ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden">
  
  
    
  
    
  
  <li class="hidden">
    <a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href="/"
      >Myxiniformes Moment</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class="inline">
    <a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href="/transformer-eli5/"
      >transformer-eli5</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class="hidden">
    <a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href="/transformer-eli5/transformers-pt-1/"
      >ELI5 Transformers (Part 1): Attention Mechanism </a
    ><span class="px-1 text-primary-500">/</span>
  </li>

</ol>


      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        ELI5 Transformers (Part 1): Attention Mechanism 
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  
    
  

  

  
    
  

  

  
    
  

  

  
    
  

  
    
  

  
    
  

  

  
    
  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="2025-10-31T00:00:00&#43;00:00">31 October 2025</time><span class="px-2 text-primary-500">&middot;</span><span>3138 words</span><span class="px-2 text-primary-500">&middot;</span><span>
  
  
    
    
      
      
        
        
      
      
    
  
  <span
    id="views_Transformer-ELI5/transformers-pt-1/index.md"
    class="animate-pulse inline-block text-transparent max-h-3 rounded-full -mt-[2px] align-middle bg-neutral-300 dark:bg-neutral-400"
    title="views"
    >loading</span
  >
  <span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512">
<path fill="currentColor" d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></span>
</span>
<span class="px-2 text-primary-500">&middot;</span><span>
  
  
    
    
      
      
        
        
      
      
    
  
  <span
    id="likes_Transformer-ELI5/transformers-pt-1/index.md"
    class="animate-pulse inline-block text-transparent max-h-3 rounded-full -mt-[2px] align-middle bg-neutral-300 dark:bg-neutral-400"
    title="likes"
    >loading</span
  >
  <span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg></span></span>
</span>
<span class="px-2 text-primary-500">&middot;</span><span>
  <button
    id="button_likes"
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    <span id="button_likes_heart" class="inline-block align-text-bottom hidden"
      ><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg></span>
    </span>
    <span id="button_likes_emtpty_heart" class="inline-block align-text-bottom"
      ><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M244 84L255.1 96L267.1 84.02C300.6 51.37 347 36.51 392.6 44.1C461.5 55.58 512 115.2 512 185.1V190.9C512 232.4 494.8 272.1 464.4 300.4L283.7 469.1C276.2 476.1 266.3 480 256 480C245.7 480 235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1 0 232.4 0 190.9V185.1C0 115.2 50.52 55.58 119.4 44.1C164.1 36.51 211.4 51.37 244 84C243.1 84 244 84.01 244 84L244 84zM255.1 163.9L210.1 117.1C188.4 96.28 157.6 86.4 127.3 91.44C81.55 99.07 48 138.7 48 185.1V190.9C48 219.1 59.71 246.1 80.34 265.3L256 429.3L431.7 265.3C452.3 246.1 464 219.1 464 190.9V185.1C464 138.7 430.4 99.07 384.7 91.44C354.4 86.4 323.6 96.28 301.9 117.1L255.1 163.9z"/></svg></span></span
    >
    <span id="button_likes_text">&nbsp;Like</span>
  </button>
</span><span class="px-2 text-primary-500">&middot;</span><span class="mb-[2px]">
  <span
    id="zen-mode-button"
    class="text-lg hover:text-primary-500"
    title="Enable zen mode"
    data-title-i18n-disable="Enable zen mode"
    data-title-i18n-enable="Disable zen mode">
    <span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="50px" height="50px">
    <path fill="currentColor" d="M 12.980469 4 C 9.1204688 4 5.9804688 7.14 5.9804688 11 L 6 26 L 9.9804688 26 L 9.9804688 11 C 9.9804688 9.35 11.320469 8 12.980469 8 L 40.019531 8 C 41.679531 8 43.019531 9.35 43.019531 11 L 43.019531 39 C 43.019531 40.65 41.679531 42 40.019531 42 L 29 42 C 29 43.54 28.420938 44.94 27.460938 46 L 40.019531 46 C 43.879531 46 47.019531 42.86 47.019531 39 L 47.019531 11 C 47.019531 7.14 43.879531 4 40.019531 4 L 12.980469 4 z M 7 28 C 4.794 28 3 29.794 3 32 L 3 42 C 3 44.206 4.794 46 7 46 L 23 46 C 25.206 46 27 44.206 27 42 L 27 32 C 27 29.794 25.206 28 23 28 L 7 28 z M 7 32 L 23 32 L 23.001953 42 L 7 42 L 7 32 z"/>
</svg></span></span>
  </span>
</span>

    

    
    
  </div>

  
    <div class="flex flex-row flex-wrap items-center">
      
        
          
        
      
        
      
        
      
        
      
        
      
    </div>
  

  
  
    <div class="flex flex-row flex-wrap items-center">
      
        
      
        
          
        
      
        
          
            
              <a class="relative mt-[0.5rem] me-2" href="/category/transformers/">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Transformers
  </span>
</span>

              </a>
            
          
        
      
        
      
        
          
            
              <a class="relative mt-[0.5rem] me-2" href="/tags/ai/">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    AI
  </span>
</span>

              </a>
            
              <a class="relative mt-[0.5rem] me-2" href="/tags/machine-learning/">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Machine Learning
  </span>
</span>

              </a>
            
              <a class="relative mt-[0.5rem] me-2" href="/tags/eli5/">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    ELI5
  </span>
</span>

              </a>
            
          
        
      
    </div>
  

  
  



      </div>
      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      
        <div class="order-first lg:ml-auto px-0 lg:order-last lg:ps-8 lg:max-w-2xs">
          <div class="toc ps-5 print:hidden lg:sticky lg:top-[140px]">
            
              <details
  open
  id="TOCView"
  class="toc-right mt-0 overflow-y-auto overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg -ms-5 ps-5 pe-2 hidden lg:block">
  <summary
    class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 -ms-5 ps-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">
    Table of Contents
  </summary>
  <div
    class="min-w-[220px] py-2 border-dotted border-s-1 -ms-5 ps-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#model-only-needs-to-be-useful">Model only needs to be useful</a></li>
    <li><a href="#transformer-model-architecture">Transformer Model Architecture</a>
      <ul>
        <li><a href="#overview">Overview</a></li>
        <li><a href="#the-transformer-itself">The Transformer Itself</a></li>
        <li><a href="#the-attention-head">The attention head</a></li>
      </ul>
    </li>
    <li><a href="#a-transformer-in-action-ai-text-translation">A transformer in action: AI text translation</a></li>
    <li><a href="#the-transformer-architecture">The Transformer Architecture</a>
      <ul>
        <li><a href="#overview-the-transformer-itself">Overview: <em>the</em> transformer itself</a></li>
      </ul>
    </li>
    <li><a href="#why-are-the-models-so-big">Why are the models so big?</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#some-other-resources">Some Other Resources</a></li>
  </ul>
</nav>
  </div>
</details>
<details class="toc-inside mt-0 overflow-hidden rounded-lg -ms-5 ps-5 lg:hidden">
  <summary
    class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 -ms-5 ps-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">
    Table of Contents
  </summary>
  <div
    class="py-2 border-dotted border-neutral-300 border-s-1 -ms-5 ps-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#model-only-needs-to-be-useful">Model only needs to be useful</a></li>
    <li><a href="#transformer-model-architecture">Transformer Model Architecture</a>
      <ul>
        <li><a href="#overview">Overview</a></li>
        <li><a href="#the-transformer-itself">The Transformer Itself</a></li>
        <li><a href="#the-attention-head">The attention head</a></li>
      </ul>
    </li>
    <li><a href="#a-transformer-in-action-ai-text-translation">A transformer in action: AI text translation</a></li>
    <li><a href="#the-transformer-architecture">The Transformer Architecture</a>
      <ul>
        <li><a href="#overview-the-transformer-itself">Overview: <em>the</em> transformer itself</a></li>
      </ul>
    </li>
    <li><a href="#why-are-the-models-so-big">Why are the models so big?</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#some-other-resources">Some Other Resources</a></li>
  </ul>
</nav>
  </div>
</details>


<script>
  (function () {
    'use strict'

    const SCROLL_OFFSET_RATIO = 0.33
    const TOC_SELECTOR = '#TableOfContents'
    const ANCHOR_SELECTOR = '.anchor'
    const TOC_LINK_SELECTOR = 'a[href^="#"]'
    const NESTED_LIST_SELECTOR = 'li ul'
    const ACTIVE_CLASS = 'active'
    let isJumpingToAnchor = false

    function getActiveAnchorId(anchors, offsetRatio) {
      const threshold = window.scrollY + window.innerHeight * offsetRatio
      const tocLinks = [...document.querySelectorAll('#TableOfContents a[href^="#"]')]
      const tocIds = new Set(tocLinks.map(link => link.getAttribute('href').substring(1)))

      if (isJumpingToAnchor) {
        for (let i = 0; i < anchors.length; i++) {
          const anchor = anchors[i]
          if (!tocIds.has(anchor.id)) continue
          const top = anchor.getBoundingClientRect().top + window.scrollY
          if (Math.abs(window.scrollY - top) < 100) {
            return anchor.id
          }
        }
      }

      for (let i = anchors.length - 1; i >= 0; i--) {
        const top = anchors[i].getBoundingClientRect().top + window.scrollY
        if (top <= threshold && tocIds.has(anchors[i].id)) {
          return anchors[i].id
        }
      }
      return anchors.find(anchor => tocIds.has(anchor.id))?.id || ''
    }

    function updateTOC({ toc, anchors, links, scrollOffset, collapseInactive }) {
      const activeId = getActiveAnchorId(anchors, scrollOffset)
      if (!activeId) return

      links.forEach(link => {
        const isActive = link.getAttribute('href') === `#${activeId}`
        link.classList.toggle(ACTIVE_CLASS, isActive)

        if (collapseInactive) {
          const ul = link.closest('li')?.querySelector('ul')
          if (ul) ul.style.display = isActive ? '' : 'none'
        }
      })

      if (collapseInactive) {
        const activeLink = toc.querySelector(`a[href="#${CSS.escape(activeId)}"]`)
        let el = activeLink
        while (el && el !== toc) {
          if (el.tagName === 'UL') el.style.display = ''
          if (el.tagName === 'LI') el.querySelector('ul')?.style.setProperty('display', '')
          el = el.parentElement
        }
      }
    }

    function initTOC() {
      const toc = document.querySelector(TOC_SELECTOR)
      if (!toc) return

      const collapseInactive = false
      const anchors = [...document.querySelectorAll(ANCHOR_SELECTOR)]
      const links = [...toc.querySelectorAll(TOC_LINK_SELECTOR)]

      if (collapseInactive) {
        toc.querySelectorAll(NESTED_LIST_SELECTOR).forEach(ul => ul.style.display = 'none')
      }

      links.forEach(link => {
        link.addEventListener('click', () => {
          isJumpingToAnchor = true
        })
      })

      const config = {
        toc,
        anchors,
        links,
        scrollOffset: SCROLL_OFFSET_RATIO,
        collapseInactive
      }

      window.addEventListener('scroll', () => updateTOC(config), { passive: true })
      window.addEventListener('hashchange', () => updateTOC(config), { passive: true })

      updateTOC(config)
    }

    document.readyState === 'loading'
      ? document.addEventListener('DOMContentLoaded', initTOC)
      : initTOC()
  })()
</script>


            
          </div>
        </div>
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          

<p>As someone without much backrounds in neither physics nor computer science, I find lots of available introductions on transformers very confusing. Transformers is the talk of the street, GPT is short for &lsquo;Generative Pre-training <i>Transformer</i>&rsquo;! However, most of the articles on transformers focuses on attention mechanisms, using either <a
  href="https://peterbloem.nl/blog/transformers"
    target="_blank"
  >the OG transformer</a> or <a
  href="https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11/"
    target="_blank"
  >the classic BERT</a> as examples. They would spend lot of time talking about embeddings &amp; attetions on the encoding side, and skipped most the decoding by saying &lsquo;well you just do the same thing again and there you have it!&rsquo;. Well that&rsquo;s not very helpful isn&rsquo;t it. Don&rsquo;t get me wrong, there are a lot of very good learning materials out there, for example the amazing <a
  href="https://poloclub.github.io/transformer-explainer"
    target="_blank"
  >interactive transofmer explainer</a>. However, I always find these heavy tutorials not very suitable for my very short attention span or the autism tendency of getting lost in details.<br></p>
<p>Finally, I&rsquo;ve decided to bite the bullet and spend some time have a read through the HuggingFace&rsquo;s source code for google&rsquo;s T5 Model, <a
  href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py"
    target="_blank"
  >THE encoder-decoder everyone on the steet are talking about</a>. It was a relatively long process with lots of back and forth jumping between classes and methods. I could not emphasis how much I appretiate HuggingFace&rsquo;s <a
  href="https://huggingface.co/blog/transformers-design-philosophy"
    target="_blank"
  >maximalist coding choice</a>, where the entire model architecture is contained inside one single .py file. The bonouns point is, I didn&rsquo;t experience the pain of come accross <code>import tensorflow</code> <a
  href="https://github.com/huggingface/transformers/blob/v4.57.1/src/transformers/data/data_collator.py#L742"
    target="_blank"
  >inside dataset iteration</a>. <br></p>
<p>I took lots of notes here and there during the process of studying transformers, think now it&rsquo;s a very good time to share some of my findings. In this (or probably a series of?) blogpost(s?), I am going to collate my past notes on text-specific transformer models piece by piece in a reader-friendly manner. I hope these notes can help others alongside their studying, or being an interesting nice little piece of articles to read through.<br></p>
<p>In this particular post, I would like to do a very brief overview of the transformer model architecture, specifically on the attention mechanism. I won&rsquo;t go metion too much math and there won&rsquo;t be any mathenathical formulas. However, I would assume readers of this silly little post already have some okay-ish background of math/ datascience. (i.e., matrix computations embeddings, tokens, model fitting etc.). I am not going to list out all the implemention details for transformers, since there are a lot of very good materials out there and they are doing fantastic jobs. Instead, in this (maybe series of?) post, İ would like to draw out a general framework on transformers to help one understand the detailed math behind.<br></p>
<p>In this particular blogpost, I&rsquo;ll very quickly go through some very basics on neural network model, just enough to cover what needed for this post, accompied by demo of transformer model as a proof of concept. In this section, there will be some codes that you can copy and paste into an interactive python session to fiddle around for a bit. And lastly, I&rsquo;ll do a quick sketch on the general architecture of transformers, and a overview of the attention mechasm.<br></p>

<h2 class="relative group">Model only needs to be useful
    <div id="model-only-needs-to-be-useful" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#model-only-needs-to-be-useful" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In order to make things easier to understand, I would wish to start with an inaccuate premise: we can view neural network models as functions that takes some sort of matrix as inputs, do some sort of matrix computations, and output another matrix as the final result. What makes one neural network different from others is how the computation is carried out. It&rsquo;s like \(y=a \times x^2\) is a different function from \(y=a \times sin(x)\), only that in the case of neural network, both x and y are matrics, and the math is much complicated. When it comes to model training, we are essentially trying to find values gives best fit to the data.<br></p>
<p>There&rsquo;s an important assumption here: just because model fits the data well does not mean the model describes mechanisms behind the data. For example, we <i>definitely</i> can fit \(y=a \times sin(x) + b\) to a normal distribution data (like distribution of customer spendings in McDonald&rsquo;s), and it&rsquo;s prob going to be a pretty good fit, but this does not mean the sine function has anything to do with explaining the normal distribution. A good model does not always need to be description, a good model just needs to be useful for its purpose.<br></p>
<p>Transformers are preciesly these kinds of models: they are, surprisingly good at fitting into all sorts of data whilst the math behind the model probably doesn&rsquo;t have much to do with the mechanisms behind. We don&rsquo;t know how transformers works so well for text-based tasks. At least not yet. Originally, transformer was designed as an add-on to the text-processing neural network models in order to tackle with some tricky problems (these problems are not the main forcus of the current blogpost so I&rsquo;m skipping them, but <a
  href="https://towardsdatascience.com/beautifully-illustrated-nlp-models-from-rnn-to-transformer-80d69faf2109/"
    target="_blank"
  >here&rsquo;s a good article if you were interested</a>). We just happened to discover that transformers alone is good enough to solve these problems, we just need to make the transformers much bigger. So that&rsquo;s where the Aİ bloom started: GPT2 solved issues in GPT1 by simply being 10 times bigger; the most-recently open-sourced <a
  href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1"
    target="_blank"
  >pretrained GPT-Oss</a>, is 200 times bigger than <a
  href="https://huggingface.co/openai-community/gpt2"
    target="_blank"
  >the previous openpsourced model, GPT2</a> <i>(note: GPT-OSS is structurlly different from the original GPT2 but the fundamental ideas are the same.)</i>. There are even speculations suggesting transformer neural network models can be seen as some sort of universal function approximator. That is, it&rsquo;s capacable of &lsquo;approximate&rsquo; other formulas/ functions with certain degree of accuracy, providing the model itself is big enough (<a
  href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"
    target="_blank"
  >&lsquo;universal approximation theorem&rsquo;</a>). <br></p>

<h2 class="relative group">Transformer Model Architecture
    <div id="transformer-model-architecture" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#transformer-model-architecture" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h3 class="relative group">Overview
    <div id="overview" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#overview" aria-label="Anchor">#</a>
    </span>
    
</h3>
<p>At conceptual level, the general idea behind transformer models are is actually pretty intuitive. It&rsquo;s essentially a big neural network models that combines multiples of mini neural networks. At a layer-by-layer level, this is how a typical transformer model functions：
<pre class="not-prose mermaid">
flowchart TB

    n1(["Raw Input"]) --> n2["Embedding"]
    n2 --> n3["Transformer"]
    n3 --> n4["Raw Output"]
    n4 --> n5(["Task-Specific Outputs"])

    style n1 fill:transparent
    style n2 fill:transparent
    style n3 fill:transparent
    style n4 fill:transparent
    style n5 fill:transparent

</pre>
</p>
<p>We can roughly categorise it into three stages:
<ol class="border-s-2 border-primary-500 dark:border-primary-300 list-none">






<li>
  <div class="flex">
    <div
      class="bg-primary-500 dark:bg-primary-300 text-neutral-50 dark:text-neutral-700 min-w-[30px] h-8 text-2xl flex items-center justify-center rounded-full ltr:-ml-12 rtl:-mr-[79px] mt-5">
      <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512">
<path fill="currentColor"  d="M392.8 1.2c-17-4.9-34.7 5-39.6 22l-128 448c-4.9 17 5 34.7 22 39.6s34.7-5 39.6-22l128-448c4.9-17-5-34.7-22-39.6zm80.6 120.1c-12.5 12.5-12.5 32.8 0 45.3L562.7 256l-89.4 89.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l112-112c12.5-12.5 12.5-32.8 0-45.3l-112-112c-12.5-12.5-32.8-12.5-45.3 0zm-306.7 0c-12.5-12.5-32.8-12.5-45.3 0l-112 112c-12.5 12.5-12.5 32.8 0 45.3l112 112c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L77.3 256l89.4-89.4c12.5-12.5 12.5-32.8 0-45.3z"/></svg></span>
    </div>
    <div class="block p-6 rounded-lg shadow-2xl flex-1 ms-6 mb-10 break-words">
      <div class="flex justify-between">
        
          <h2 class="mt-0">Stage 1</h2>
        
        
          <h3 class="">
            <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Embedding
  </span>
</span>

          </h3>
        
      </div>
      
        <h4 class="mt-0">
          The input gets converted into vectors or matrics.
        </h4>
      
      <div class="mb-6">
        
  The first step starts with creating a mathematical representation of our input data. This process can vary based on different types of inputs. It can simply be some sort of look-up tables (text embedding), some matrix transformations of the raw inputs (convolution) etc.

      </div>
    </div>
  </div>
</li>






<li>
  <div class="flex">
    <div
      class="bg-primary-500 dark:bg-primary-300 text-neutral-50 dark:text-neutral-700 min-w-[30px] h-8 text-2xl flex items-center justify-center rounded-full ltr:-ml-12 rtl:-mr-[79px] mt-5">
      <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512">
<path fill="currentColor"  d="M392.8 1.2c-17-4.9-34.7 5-39.6 22l-128 448c-4.9 17 5 34.7 22 39.6s34.7-5 39.6-22l128-448c4.9-17-5-34.7-22-39.6zm80.6 120.1c-12.5 12.5-12.5 32.8 0 45.3L562.7 256l-89.4 89.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l112-112c12.5-12.5 12.5-32.8 0-45.3l-112-112c-12.5-12.5-32.8-12.5-45.3 0zm-306.7 0c-12.5-12.5-32.8-12.5-45.3 0l-112 112c-12.5 12.5-12.5 32.8 0 45.3l112 112c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L77.3 256l89.4-89.4c12.5-12.5 12.5-32.8 0-45.3z"/></svg></span>
    </div>
    <div class="block p-6 rounded-lg shadow-2xl flex-1 ms-6 mb-10 break-words">
      <div class="flex justify-between">
        
          <h2 class="mt-0">Stage 2</h2>
        
        
          <h3 class="">
            <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Transformer
  </span>
</span>

          </h3>
        
      </div>
      
        <h4 class="mt-0">
          The raw output from Stage 1 feeds into the multiple different attention layers.
        </h4>
      
      <div class="mb-6">
        
  Mathematically, each attention layer is doing very much the same mathematical operation, with each layer having its own sets of parameters. Each layer takes a matrix as an input, and outputs another matrix to pass onto the next layer. This process is repeated multiple times. Stage 2 is the core of a transformer model, it *transforms* our inputs into something else.

      </div>
    </div>
  </div>
</li>






<li>
  <div class="flex">
    <div
      class="bg-primary-500 dark:bg-primary-300 text-neutral-50 dark:text-neutral-700 min-w-[30px] h-8 text-2xl flex items-center justify-center rounded-full ltr:-ml-12 rtl:-mr-[79px] mt-5">
      <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512">
<path fill="currentColor"  d="M392.8 1.2c-17-4.9-34.7 5-39.6 22l-128 448c-4.9 17 5 34.7 22 39.6s34.7-5 39.6-22l128-448c4.9-17-5-34.7-22-39.6zm80.6 120.1c-12.5 12.5-12.5 32.8 0 45.3L562.7 256l-89.4 89.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l112-112c12.5-12.5 12.5-32.8 0-45.3l-112-112c-12.5-12.5-32.8-12.5-45.3 0zm-306.7 0c-12.5-12.5-32.8-12.5-45.3 0l-112 112c-12.5 12.5-12.5 32.8 0 45.3l112 112c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L77.3 256l89.4-89.4c12.5-12.5 12.5-32.8 0-45.3z"/></svg></span>
    </div>
    <div class="block p-6 rounded-lg shadow-2xl flex-1 ms-6 mb-10 break-words">
      <div class="flex justify-between">
        
          <h2 class="mt-0">Stage 3</h2>
        
        
          <h3 class="">
            <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Output
  </span>
</span>

          </h3>
        
      </div>
      
        <h4 class="mt-0">
          We convert the matrix output from Stage 2 into task-specific results.
        </h4>
      
      <div class="mb-6">
        
  This is usually done by another set of simple matrix operations, depending on the task. For example, if we are doing text sentimental analysis task, this operation could be a simple matrix multiplication, resulting in a final score of 0-10.

      </div>
    </div>
  </div>
</li>

</ol>
</p>
<p>In other words, you can concepturally see <strong>Stage 1</strong> as a conversion stage in order to initiate the model, <strong>Stage 2</strong> being the core of a transformer model, and <strong>Stage 3</strong> as a &lsquo;decoding&rsquo; step to convert the output back into human-readable form. When we talk about transformer models, we are mostly referring to  <strong>Stage 2</strong>, which is the focus of the current post. I&rsquo;ll elabrate a lot more on what&rsquo;s hapenning in <strong>Stage 3</strong> in the next post.<br></p>

<h3 class="relative group">The Transformer Itself
    <div id="the-transformer-itself" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#the-transformer-itself" aria-label="Anchor">#</a>
    </span>
    
</h3>
<p>The transformer itself is pretty straightforward: it consists of stacks of multiple attention layers that often share exactly the same, or very similar structure:
<pre class="not-prose mermaid">
---
config:
  layout: dagre
---
flowchart LR

    n1["Input"] --> n2["Attention<br>Layer"]
    n2 --> n3["Attention<br>Layer"]
    n3 --> n6["Repeat"]
    n6 --> n5["Output"]

    n2@{ shape: rounded}
    n3@{ shape: rounded}
    n6@{ shape: text}
    classDef default fill: transparent, bg-color: transparent

</pre>
</p>
<p>Asttention layer consists of multiple &lsquo;attention heads&rsquo; that works in parallel: each attention head processes the inputs independently, and the output of all attention heads are merged back together. The joined matrix is the final output of the current layer: <br></p>
<pre class="not-prose mermaid">
flowchart LR
 subgraph s1["Attention Heads"]

    direction LR
        n14["Attention Head"]
        n16["Attention Head"]
        n17["Attention Head"]

  end

    s1 --> n10["Merge"]
    n10 --> n18["Next Layer"]
    n19["Previous Layer"] --> s1

    n18@{ shape: text}
    n19@{ shape: text}
    classDef default fill: transparent
    style s1 fill:transparent

</pre>


<h3 class="relative group">The attention head
    <div id="the-attention-head" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#the-attention-head" aria-label="Anchor">#</a>
    </span>
    
</h3>
<p>You can think each of the attention head as a mini neural network. A typical attention head works like this:</p>
<p><pre class="not-prose mermaid">
flowchart TB

    IN["Previous Layer"] --> Q["Matrix 1"] & K["Matrix 2"] & V["Matrix 3"]
    Q ---> SDPA["Matrix 1 & 2"]
    K ---> SDPA
    SDPA --> n1["Output"]
    V ---> n1
    classDef default fill: transparent, bg-color: transparent

</pre>

<br></p>
<ol>
<li><bullet> Step 1: The input matrix gets converted into multiple matices through matrix multiplication. Most current transformers converts input matrix into three smaller matrices.</bullet></li>
<li><bullet>Step 2: Two of the matrix from step (1) gets combined together using some matrix operation, usually dot products.</bullet></li>
<li><bullet>Step 3: The third matrix from step (1) combines with output from step (2), using some other matrix operation.</bullet></li>
</ol>

<h2 class="relative group">A transformer in action: AI text translation
    <div id="a-transformer-in-action-ai-text-translation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-transformer-in-action-ai-text-translation" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>It&rsquo;s very suprising is that, transformers are able to produce pretty impressive results for tasks model that are not specifically trained for. Here, we&rsquo;ll use <a
  href="https://arxiv.org/pdf/2210.11416"
    target="_blank"
  >Flan-T5, released by Google a couple of years ago</a> as a demo. Flan-T5 is a variation of T5 model that fine-tuned on <em>instructions</em>. That is, we can insert some instructions before our prompt and the model shall return different results based on different instructions.<br></p>
<p>Open terminal, type this command to install dependencies:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>pip install torch transformers
</span></span></code></pre></div><p>And then type:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>python
</span></span></code></pre></div><p>..to open python and start an interactive python session.<br>
Alternatively, if you installed <a
  href="https://ipython.org/install.html"
    target="_blank"
  >ipython</a> or <a
  href="https://jupyter.org/install"
    target="_blank"
  >jupyter notebook</a>, you can start ipython instead:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>ipython
</span></span></code></pre></div><p>Now that we are in python, copy and paste these lines to download &amp; initialise the model.</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#94e2d5">import</span> <span style="color:#fab387">pprint</span>  <span style="color:#6c7086;font-style:italic"># to print indented dictionary</span>
</span></span><span style="display:flex;"><span><span style="color:#94e2d5">from</span> <span style="color:#fab387">transformers</span> <span style="color:#94e2d5">import</span> pipeline
</span></span><span style="display:flex;"><span>pipe <span style="color:#89dceb;font-weight:bold">=</span> pipeline(<span style="color:#a6e3a1">&#39;text2text-generation&#39;</span>, model<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#a6e3a1">&#34;google/flan-t5-base&#34;</span>)
</span></span></code></pre></div><p>Your terminal should looks something like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>config<span style="color:#89dceb;font-weight:bold">.</span>json: <span style="color:#fab387">1.40</span>kB [<span style="color:#fab387">00</span>:<span style="color:#fab387">00</span>, <span style="color:#fab387">336</span>kB<span style="color:#89dceb;font-weight:bold">/</span>s]
</span></span><span style="display:flex;"><span>model<span style="color:#89dceb;font-weight:bold">.</span>safetensors:   <span style="color:#fab387">0</span><span style="color:#89dceb;font-weight:bold">%|</span>                   <span style="color:#89dceb;font-weight:bold">|</span> <span style="color:#fab387">0.00</span><span style="color:#89dceb;font-weight:bold">/</span><span style="color:#fab387">990</span>M [<span style="color:#fab387">00</span>:<span style="color:#fab387">00</span><span style="color:#89dceb;font-weight:bold">&lt;</span><span style="color:#f38ba8">?</span>, <span style="color:#f38ba8">?</span>B<span style="color:#89dceb;font-weight:bold">/</span>s]
</span></span></code></pre></div><p>Whilst you were waiting, here&rsquo;s huggingface&rsquo;s link to the model: <br>
<div class="huggingface-card-wrapper">
    <a id="huggingface-17aca77b8c6d8634048f3a2a5222b58a" target="_blank" href="https://huggingface.co/google/flan-t5-base" class="cursor-pointer">
      <div
        class="w-full md:w-auto p-0 m-0 border border-neutral-200 dark:border-neutral-700 border rounded-md shadow-2xl">
        <div class="w-full md:w-auto pt-3 p-5">
          <div class="flex items-center">
            <span class="text-2xl text-neutral-800 dark:text-neutral me-2">
              <span class="relative inline-block align-text-bottom icon">
                
  
  <svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 475 439"><path d="M235.793 396.126a187.281 187.281 0 00187.285-187.284A187.283 187.283 0 00235.793 21.558 187.287 187.287 0 0048.509 208.842a187.286 187.286 0 00187.284 187.284z" fill="#FFD21E"/><path d="M423.078 208.842A187.283 187.283 0 00235.793 21.558 187.283 187.283 0 0048.509 208.842a187.283 187.283 0 00319.714 132.43 187.284 187.284 0 0054.855-132.43zm-396.127 0a208.842 208.842 0 11417.685 0 208.842 208.842 0 01-417.685 0z" fill="#FF9D0B"/><path d="M296.641 157.912c6.898 2.371 9.593 16.491 16.545 12.827a26.946 26.946 0 008.24-40.841 26.952 26.952 0 00-28.632-8.767 26.942 26.942 0 00-19.055 23.099 26.943 26.943 0 003.014 15.352c3.288 6.198 13.744-3.88 19.941-1.724l-.053.054zm-126.923 0c-6.898 2.371-9.647 16.491-16.545 12.827a26.946 26.946 0 01-8.24-40.841 26.952 26.952 0 0128.632-8.767 26.944 26.944 0 0116.041 38.451c-3.288 6.198-13.797-3.88-19.941-1.724l.053.054z" fill="#3A3B45"/><path d="M234.446 287.205c52.979 0 70.063-47.212 70.063-71.464 0-12.612-8.461-8.624-22.043-1.941-12.557 6.198-29.426 14.768-47.966 14.768-38.75 0-70.063-37.08-70.063-12.827 0 24.252 17.031 71.464 70.063 71.464h-.054z" fill="#FF323D"/><path fill-rule="evenodd" clip-rule="evenodd" d="M193.863 274.863a46.895 46.895 0 0128.565-24.199c2.155-.646 4.365 3.072 6.682 6.899 2.156 3.665 4.42 7.384 6.683 7.384 2.426 0 4.851-3.665 7.168-7.276 2.426-3.773 4.797-7.438 7.115-6.737a46.403 46.403 0 0126.947 22.474c20.103-15.845 27.486-41.715 27.486-57.667 0-12.612-8.461-8.624-22.043-1.941l-.754.378c-12.45 6.198-29.05 14.39-47.266 14.39-18.216 0-34.762-8.192-47.266-14.39-14.012-6.953-22.797-11.318-22.797 1.563 0 16.438 7.869 43.439 29.48 59.122z" fill="#3A3B45"/><path d="M362.446 183.242a17.515 17.515 0 10.002-35.03 17.515 17.515 0 00-.002 35.03zm-250.61 0a17.515 17.515 0 100-35.03 17.515 17.515 0 000 35.03zM75.78 242.526c-8.731 0-16.492 3.557-21.935 10.079a32.173 32.173 0 00-7.168 20.264 38.275 38.275 0 00-10.456-1.617c-8.353 0-15.899 3.18-21.234 8.947a31.257 31.257 0 00-4.312 37.726 28.566 28.566 0 00-9.647 15.198 31.512 31.512 0 004.312 25.546 28.136 28.136 0 00-1.995 27.056c5.498 12.503 19.24 22.312 45.919 32.875 16.545 6.576 31.744 10.779 31.851 10.833a238.92 238.92 0 0058.907 8.623c31.583 0 54.165-9.701 67.153-28.779 20.911-30.666 17.947-58.746-9.162-85.801-14.929-14.983-24.899-37.025-26.947-41.876-4.204-14.336-15.306-30.289-33.684-30.289a30.716 30.716 0 00-24.792 13.258c-5.389-6.79-10.671-12.126-15.414-15.198a39.885 39.885 0 00-21.396-6.845zm0 21.558c2.749 0 6.144 1.186 9.809 3.503 11.533 7.33 33.684 45.434 41.822 60.255 2.695 4.958 7.384 7.06 11.534 7.06 8.353 0 14.821-8.246.808-18.755-21.127-15.792-13.743-41.607-3.665-43.17.431-.108.916-.108 1.294-.108 9.162 0 13.204 15.791 13.204 15.791s11.857 29.75 32.229 50.122c20.318 20.319 21.396 36.649 6.575 58.368-10.132 14.821-29.48 19.295-49.368 19.295-20.533 0-41.66-4.851-53.463-7.869-.593-.162-72.489-20.48-63.38-37.726 1.509-2.911 4.042-4.096 7.222-4.096 12.826 0 36.11 19.078 46.187 19.078 2.21 0 3.773-.916 4.474-3.233 4.257-15.36-64.997-21.828-59.177-44.032 1.078-3.935 3.827-5.498 7.761-5.498 16.923 0 54.973 29.804 62.95 29.804.592 0 1.077-.161 1.293-.539 3.988-6.467 1.778-10.994-26.409-28.025-28.079-17.031-47.858-27.271-36.648-39.505 1.293-1.401 3.126-2.048 5.39-2.048 17.084 0 57.451 36.756 57.451 36.756s10.887 11.318 17.516 11.318c1.509 0 2.802-.539 3.665-2.048 4.635-7.868-43.44-44.301-46.134-59.338-1.833-10.24 1.293-15.36 7.06-15.36z" fill="#FF9D0B"/><path d="M189.39 397.15c14.821-21.773 13.743-38.103-6.575-58.422-20.372-20.318-32.229-50.122-32.229-50.122s-4.419-17.246-14.498-15.629c-10.078 1.617-17.462 27.378 3.665 43.169 21.073 15.792-4.204 26.517-12.342 11.696-8.084-14.821-30.289-52.925-41.822-60.255-11.48-7.276-19.564-3.233-16.87 11.857 2.696 15.037 50.824 51.47 46.135 59.284-4.689 7.923-21.181-9.216-21.181-9.216s-51.577-46.942-62.841-34.708c-11.21 12.234 8.569 22.474 36.648 39.505 28.187 17.031 30.397 21.558 26.409 28.025-4.042 6.468-66.183-45.972-72.004-23.713-5.82 22.15 63.434 28.564 59.177 43.924-4.312 15.36-48.829-28.996-57.883-11.749-9.162 17.3 62.787 37.618 63.38 37.78 23.175 6.036 82.189 18.809 102.831-11.426z" fill="#FFD21E"/><path d="M398.502 242.526c8.731 0 16.545 3.557 21.935 10.079a32.173 32.173 0 017.168 20.264 38.272 38.272 0 0110.509-1.617c8.354 0 15.899 3.18 21.235 8.947a31.257 31.257 0 014.311 37.726 28.564 28.564 0 019.594 15.198 31.513 31.513 0 01-4.312 25.546 28.142 28.142 0 011.994 27.056c-5.497 12.503-19.24 22.312-45.864 32.875-16.6 6.576-31.798 10.779-31.906 10.833a238.914 238.914 0 01-58.907 8.623c-31.582 0-54.164-9.701-67.153-28.779-20.911-30.667-17.947-58.746 9.162-85.801 14.983-14.983 24.954-37.026 27.002-41.876 4.203-14.336 15.252-30.289 33.63-30.289a30.716 30.716 0 0124.792 13.258c5.389-6.791 10.671-12.126 15.467-15.198a39.888 39.888 0 0121.343-6.845zm0 21.558c-2.749 0-6.09 1.186-9.809 3.503-11.48 7.33-33.684 45.434-41.822 60.255a13.106 13.106 0 01-11.534 7.06c-8.3 0-14.821-8.246-.754-18.756 21.072-15.791 13.689-41.606 3.61-43.169a8.233 8.233 0 00-1.293-.108c-9.162 0-13.204 15.791-13.204 15.791s-11.857 29.75-32.175 50.122c-20.373 20.319-21.45 36.649-6.576 58.368 10.079 14.821 29.481 19.295 49.314 19.295 20.588 0 41.661-4.851 53.518-7.869.539-.162 72.488-20.48 63.38-37.726-1.563-2.911-4.042-4.096-7.222-4.096-12.827 0-36.163 19.078-46.188 19.078-2.263 0-3.826-.916-4.473-3.233-4.312-15.36 64.943-21.828 59.122-44.032-1.024-3.935-3.772-5.498-7.76-5.498-16.923 0-54.973 29.804-62.949 29.804-.539 0-1.024-.161-1.24-.539-3.988-6.467-1.832-10.994 26.301-28.025 28.187-17.031 47.966-27.271 36.648-39.505-1.24-1.401-3.072-2.048-5.282-2.048-17.138 0-57.505 36.756-57.505 36.756s-10.887 11.318-17.462 11.318a3.99 3.99 0 01-3.665-2.048c-4.689-7.868 43.385-44.301 46.08-59.338 1.832-10.24-1.294-15.36-7.06-15.36z" fill="#FF9D0B"/><path d="M284.945 397.15c-14.821-21.773-13.797-38.103 6.576-58.422 20.318-20.318 32.175-50.122 32.175-50.122s4.419-17.246 14.551-15.629c10.025 1.617 17.408 27.378-3.664 43.169-21.127 15.792 4.203 26.517 12.287 11.696 8.139-14.821 30.343-52.925 41.823-60.255 11.479-7.276 19.617-3.233 16.869 11.857-2.695 15.037-50.769 51.47-46.08 59.284 4.635 7.923 21.127-9.216 21.127-9.216s51.631-46.942 62.841-34.708c11.21 12.234-8.515 22.474-36.649 39.505-28.186 17.031-30.342 21.558-26.408 28.025 4.042 6.468 66.183-45.972 72.003-23.713 5.821 22.15-63.38 28.564-59.122 43.924 4.311 15.36 48.775-28.996 57.883-11.749 9.108 17.3-62.788 37.618-63.38 37.78-23.229 6.036-82.244 18.809-102.832-11.426z" fill="#FFD21E"/></svg>
  

              </span>
            </span>
            <div
              id="huggingface-17aca77b8c6d8634048f3a2a5222b58a-id"
              class="m-0 font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral">
              google/flan-t5-base
            </div>
          </div><div class="m-0 mt-2 flex items-center">
            <span class="mr-1 inline-block h-3 w-3 rounded-full language-dot" data-language="model"></span>
            <div class="m-0 mr-5 text-md text-neutral-800 dark:text-neutral">
              
                model
              
            </div>

            <span class="text-md mr-1 text-neutral-800 dark:text-neutral">
              <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M244 84L255.1 96L267.1 84.02C300.6 51.37 347 36.51 392.6 44.1C461.5 55.58 512 115.2 512 185.1V190.9C512 232.4 494.8 272.1 464.4 300.4L283.7 469.1C276.2 476.1 266.3 480 256 480C245.7 480 235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1 0 232.4 0 190.9V185.1C0 115.2 50.52 55.58 119.4 44.1C164.1 36.51 211.4 51.37 244 84C243.1 84 244 84.01 244 84L244 84zM255.1 163.9L210.1 117.1C188.4 96.28 157.6 86.4 127.3 91.44C81.55 99.07 48 138.7 48 185.1V190.9C48 219.1 59.71 246.1 80.34 265.3L256 429.3L431.7 265.3C452.3 246.1 464 219.1 464 190.9V185.1C464 138.7 430.4 99.07 384.7 91.44C354.4 86.4 323.6 96.28 301.9 117.1L255.1 163.9z"/></svg></span>
            </span>
            <div id="huggingface-17aca77b8c6d8634048f3a2a5222b58a-likes" class="m-0 mr-5 text-md text-neutral-800 dark:text-neutral">
              1018
            </div>

            <span class="text-md mr-1 text-neutral-800 dark:text-neutral">
              <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M288 32c0-17.7-14.3-32-32-32s-32 14.3-32 32V274.7l-73.4-73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l128 128c12.5 12.5 32.8 12.5 45.3 0l128-128c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L288 274.7V32zM64 352c-35.3 0-64 28.7-64 64v32c0 35.3 28.7 64 64 64H448c35.3 0 64-28.7 64-64V416c0-35.3-28.7-64-64-64H346.5l-45.3 45.3c-25 25-65.5 25-90.5 0L165.5 352H64zM432 456c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
            </span>
            <div id="huggingface-17aca77b8c6d8634048f3a2a5222b58a-downloads" class="m-0 mr-5 text-md text-neutral-800 dark:text-neutral">
              1.185616e&#43;06
            </div>
          </div>
        </div>
      </div>
      
      
      <script
        async
        type="text/javascript"
        src="/js/fetch-repo.min.dc5533c50cefd50405344b235937142271f26229fe39cbee27fd4960e8bb897a0beebfad77a1091ca91cd0d1fb14e70fc37cc114dd9674fb2c32e0ab512ec8a4.js"
        integrity="sha512-3FUzxQzv1QQFNEsjWTcUInHyYin&#43;OcvuJ/1JYOi7iXoL7r&#43;td6EJHKkc0NH7FOcPw3zBFN2WdPssMuCrUS7IpA=="
        data-repo-url="https://huggingface.co/api/models/google/flan-t5-base"
        data-repo-id="huggingface-17aca77b8c6d8634048f3a2a5222b58a"></script>
    </a>
  </div>
</p>
<p>T5 is one of the very few models that comes with very well-documented records on what kind of task the particular model has been trained on.</p>
<p>After downloading, to view the list of tasks the original T5 model fine-tuned on, we can check the &rsquo;task_specific_params&rsquo; attribute in model.config:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pprint<span style="color:#89dceb;font-weight:bold">.</span>pp(pipe<span style="color:#89dceb;font-weight:bold">.</span>model<span style="color:#89dceb;font-weight:bold">.</span>config<span style="color:#89dceb;font-weight:bold">.</span>task_specific_params)
</span></span></code></pre></div><p>Output:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>{<span style="color:#a6e3a1">&#39;summarization&#39;</span>: {<span style="color:#a6e3a1">&#39;early_stopping&#39;</span>: <span style="color:#fab387">True</span>,
</span></span><span style="display:flex;"><span>                   <span style="color:#a6e3a1">&#39;length_penalty&#39;</span>: <span style="color:#fab387">2.0</span>,
</span></span><span style="display:flex;"><span>                   <span style="color:#a6e3a1">&#39;max_length&#39;</span>: <span style="color:#fab387">200</span>,
</span></span><span style="display:flex;"><span>                   <span style="color:#a6e3a1">&#39;min_length&#39;</span>: <span style="color:#fab387">30</span>,
</span></span><span style="display:flex;"><span>                   <span style="color:#a6e3a1">&#39;no_repeat_ngram_size&#39;</span>: <span style="color:#fab387">3</span>,
</span></span><span style="display:flex;"><span>                   <span style="color:#a6e3a1">&#39;num_beams&#39;</span>: <span style="color:#fab387">4</span>,
</span></span><span style="display:flex;"><span>                   <span style="color:#a6e3a1">&#39;prefix&#39;</span>: <span style="color:#a6e3a1">&#39;summarize: &#39;</span>},
</span></span><span style="display:flex;"><span> <span style="color:#a6e3a1">&#39;translation_en_to_de&#39;</span>: {<span style="color:#a6e3a1">&#39;early_stopping&#39;</span>: <span style="color:#fab387">True</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;max_length&#39;</span>: <span style="color:#fab387">300</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;num_beams&#39;</span>: <span style="color:#fab387">4</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;prefix&#39;</span>: <span style="color:#a6e3a1">&#39;translate English to German: &#39;</span>},
</span></span><span style="display:flex;"><span> <span style="color:#a6e3a1">&#39;translation_en_to_fr&#39;</span>: {<span style="color:#a6e3a1">&#39;early_stopping&#39;</span>: <span style="color:#fab387">True</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;max_length&#39;</span>: <span style="color:#fab387">300</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;num_beams&#39;</span>: <span style="color:#fab387">4</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;prefix&#39;</span>: <span style="color:#a6e3a1">&#39;translate English to French: &#39;</span>},
</span></span><span style="display:flex;"><span> <span style="color:#a6e3a1">&#39;translation_en_to_ro&#39;</span>: {<span style="color:#a6e3a1">&#39;early_stopping&#39;</span>: <span style="color:#fab387">True</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;max_length&#39;</span>: <span style="color:#fab387">300</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;num_beams&#39;</span>: <span style="color:#fab387">4</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;prefix&#39;</span>: <span style="color:#a6e3a1">&#39;translate English to Romanian: &#39;</span>}}
</span></span></code></pre></div><p>These configurations are task-specific parameters for <a
  href="https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.generate"
    target="_blank"
  >text-generation</a>. Each item in the dictionary <i>(&rsquo;translation_en_to_de&rsquo;, &lsquo;summarization&rsquo; etc., )</i> corresponds to each text-generation task that the model been previously trained on. As shown in the code block above, the particular model we are testing today was trained on summarization, and three translation tasks: English-German, English-French, and English-Romanian. What we are interested is the &lsquo;prefix&rsquo; key under task name <i style="0.8em">(i.e., &rsquo;translate English to Romanian: &lsquo;)</i>. These are the texts that inserted at the beginning of every text input, as extra instructions of telling our model a bit more information about what it should do.<br>
For example, if one wants to do english-to-romanian translation, the model input would be converted as follows:<br>
        &lsquo;I LOVE FISH!!!!&rsquo; &ndash;&gt; <i>&lsquo;translate English to German: I LOVE FISH!!!!&rsquo;</i><br></p>
<p>.. So instead of languages model already trained on, let&rsquo;s try something model never trined before: English to Spanish translation:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#89dceb">print</span>(pipe(<span style="color:#a6e3a1">&#39;translate English to Spanish: I love fish!!!!!&#39;</span>))
</span></span></code></pre></div><p>Althogh the model was not trained on Spanish translation tasks, it still produced pretty impressive results:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>[{<span style="color:#f38ba8">&#39;generated_text&#39;:</span> <span style="color:#f38ba8">&#39;Me</span> <span style="color:#f38ba8">encanta</span> <span style="color:#f38ba8">el</span> <span style="color:#f38ba8">pescado!&#39;</span>}]
</span></span></code></pre></div><p>You can play around model generations using <code>`pipe('some_text')`</code>, while some_text are whatever text you want the model to generate. But, remeber T5 is trained with text-completion in mind. Meaning that T5 works best for <em>auto-completion</em> types of inputs <em>(&ldquo;Big Mac is the signature dish of &ldquo;)</em> instead of <em>question-answer</em> types of inputs <em>(&ldquo;What restaurant sells Big Mac?&rdquo;)</em>. <br>
For example: <br></p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#89dceb">print</span>(pipe(<span style="color:#a6e3a1">&#39;Microsoft is a machine that turns &#39;</span>)
</span></span></code></pre></div><p>Or, something like:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#89dceb">print</span>(pipe(<span style="color:#a6e3a1">&#34;Tomrrow I must go to the&#34;</span>))
</span></span></code></pre></div><p>It&rsquo;s very uncanny that model is able to do things us human did not ask it to do. There are lot of speculations on why model is able to perform such tasks. For exmaple, <a
  href="https://aclanthology.org/W19-4828"
    target="_blank"
  >some researchers do suggest</a> models that are big enough might [capture meanings behind words as well as language-specific syntax features, and thus are able to convert one language to another. You can view how big the model in the demo is:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pprint<span style="color:#89dceb;font-weight:bold">.</span>pp(<span style="color:#f38ba8">f</span><span style="color:#a6e3a1">&#34;Number of parameters: </span><span style="color:#a6e3a1">{</span>pipe<span style="color:#89dceb;font-weight:bold">.</span>model<span style="color:#89dceb;font-weight:bold">.</span>num_parameters()<span style="color:#a6e3a1">:</span><span style="color:#a6e3a1">,</span><span style="color:#a6e3a1">}</span><span style="color:#a6e3a1">&#34;</span>)
</span></span></code></pre></div><p>Output:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e3a1">&#39;Number of parameters: 247,577,856&#39;</span>
</span></span></code></pre></div><figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="llm is magic text"
    src="./featured.jpg"
    ><figcaption>&lsquo;LLM is magic&rsquo;</figcaption></figure>
<!--  -->

<h2 class="relative group">The Transformer Architecture
    <div id="the-transformer-architecture" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#the-transformer-architecture" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h3 class="relative group">Overview: <em>the</em> transformer itself
    <div id="overview-the-transformer-itself" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#overview-the-transformer-itself" aria-label="Anchor">#</a>
    </span>
    
</h3>
<p>We&rsquo;ll now take a look inside the transformer models and see what kind of math calculations is hapenning. <code>pytorch</code> , the python package that the T5 model in this demo is based off, provies very good tool for visulising model structures Using models mentioned from the previous section, if you want to look at what the model actually lookes like, you can do so by running:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pprint<span style="color:#89dceb;font-weight:bold">.</span>pp(pipe<span style="color:#89dceb;font-weight:bold">.</span>model)
</span></span></code></pre></div><p>&hellip;which in term will give you this monstrous output:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>T5ForConditionalGeneration(
</span></span><span style="display:flex;"><span>  (shared): Embedding(<span style="color:#fab387">32128</span>, <span style="color:#fab387">768</span>)
</span></span><span style="display:flex;"><span>  (encoder): T5Stack(
</span></span><span style="display:flex;"><span>    (embed_tokens): Embedding(<span style="color:#fab387">32128</span>, <span style="color:#fab387">768</span>)
</span></span><span style="display:flex;"><span>    (block): ModuleList(
</span></span><span style="display:flex;"><span>      (<span style="color:#fab387">0</span>): T5Block(
</span></span><span style="display:flex;"><span>        (layer): ModuleList(
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">0</span>): T5LayerSelfAttention(
</span></span><span style="display:flex;"><span>            (SelfAttention): T5Attention(
</span></span><span style="display:flex;"><span>              (q): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (k): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (v): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (o): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (relative_attention_bias): Embedding(<span style="color:#fab387">32</span>, <span style="color:#fab387">12</span>)
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">1</span>): T5LayerFF(
</span></span><span style="display:flex;"><span>            (DenseReluDense): T5DenseGatedActDense(
</span></span><span style="display:flex;"><span>              (wi_0): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (wi_1): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (wo): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (act): NewGELUActivation()
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#fab387">1</span><span style="color:#89dceb;font-weight:bold">-</span><span style="color:#fab387">11</span>): <span style="color:#fab387">11</span> x T5Block(
</span></span><span style="display:flex;"><span>        (layer): ModuleList(
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">0</span>): T5LayerSelfAttention(
</span></span><span style="display:flex;"><span>            (SelfAttention): T5Attention(
</span></span><span style="display:flex;"><span>              (q): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (k): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (v): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (o): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">1</span>): T5LayerFF(
</span></span><span style="display:flex;"><span>            (DenseReluDense): T5DenseGatedActDense(
</span></span><span style="display:flex;"><span>              (wi_0): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (wi_1): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (wo): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (act): NewGELUActivation()
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    (final_layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>    (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (decoder): T5Stack(
</span></span><span style="display:flex;"><span>    (embed_tokens): Embedding(<span style="color:#fab387">32128</span>, <span style="color:#fab387">768</span>)
</span></span><span style="display:flex;"><span>    (block): ModuleList(
</span></span><span style="display:flex;"><span>      (<span style="color:#fab387">0</span>): T5Block(
</span></span><span style="display:flex;"><span>        (layer): ModuleList(
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">0</span>): T5LayerSelfAttention(
</span></span><span style="display:flex;"><span>            (SelfAttention): T5Attention(
</span></span><span style="display:flex;"><span>              (q): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (k): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (v): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (o): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (relative_attention_bias): Embedding(<span style="color:#fab387">32</span>, <span style="color:#fab387">12</span>)
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">1</span>): T5LayerCrossAttention(
</span></span><span style="display:flex;"><span>            (EncDecAttention): T5Attention(
</span></span><span style="display:flex;"><span>              (q): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (k): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (v): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (o): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">2</span>): T5LayerFF(
</span></span><span style="display:flex;"><span>            (DenseReluDense): T5DenseGatedActDense(
</span></span><span style="display:flex;"><span>              (wi_0): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (wi_1): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (wo): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (act): NewGELUActivation()
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#fab387">1</span><span style="color:#89dceb;font-weight:bold">-</span><span style="color:#fab387">11</span>): <span style="color:#fab387">11</span> x T5Block(
</span></span><span style="display:flex;"><span>        (layer): ModuleList(
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">0</span>): T5LayerSelfAttention(
</span></span><span style="display:flex;"><span>            (SelfAttention): T5Attention(
</span></span><span style="display:flex;"><span>              (q): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (k): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (v): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (o): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">1</span>): T5LayerCrossAttention(
</span></span><span style="display:flex;"><span>            (EncDecAttention): T5Attention(
</span></span><span style="display:flex;"><span>              (q): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (k): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (v): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (o): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">2</span>): T5LayerFF(
</span></span><span style="display:flex;"><span>            (DenseReluDense): T5DenseGatedActDense(
</span></span><span style="display:flex;"><span>              (wi_0): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (wi_1): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (wo): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (act): NewGELUActivation()
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    (final_layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>    (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (lm_head): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">32128</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div>
<h2 class="relative group">Why are the models so big?
    <div id="why-are-the-models-so-big" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#why-are-the-models-so-big" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>I wanted to point out the (maybe) obvious thing here: almost all operations mentioned contain learnable parameters. Inside individual attention heads, the three matrics convreted by inputs are typically converted by multiplying (&lsquo;dot product&rsquo;) inputs with three <strong>separate</strong> matrices. These matrics are part of the learnable parameters for the attention head. When we combine matrices, the combination operation also has <a
  href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"
    target="_blank"
  >their own learnable parameters</a>. Furthermore, when we combining outputs from each &lsquo;attention heads&rsquo;, this combining opearation also has its own set of trainable parameters, so on and so on&hellip; Almost every stage of the matrix computations are parameterised, resulting the unbelievably <strong>massive</strong> Aİ models as of today.</p>
<p><strong>So, it&rsquo;s just a huge stack of matirx calculation?</strong><br>
In sense yes, it is. The three matrics in the attention head are commonly named as &lsquo;key&rsquo;. &lsquo;query&rsquo; and &lsquo;value&rsquo;, the idea behind these names are: &lsquo;user queries something, program looks for keys (i.e., keywords) to match with query, and value is whatever gets matched with&rsquo;. Honstly I found this explaination very confusing, although by design, it does (sort of) work in such way. My main skeptcisim is that, just because we vaguely designed it this way does not mean it is really what&rsquo;s happening underneath. As shown in the demo earlier, a model that&rsquo;s not trained for English-Spanish translation did have some ability to do English-Spanish translation, innit. We gave names to these matrics, we don&rsquo;t know much about their behaviour.<br></p>

<h2 class="relative group">Conclusion
    <div id="conclusion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#conclusion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Transformer neural network models aren&rsquo;t as mysterious as one think it would be. It has no difference compared with any other functions. At the end of the day it&rsquo;s just another mathematical function, but takes <em>matrix</em> as inputs, does <em>matrix</em> calculations, and outputs  <em>matrices</em>. Just like any other neural network models, it takes multiple steps to do the calculation. Within each computational step, the inputs gets processed by three mini-neural networks, and outputs are re-combined together as the output of the current step. In this sense, transformers are like nested neural networks: they are bigger neural network that contains lots of mini neural networks.<br></p>
<p><strong>What&rsquo;s up next?</strong><br>
There are still a bit more stuffs that I&rsquo;d like to share, like how text-generation works and what&rsquo;s really happenning when we are training a generative model. We&rsquo;ve heard of the same old things over and over: <em>&lsquo;generative LLM is just a very massive auto-complete!&rsquo;</em>. Whilst I do aggree with it, I also find it not helpful if one wants to <em>understand</em> text generation, for both model training and model inference. In the next post, we&rsquo;ll have a look at the <strong>Stage 3</strong> for text generation.</p>

<h2 class="relative group">Some Other Resources
    <div id="some-other-resources" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#some-other-resources" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>I hope whoever come accross this post would find it useful. Here are some extra reading materials that İ found particularly useful:<br></p>
<ol>
<li><a
  href="https://docs.pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html"
    target="_blank"
  >Pytorch&rsquo;s step-byp-step guide</a> on creating a generative LLM is prob one of the best out there that teaches you all the fundenmentals.</li>
<li><a
  href="https://github.com/jessevig/bertviz"
    target="_blank"
  >BertViz, a very good visualisation tool</a> for looking at attention heads layer by layer. You can run it interactively in a jupyter notebook.</li>
<li><a
  href="https://huggingface.co/learn/llm-course/en/chapter0/1"
    target="_blank"
  >Huggingface&rsquo;s LLM cources.</a> Although they tends to focus on the side of programming &amp; practical applications, I found many of their conceptual guides very good for a beginner.</li>
</ol>

          
          
          
        </div>
        
          
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
    
    
      
    
    
      
      
        
      
      <img
        class="!mt-0 !mb-0 h-24 w-24 rounded-full me-4"
        width="96"
        height="96"
        alt="@Yiheng Yu"
        src="/doggo_hu_866cab9ed3fa5abb.jpg"
        data-zoom-src="/doggo.jpg">
    
  
  <div class="place-self-center">
    
      <div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">
        Author
      </div>
      <div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">
        @Yiheng Yu
      </div>
    
    
      <div class="text-sm text-neutral-700 dark:text-neutral-400">Everyday I feel blessed to live a PHP-free life 🙏</div>
    
    <div class="text-2xl sm:text-lg">
  <div class="flex flex-wrap text-neutral-400 dark:text-neutral-500">
    
      
        <a
          class="px-1 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://github.com/Yiheng-Yu"
          target="_blank"
          aria-label="Github"
          title="Github"
          rel="me noopener noreferrer"
          ><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a
        >
      
    
  </div>

</div>
  </div>
</div>

  

  

  
    <div class="mb-10"></div>
  

        
        

        
  
  <section class="flex flex-row flex-wrap justify-center pt-4 text-xl">
    
      
        <a
          target="_blank"
          class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://bsky.app/intent/compose?text=ELI5%20Transformers%20%28Part%201%29:%20Attention%20Mechanism%20&#43;http://localhost:1313/transformer-eli5/transformers-pt-1/"
          title="Post on Bluesky"
          aria-label="Post on Bluesky">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256,232.562c-21.183,-41.196 -78.868,-117.97 -132.503,-155.834c-51.378,-36.272 -70.978,-29.987 -83.828,-24.181c-14.872,6.72 -17.577,29.554 -17.577,42.988c0,13.433 7.365,110.138 12.169,126.281c15.873,53.336 72.376,71.358 124.413,65.574c2.66,-0.395 5.357,-0.759 8.089,-1.097c-2.68,0.429 -5.379,0.796 -8.089,1.097c-76.259,11.294 -143.984,39.085 -55.158,137.972c97.708,101.165 133.908,-21.692 152.484,-83.983c18.576,62.291 39.972,180.718 150.734,83.983c83.174,-83.983 22.851,-126.674 -53.408,-137.969c-2.71,-0.302 -5.409,-0.667 -8.089,-1.096c2.732,0.337 5.429,0.702 8.089,1.096c52.037,5.785 108.54,-12.239 124.413,-65.574c4.804,-16.142 12.169,-112.847 12.169,-126.281c-0,-13.434 -2.705,-36.267 -17.577,-42.988c-12.85,-5.806 -32.45,-12.09 -83.829,24.181c-53.634,37.864 -111.319,114.635 -132.502,155.831Z"/></svg></span>
        </a>
      
    
      
        <a
          target="_blank"
          class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:1313/transformer-eli5/transformers-pt-1/&amp;title=ELI5%20Transformers%20%28Part%201%29:%20Attention%20Mechanism%20"
          title="Share on LinkedIn"
          aria-label="Share on LinkedIn">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
</span>
        </a>
      
    
      
        <a
          target="_blank"
          class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://s2f.kytta.dev/?text=ELI5%20Transformers%20%28Part%201%29:%20Attention%20Mechanism%20%20http://localhost:1313/transformer-eli5/transformers-pt-1/"
          title="Share via Mastodon"
          aria-label="Share via Mastodon">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.54 102.54 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5zm-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg>
</span>
        </a>
      
    
  </section>


        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_Transformer-ELI5/transformers-pt-1/index.md"
          data-oid-likes="likes_Transformer-ELI5/transformers-pt-1/index.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  


      
        
          <div class="pt-3">
            <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
            <div class="pt-3">
              
            </div>
          </div>
        
      
    </footer>
  </article>

        


  






<div
  id="scroll-to-top"
  class="fixed bottom-6 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
          
          
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 ">
        <ul class="flex list-none flex-col sm:flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 ">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href="/tags/"
                title="Tags">
                
                Tags
              </a>
            </li>
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 ">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href="/categories/"
                title="Categories">
                
                Categories
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          @Yiheng Yu
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>

<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="dark"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title>ELI5 Transformers (Part 1): Attention Mechanism  &middot; Myxiniformes Moment</title>
    <meta name="title" content="ELI5 Transformers (Part 1): Attention Mechanism  &middot; Myxiniformes Moment">
  

  
  
    <meta name="description" content="(it&#39;s glorified linear algebra)">
  
  
    <meta name="keywords" content="huggingface,AI,ELI5,">
  
  
  
  <link rel="canonical" href="http://localhost:1313/posts/transformers-pt-1/">
  

  
  
    <meta name="author" content="@Yiheng Yu">
  
  
    
      
        
          <link href="https://github.com/Yiheng-Yu" rel="me">
        
      
    
  

  
  <meta property="og:url" content="http://localhost:1313/posts/transformers-pt-1/">
  <meta property="og:site_name" content="Myxiniformes Moment">
  <meta property="og:title" content="ELI5 Transformers (Part 1): Attention Mechanism ">
  <meta property="og:description" content="(it’s glorified linear algebra)">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-31T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-31T00:00:00+00:00">
    <meta property="article:tag" content="Huggingface">
    <meta property="article:tag" content="AI">
    <meta property="article:tag" content="ELI5">
    <meta property="og:image" content="http://localhost:1313/posts/transformers-pt-1/featured.jpg">

  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/posts/transformers-pt-1/featured.jpg">
  <meta name="twitter:title" content="ELI5 Transformers (Part 1): Attention Mechanism ">
  <meta name="twitter:description" content="(it’s glorified linear algebra)">

  
  
  
  
    
      
    
  
    
  
    
  
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
    
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.74c3f650a1b9d1325ddc12939a0fb9bbf02734be9a44d8f0e2f534f280127ef49706aad9a7390bb94edde7e4a5cc5c3d57927a84e5b5e9496f725ad08b3e7604.css"
    integrity="sha512-dMP2UKG50TJd3BKTmg&#43;5u/AnNL6aRNjw4vU08oASfvSXBqrZpzkLuU7d5&#43;SlzFw9V5J6hOW16UlvclrQiz52BA==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
    
    <script src="/js/a11y.min.7eeed9f7a6fe7c8aa3b999966484369eb1b5380455116fc15c2b407140f3c2e3b174fc00201797dba34e4d4013213736ae375c9bcf2d7ad1fadb224355d8d54d.js" integrity="sha512-fu7Z96b&#43;fIqjuZmWZIQ2nrG1OARVEW/BXCtAcUDzwuOxdPwAIBeX26NOTUATITc2rjdcm88tetH62yJDVdjVTQ=="></script>
  
  
  
  
    
    <script
      type="text/javascript"
      src="/js/zen-mode.min.9814dee9614d32aeb56239d118edfe20acd6231424f9b19c2dd48835038414130a5e946c0d1c9087d60e60e31840c9babfd217e3d4b95643dc8d651a71ccdf4a.js"
      integrity="sha512-mBTe6WFNMq61YjnRGO3&#43;IKzWIxQk&#43;bGcLdSINQOEFBMKXpRsDRyQh9YOYOMYQMm6v9IX49S5VkPcjWUacczfSg=="></script>
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
    
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9cc802d09f28c6af56ceee7bc6e320a39251fdae98243f2a9942f221ac57a9f49c51609699a91794a7b2580ee1deaa8e4d794a68ffa94aa317c66e893ce51e02.js"
      integrity="sha512-nMgC0J8oxq9Wzu57xuMgo5JR/a6YJD8qmULyIaxXqfScUWCWmakXlKeyWA7h3qqOTXlKaP&#43;pSqMXxm6JPOUeAg=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "",
    "name": "ELI5 Transformers (Part 1): Attention Mechanism ",
    "headline": "ELI5 Transformers (Part 1): Attention Mechanism ",
    
    "abstract": "(it\u0026rsquo;s glorified linear algebra)",
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/posts\/transformers-pt-1\/",
    "author" : {
      "@type": "Person",
      "name": "@Yiheng Yu"
    },
    "copyrightYear": "2025",
    "dateCreated": "2025-10-31T00:00:00\u002b00:00",
    "datePublished": "2025-10-31T00:00:00\u002b00:00",
    
    "dateModified": "2025-10-31T00:00:00\u002b00:00",
    
    "keywords": ["huggingface","AI","ELI5"],
    
    "mainEntityOfPage": "true",
    "wordCount": "3059"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      <div class="min-h-[148px]"></div>
<div class="fixed inset-x-0 z-100">
  <div
    id="menu-blur"
    class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div>
  <div class="relative m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32">
    













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  
    
    

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Myxiniformes Moment
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/posts/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="Blog"
  title="">
  
  
    <p class="text-base font-medium">
      Blog
    </p>
  
</a>



      
    

    

    
      <div class="flex items-center">
    <button
      id="desktop-a11y-toggle"
      aria-label="Open accessibility panel"
      aria-expanded="false"
      type="button"
      class="text-base hover:text-primary-600 dark:hover:text-primary-400"
      role="button"
      aria-pressed="false">
      <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M0 256a256 256 0 1 1 512 0A256 256 0 1 1 0 256zm161.5-86.1c-12.2-5.2-26.3 .4-31.5 12.6s.4 26.3 12.6 31.5l11.9 5.1c17.3 7.4 35.2 12.9 53.6 16.3l0 50.1c0 4.3-.7 8.6-2.1 12.6l-28.7 86.1c-4.2 12.6 2.6 26.2 15.2 30.4s26.2-2.6 30.4-15.2l24.4-73.2c1.3-3.8 4.8-6.4 8.8-6.4s7.6 2.6 8.8 6.4l24.4 73.2c4.2 12.6 17.8 19.4 30.4 15.2s19.4-17.8 15.2-30.4l-28.7-86.1c-1.4-4.1-2.1-8.3-2.1-12.6l0-50.1c18.4-3.5 36.3-8.9 53.6-16.3l11.9-5.1c12.2-5.2 17.8-19.3 12.6-31.5s-19.3-17.8-31.5-12.6L338.7 175c-26.1 11.2-54.2 17-82.7 17s-56.5-5.8-82.7-17l-11.9-5.1zM256 160a40 40 0 1 0 0-80 40 40 0 1 0 0 80z"/></svg>
</span>
    </button>

    <div id="desktop-a11y-overlay" class="fixed inset-0 z-500 hidden"></div>

    <div
      id="desktop-a11y-panel"
      role="dialog"
      aria-labelledby="desktop-a11y-panel-title"
      class="a11y-panel-enter fixed hidden z-500 p-6 top-1/2 left-1/2 -translate-x-1/2 -translate-y-1/2 w-80 rounded-lg shadow-xl bg-neutral-50 dark:bg-neutral-800 border border-neutral-200 dark:border-neutral-700"
      style="min-width: 20rem;">
      <div class="flex items-center justify-between mb-6">
        <h3
          id="desktop-a11y-panel-title"
          class="text-lg font-semibold text-neutral-900 dark:text-neutral-100">
          Accessibility settings
        </h3>
        <button
          id="desktop-a11y-close"
          class="text-neutral-500 hover:text-neutral-700 dark:text-neutral-400 dark:hover:text-neutral-200"
          aria-label="Close a11y panel">
          <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
          </svg>
        </button>
      </div>

      <div class="space-y-5">
        
        
        
        
          
        
        
          <div class="flex items-center justify-between">
            <label for="desktop-disable-blur" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Disable blur
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="desktop-disable-blur">
            </div>
          </div>
          <div class="flex items-center justify-between">
            <label for="desktop-underline-links" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Show link underline
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="desktop-underline-links">
            </div>
          </div>
          <div class="flex items-center justify-between">
            <label for="desktop-zen-mode" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Enable zen mode
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="desktop-zen-mode">
            </div>
          </div>


        <div class="flex items-center justify-between">
          <label
            for="desktop-font-size-select"
            class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
            Font size
          </label>
          <select
            id="desktop-font-size-select"
            class="border rounded-lg px-3 py-1.5 pr-8 text-neutral-900 text-sm dark:bg-neutral-700 dark:text-neutral-200 focus:ring-primary-500 focus:border-primary-500">
            
            
              <option value="default">default</option>
            
              <option value="12px">12px</option>
            
              <option value="14px">14px</option>
            
              <option value="16px">16px</option>
            
              <option value="18px">18px</option>
            
              <option value="20px">20px</option>
            
              <option value="22px">22px</option>
            
              <option value="24px">24px</option>
            
          </select>
        </div>
      </div>
    </div>
  </div>

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    
      <div class="flex items-center">
    <button
      id="mobile-a11y-toggle"
      aria-label="Open accessibility panel"
      aria-expanded="false"
      type="button"
      class="text-base hover:text-primary-600 dark:hover:text-primary-400"
      role="button"
      aria-pressed="false">
      <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M0 256a256 256 0 1 1 512 0A256 256 0 1 1 0 256zm161.5-86.1c-12.2-5.2-26.3 .4-31.5 12.6s.4 26.3 12.6 31.5l11.9 5.1c17.3 7.4 35.2 12.9 53.6 16.3l0 50.1c0 4.3-.7 8.6-2.1 12.6l-28.7 86.1c-4.2 12.6 2.6 26.2 15.2 30.4s26.2-2.6 30.4-15.2l24.4-73.2c1.3-3.8 4.8-6.4 8.8-6.4s7.6 2.6 8.8 6.4l24.4 73.2c4.2 12.6 17.8 19.4 30.4 15.2s19.4-17.8 15.2-30.4l-28.7-86.1c-1.4-4.1-2.1-8.3-2.1-12.6l0-50.1c18.4-3.5 36.3-8.9 53.6-16.3l11.9-5.1c12.2-5.2 17.8-19.3 12.6-31.5s-19.3-17.8-31.5-12.6L338.7 175c-26.1 11.2-54.2 17-82.7 17s-56.5-5.8-82.7-17l-11.9-5.1zM256 160a40 40 0 1 0 0-80 40 40 0 1 0 0 80z"/></svg>
</span>
    </button>

    <div id="mobile-a11y-overlay" class="fixed inset-0 z-500 hidden"></div>

    <div
      id="mobile-a11y-panel"
      role="dialog"
      aria-labelledby="mobile-a11y-panel-title"
      class="a11y-panel-enter fixed hidden z-500 p-6 top-1/2 left-1/2 -translate-x-1/2 -translate-y-1/2 w-80 rounded-lg shadow-xl bg-neutral-50 dark:bg-neutral-800 border border-neutral-200 dark:border-neutral-700"
      style="min-width: 20rem;">
      <div class="flex items-center justify-between mb-6">
        <h3
          id="mobile-a11y-panel-title"
          class="text-lg font-semibold text-neutral-900 dark:text-neutral-100">
          Accessibility settings
        </h3>
        <button
          id="mobile-a11y-close"
          class="text-neutral-500 hover:text-neutral-700 dark:text-neutral-400 dark:hover:text-neutral-200"
          aria-label="Close a11y panel">
          <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
          </svg>
        </button>
      </div>

      <div class="space-y-5">
        
        
        
        
          
        
        
          <div class="flex items-center justify-between">
            <label for="mobile-disable-blur" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Disable blur
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="mobile-disable-blur">
            </div>
          </div>
          <div class="flex items-center justify-between">
            <label for="mobile-underline-links" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Show link underline
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="mobile-underline-links">
            </div>
          </div>
          <div class="flex items-center justify-between">
            <label for="mobile-zen-mode" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Enable zen mode
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="mobile-zen-mode">
            </div>
          </div>


        <div class="flex items-center justify-between">
          <label
            for="mobile-font-size-select"
            class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
            Font size
          </label>
          <select
            id="mobile-font-size-select"
            class="border rounded-lg px-3 py-1.5 pr-8 text-neutral-900 text-sm dark:bg-neutral-700 dark:text-neutral-200 focus:ring-primary-500 focus:border-primary-500">
            
            
              <option value="default">default</option>
            
              <option value="12px">12px</option>
            
              <option value="14px">14px</option>
            
              <option value="16px">16px</option>
            
              <option value="18px">18px</option>
            
              <option value="20px">20px</option>
            
              <option value="22px">22px</option>
            
              <option value="24px">24px</option>
            
          </select>
        </div>
      </div>
    </div>
  </div>

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/posts/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="Blog"
    title="">
    
    
      <p class="text-bg font-bg">
        Blog
      </p>
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>




  <script>
    (function () {
      var $mainmenu = $(".main-menu");
      var path = window.location.pathname;
      $mainmenu.find('a[href="' + path + '"]').each(function (i, e) {
        $(e).children("p").addClass("active");
      });
    })();
  </script>


  </div>
</div>


<script
  type="text/javascript"
  src="/js/background-blur.min.00a57c73ea12f2cab2980c3c3d649e89f6d82f190f74bbe2b67f2f5e39ab7d032ece47086400ca05396758aace13299da49aca43ea643d2625e62c506267a169.js"
  integrity="sha512-AKV8c&#43;oS8sqymAw8PWSeifbYLxkPdLvitn8vXjmrfQMuzkcIZADKBTlnWKrOEymdpJrKQ&#43;pkPSYl5ixQYmehaQ=="
  data-blur-id="menu-blur"></script>

    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        ELI5 Transformers (Part 1): Attention Mechanism 
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  
    
  

  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="2025-10-31T00:00:00&#43;00:00">31 October 2025</time><span class="px-2 text-primary-500">&middot;</span><span>3059 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">15 mins</span>
    

    
    
  </div>

  

  
  
    <div class="flex flex-row flex-wrap items-center">
      
        
      
        
          
        
      
        
          
            
              <a class="relative mt-[0.5rem] me-2" href="/category/eli5-transformers/">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    ELI5-Transformers
  </span>
</span>

              </a>
            
          
        
      
        
      
        
          
            
              <a class="relative mt-[0.5rem] me-2" href="/tags/huggingface/">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Huggingface
  </span>
</span>

              </a>
            
              <a class="relative mt-[0.5rem] me-2" href="/tags/ai/">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    AI
  </span>
</span>

              </a>
            
              <a class="relative mt-[0.5rem] me-2" href="/tags/eli5/">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    ELI5
  </span>
</span>

              </a>
            
          
        
      
    </div>
  

  
  



      </div>
      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          <p>As someone without much backrounds in neither physics nor computer science, I find lots of available introductions on transformers very confusing. Transformers is the talk of the street, GPT is short for &lsquo;Generative Pre-training <i>Transformer</i>&rsquo;! However, most of the articles on transformers focuses on attention mechanisms, using either <a
  href="https://peterbloem.nl/blog/transformers"
    target="_blank"
  >the OG transformer</a> or <a
  href="https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11/"
    target="_blank"
  >the classic BERT</a> as examples. They would spend lot of time talking about embeddings &amp; attetions on the encoding side, and skipped most the decoding by saying &lsquo;well you just do the same thing again and there you have it!&rsquo;. Well that&rsquo;s not very helpful isn&rsquo;t it. Don&rsquo;t get me wrong, there are a lot of very good learning materials out there, for example the amazing <a
  href="https://poloclub.github.io/transformer-explainer"
    target="_blank"
  >interactive transofmer explainer</a>. However, I always find these heavy tutorials not very suitable for my very short attention span or the autism tendency of getting lost in details.<br></p>
<p>Finally, I&rsquo;ve decided to bite the bullet and spend some time have a read through the HuggingFace&rsquo;s source code for google&rsquo;s T5 Model, <a
  href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py"
    target="_blank"
  >THE encoder-decoder everyone on the steet are talking about</a>. It was a relatively long process with lots of back and forth jumping between classes and methods. I could not emphasis how much I appretiate HuggingFace&rsquo;s <a
  href="https://huggingface.co/blog/transformers-design-philosophy"
    target="_blank"
  >maximalist coding choice</a>, where the entire model architecture is contained inside one single .py file. The bonouns point is, I didn&rsquo;t experience the pain of come accross <code>import tensorflow</code> <a
  href="https://github.com/huggingface/transformers/blob/v4.57.1/src/transformers/data/data_collator.py#L742"
    target="_blank"
  >inside dataset iteration</a>. <br></p>
<p>I took lots of notes here and there during the process of studying transformers, think now it&rsquo;s a very good time to share some of my findings. In this (or probably a series of?) blogpost(s?), I am going to collate my past notes on text-specific transformer models piece by piece in a reader-friendly manner. I hope these notes can help others alongside their studying, or being an interesting nice little piece of articles to read through.<br></p>
<p>In this particular post, I would like to do a very brief overview of the transformer model architecture, specifically on the attention mechanism. I won&rsquo;t go metion too much math and there won&rsquo;t be any mathenathical formulas. However, I would assume readers of this silly little post already have some okay-ish background of math/ datascience. (i.e., matrix computations embeddings, tokens, model fitting etc.). I am not going to list out all the implemention details for transformers, since there are a lot of very good materials out there and they are doing fantastic jobs. Instead, in this (maybe series of?) post, İ would like to draw out a general framework on transformers to help one understand the detailed math behind.<br></p>
<p>In this particular blogpost, I&rsquo;ll very quickly go through some very basics on neural network model, just enough to cover what needed for this post, accompied by demo of transformer model as a proof of concept. In this section, there will be some codes that you can copy and paste into an interactive python session to fiddle around for a bit. And lastly, I&rsquo;ll do a quick sketch on the general architecture of transformers, and a overview of the attention mechasm.<br></p>

<h2 class="relative group">A good model only needs to be useful
    <div id="a-good-model-only-needs-to-be-useful" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-good-model-only-needs-to-be-useful" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In order to make things easier to understand, I would wish to start with an inaccuate premise: we can view neural network models as functions that takes some sort of matrix as inputs, do some sort of matrix computations, and output another matrix as the final result. What makes one neural network different from others is how the computation is carried out. It&rsquo;s like \(y=a \times x^2\) is a different function from \(y=a \times sin(x)\), only that in the case of neural network, both x and y are matrics, and the math is much complicated. When it comes to model training, we are essentially trying to find values gives best fit to the data.<br></p>
<p>There&rsquo;s an important assumption here: just because model fits the data well does not mean the model describes mechanisms behind the data. For example, we <i>definitely</i> can fit \(y=a \times sin(x) + b\) to a normal distribution data (like distribution of customer spendings in McDonald&rsquo;s), and it&rsquo;s prob going to be a pretty good fit, but this does not mean the sine function has anything to do with explaining the normal distribution. A good model does not always need to be description, a good model just needs to be useful for its purpose.<br></p>
<p>Transformers are preciesly these kinds of models: they are, surprisingly good at fitting into all sorts of data whilst the math behind the model probably doesn&rsquo;t have much to do with the mechanisms behind. We don&rsquo;t know how transformers works so well for text-based tasks. At least not yet. Originally, transformer was designed as an add-on to the text-processing neural network models in order to tackle with some tricky problems (these problems are not the main forcus of the current blogpost so I&rsquo;m skipping them, but <a
  href="https://towardsdatascience.com/beautifully-illustrated-nlp-models-from-rnn-to-transformer-80d69faf2109/"
    target="_blank"
  >here&rsquo;s a good article if you were interested</a>). We just happened to discover that transformers alone is good enough to solve these problems, we just need to make the transformers much bigger. So that&rsquo;s where the Aİ bloom started: GPT2 solved issues in GPT1 by simply being 10 times bigger; the most-recently open-sourced <a
  href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1"
    target="_blank"
  >pretrained GPT-Oss</a>, is 200 times bigger than <a
  href="https://huggingface.co/openai-community/gpt2"
    target="_blank"
  >the previous openpsourced model, GPT2</a> <i>(note: GPT-OSS is structurlly different from the original GPT2 but the fundamental ideas are the same.)</i>. There are even speculations suggesting transformer neural network models can be seen as some sort of universal function approximator. That is, it&rsquo;s capacable of &lsquo;approximate&rsquo; other formulas/ functions with certain degree of accuracy, providing the model itself is big enough (<a
  href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"
    target="_blank"
  >&lsquo;universal approximation theorem&rsquo;</a>). <br></p>

<h4 class="relative group">Demo: Machine translation with T5
    <div id="demo-machine-translation-with-t5" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#demo-machine-translation-with-t5" aria-label="Anchor">#</a>
    </span>
    
</h4>
<p>It is preciesly the reason why it&rsquo;s very suprising is that, transformers are able to produce pretty impressive results for tasks model that are not specifically trained for. You can try this out yourself. I&rsquo;ll use Flan-T5 as a demo here. Flan-T5 is a variation of T5 model that fine-tuned on instruction-specific tasks. That is, we can insert some instructions before our prompt and the model shall return different results based on different instructions.<br></p>
<p>I&rsquo;ll use Python for the demo here because it&rsquo;s convinent. Before starting, you might want to install <code>transformers</code> if you haven&rsquo;t done aleady. It&rsquo;s a library collecting huge tone of open-sourced transformer models that allows you explore around.<br>
Open terminal, run this command to install transformers:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>pip install torch
</span></span><span style="display:flex;"><span>pip install transformers
</span></span></code></pre></div><p>The model I am going to use is <a
  href="https://arxiv.org/pdf/2210.11416"
    target="_blank"
  >T5, released by Google a couple of years ago</a> Here&rsquo;s huggingface&rsquo;s link to the model: <a
  href="https://huggingface.co/google/flan-t5-base"
    target="_blank"
  >link</a>. <br></p>
<p>In python, run these lines to download &amp; initialise the model:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#94e2d5">from</span> <span style="color:#fab387">transformers</span> <span style="color:#94e2d5">import</span> pipeline
</span></span><span style="display:flex;"><span><span style="color:#94e2d5">import</span> <span style="color:#fab387">pprint</span>  <span style="color:#6c7086;font-style:italic"># to print indented dictionary</span>
</span></span><span style="display:flex;"><span>pipe <span style="color:#89dceb;font-weight:bold">=</span> pipeline(<span style="color:#a6e3a1">&#39;text2text-generation&#39;</span>, model<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#a6e3a1">&#34;google/flan-t5-base&#34;</span>)
</span></span></code></pre></div><p>T5 is one of the very few models that comes with very well-documented records on what kind of task the particular model has been trained on. To view the list of tasks the original T5 model fine-tuned on, we can check the &rsquo;task_specific_params&rsquo; attribute in model.config:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pprint<span style="color:#89dceb;font-weight:bold">.</span>pp(pipe<span style="color:#89dceb;font-weight:bold">.</span>model<span style="color:#89dceb;font-weight:bold">.</span>config<span style="color:#89dceb;font-weight:bold">.</span>task_specific_params)
</span></span></code></pre></div><p>Output:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>{<span style="color:#a6e3a1">&#39;summarization&#39;</span>: {<span style="color:#a6e3a1">&#39;early_stopping&#39;</span>: <span style="color:#fab387">True</span>,
</span></span><span style="display:flex;"><span>                   <span style="color:#a6e3a1">&#39;length_penalty&#39;</span>: <span style="color:#fab387">2.0</span>,
</span></span><span style="display:flex;"><span>                   <span style="color:#a6e3a1">&#39;max_length&#39;</span>: <span style="color:#fab387">200</span>,
</span></span><span style="display:flex;"><span>                   <span style="color:#a6e3a1">&#39;min_length&#39;</span>: <span style="color:#fab387">30</span>,
</span></span><span style="display:flex;"><span>                   <span style="color:#a6e3a1">&#39;no_repeat_ngram_size&#39;</span>: <span style="color:#fab387">3</span>,
</span></span><span style="display:flex;"><span>                   <span style="color:#a6e3a1">&#39;num_beams&#39;</span>: <span style="color:#fab387">4</span>,
</span></span><span style="display:flex;"><span>                   <span style="color:#a6e3a1">&#39;prefix&#39;</span>: <span style="color:#a6e3a1">&#39;summarize: &#39;</span>},
</span></span><span style="display:flex;"><span> <span style="color:#a6e3a1">&#39;translation_en_to_de&#39;</span>: {<span style="color:#a6e3a1">&#39;early_stopping&#39;</span>: <span style="color:#fab387">True</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;max_length&#39;</span>: <span style="color:#fab387">300</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;num_beams&#39;</span>: <span style="color:#fab387">4</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;prefix&#39;</span>: <span style="color:#a6e3a1">&#39;translate English to German: &#39;</span>},
</span></span><span style="display:flex;"><span> <span style="color:#a6e3a1">&#39;translation_en_to_fr&#39;</span>: {<span style="color:#a6e3a1">&#39;early_stopping&#39;</span>: <span style="color:#fab387">True</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;max_length&#39;</span>: <span style="color:#fab387">300</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;num_beams&#39;</span>: <span style="color:#fab387">4</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;prefix&#39;</span>: <span style="color:#a6e3a1">&#39;translate English to French: &#39;</span>},
</span></span><span style="display:flex;"><span> <span style="color:#a6e3a1">&#39;translation_en_to_ro&#39;</span>: {<span style="color:#a6e3a1">&#39;early_stopping&#39;</span>: <span style="color:#fab387">True</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;max_length&#39;</span>: <span style="color:#fab387">300</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;num_beams&#39;</span>: <span style="color:#fab387">4</span>,
</span></span><span style="display:flex;"><span>                          <span style="color:#a6e3a1">&#39;prefix&#39;</span>: <span style="color:#a6e3a1">&#39;translate English to Romanian: &#39;</span>}}
</span></span></code></pre></div><p>These configurations are task-specific parameters for <a
  href="https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.generate"
    target="_blank"
  >text-generation</a>. Each item in the dictionary <i>(&rsquo;translation_en_to_de&rsquo;, &lsquo;summarization&rsquo; etc., )</i> corresponds to each text-generation task that the model been previously trained on. As shown in the code block above, the particular model we are testing today was trained on summarization, and three translation tasks: English-German, English-French, and English-Romanian.What we are interested is the &lsquo;prefix&rsquo; key under task name <i style="0.8em">(i.e., &rsquo;translate English to Romanian: &lsquo;)</i>. These are the texts that inserted at the beginning of every text input, as extra instructions of telling our model a bit more information about what it should do.<br>
For example, if one wants to do english-to-romanian translation, the model input would be converted as follows:<br>
        &lsquo;I LOVE FISH!!!!&rsquo; &ndash;&gt; <i>&lsquo;translate English to German: I LOVE FISH!!!!&rsquo;</i><br></p>
<p>..So instead of languages model already trained on, let&rsquo;s try something model never trined before: English to Spanish translation:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#89dceb">print</span>(pipe(<span style="color:#a6e3a1">&#39;translate English to Spanish: I love fish!!!!!&#39;</span>))
</span></span></code></pre></div><p>Althogh the model was not trained on Spanish translation tasks, it still produced pretty impressive results:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>[{<span style="color:#f38ba8">&#39;generated_text&#39;:</span> <span style="color:#f38ba8">&#39;Me</span> <span style="color:#f38ba8">encanta</span> <span style="color:#f38ba8">el</span> <span style="color:#f38ba8">pescado!&#39;</span>}]
</span></span></code></pre></div><p>It&rsquo;s very uncanny that model is able to do things us human did not ask it to do. There are lot of speculations on why model is able to perform such tasks. For exmaple, some researchers do suggest models that are big enough might <a
  href="https://aclanthology.org/W19-4828/"
    target="_blank"
  >capture meanings behind words as well as language-specific syntax features</a>, and thus are able to convert one language to another. You can view how big the model in the demo is:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pprint<span style="color:#89dceb;font-weight:bold">.</span>pp(<span style="color:#f38ba8">f</span><span style="color:#a6e3a1">&#34;Number of parameters: </span><span style="color:#a6e3a1">{</span>pipe<span style="color:#89dceb;font-weight:bold">.</span>model<span style="color:#89dceb;font-weight:bold">.</span>num_parameters()<span style="color:#a6e3a1">:</span><span style="color:#a6e3a1">,</span><span style="color:#a6e3a1">}</span><span style="color:#a6e3a1">&#34;</span>)
</span></span></code></pre></div><p>Output:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e3a1">&#39;Number of parameters: 247,577,856&#39;</span>
</span></span></code></pre></div><figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="llm is magic text"
    src="./featured.jpg"
    ><figcaption>&lsquo;LLM is magic&rsquo;</figcaption></figure>
<!--  -->

<h2 class="relative group">The Transformer Architecture
    <div id="the-transformer-architecture" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#the-transformer-architecture" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h3 class="relative group">Overview: <em>the</em> transformer itself
    <div id="overview-the-transformer-itself" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#overview-the-transformer-itself" aria-label="Anchor">#</a>
    </span>
    
</h3>
<p>We&rsquo;ll now take a look inside the transformer models and see what kind of math calculations is hapenning. <code>pytorch</code>, the python package that the T5 model in this demo is based off, provies very good tool for visulising model structures Using models mentioned from the previous section, if you want to look at what the model actually lookes like, you can do so by running:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pprint<span style="color:#89dceb;font-weight:bold">.</span>pp(pipe<span style="color:#89dceb;font-weight:bold">.</span>model)
</span></span></code></pre></div><p>&hellip;which in term will give you this monstrous output:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>T5ForConditionalGeneration(
</span></span><span style="display:flex;"><span>  (shared): Embedding(<span style="color:#fab387">32128</span>, <span style="color:#fab387">768</span>)
</span></span><span style="display:flex;"><span>  (encoder): T5Stack(
</span></span><span style="display:flex;"><span>    (embed_tokens): Embedding(<span style="color:#fab387">32128</span>, <span style="color:#fab387">768</span>)
</span></span><span style="display:flex;"><span>    (block): ModuleList(
</span></span><span style="display:flex;"><span>      (<span style="color:#fab387">0</span>): T5Block(
</span></span><span style="display:flex;"><span>        (layer): ModuleList(
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">0</span>): T5LayerSelfAttention(
</span></span><span style="display:flex;"><span>            (SelfAttention): T5Attention(
</span></span><span style="display:flex;"><span>              (q): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (k): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (v): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (o): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (relative_attention_bias): Embedding(<span style="color:#fab387">32</span>, <span style="color:#fab387">12</span>)
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">1</span>): T5LayerFF(
</span></span><span style="display:flex;"><span>            (DenseReluDense): T5DenseGatedActDense(
</span></span><span style="display:flex;"><span>              (wi_0): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (wi_1): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (wo): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (act): NewGELUActivation()
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#fab387">1</span><span style="color:#89dceb;font-weight:bold">-</span><span style="color:#fab387">11</span>): <span style="color:#fab387">11</span> x T5Block(
</span></span><span style="display:flex;"><span>        (layer): ModuleList(
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">0</span>): T5LayerSelfAttention(
</span></span><span style="display:flex;"><span>            (SelfAttention): T5Attention(
</span></span><span style="display:flex;"><span>              (q): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (k): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (v): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (o): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">1</span>): T5LayerFF(
</span></span><span style="display:flex;"><span>            (DenseReluDense): T5DenseGatedActDense(
</span></span><span style="display:flex;"><span>              (wi_0): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (wi_1): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (wo): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (act): NewGELUActivation()
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    (final_layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>    (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (decoder): T5Stack(
</span></span><span style="display:flex;"><span>    (embed_tokens): Embedding(<span style="color:#fab387">32128</span>, <span style="color:#fab387">768</span>)
</span></span><span style="display:flex;"><span>    (block): ModuleList(
</span></span><span style="display:flex;"><span>      (<span style="color:#fab387">0</span>): T5Block(
</span></span><span style="display:flex;"><span>        (layer): ModuleList(
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">0</span>): T5LayerSelfAttention(
</span></span><span style="display:flex;"><span>            (SelfAttention): T5Attention(
</span></span><span style="display:flex;"><span>              (q): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (k): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (v): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (o): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (relative_attention_bias): Embedding(<span style="color:#fab387">32</span>, <span style="color:#fab387">12</span>)
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">1</span>): T5LayerCrossAttention(
</span></span><span style="display:flex;"><span>            (EncDecAttention): T5Attention(
</span></span><span style="display:flex;"><span>              (q): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (k): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (v): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (o): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">2</span>): T5LayerFF(
</span></span><span style="display:flex;"><span>            (DenseReluDense): T5DenseGatedActDense(
</span></span><span style="display:flex;"><span>              (wi_0): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (wi_1): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (wo): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (act): NewGELUActivation()
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#fab387">1</span><span style="color:#89dceb;font-weight:bold">-</span><span style="color:#fab387">11</span>): <span style="color:#fab387">11</span> x T5Block(
</span></span><span style="display:flex;"><span>        (layer): ModuleList(
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">0</span>): T5LayerSelfAttention(
</span></span><span style="display:flex;"><span>            (SelfAttention): T5Attention(
</span></span><span style="display:flex;"><span>              (q): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (k): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (v): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (o): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">1</span>): T5LayerCrossAttention(
</span></span><span style="display:flex;"><span>            (EncDecAttention): T5Attention(
</span></span><span style="display:flex;"><span>              (q): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (k): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (v): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (o): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>          (<span style="color:#fab387">2</span>): T5LayerFF(
</span></span><span style="display:flex;"><span>            (DenseReluDense): T5DenseGatedActDense(
</span></span><span style="display:flex;"><span>              (wi_0): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (wi_1): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (wo): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>              (act): NewGELUActivation()
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            (layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>            (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          )
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    (final_layer_norm): T5LayerNorm()
</span></span><span style="display:flex;"><span>    (dropout): Dropout(p<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">0.1</span>, inplace<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (lm_head): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">768</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">32128</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>Despite its indimidate looks, at conceptual level, our T5 model is actually pretty simple. It does have a lot of layers, however, most of the layers are more or less the same, apart from the very beginning (<code>Embedding(32128, 768)</code>) and the very end (<code>lm_head</code>). Not only just T5, most of the popular transformer models that you heard of (GPT, Gemmini etc.,) all share similar three-stage structure like T5. Here&rsquo;s a simplified flow chart:</p>
<body>
    <pre style="font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><code>╭─────────────────────────────────────────╮
│                Embedding                │
╰─────────────────────────────────────────╯
╭─────────────────────────────────────────╮
│                    ↓                    │
╰─────────────────────────────────────────╯
╭──────────── Attention Layer ────────────╮
│ ╭───────────╮╭───────────╮╭───────────╮ │
│ │ Attention ││ Attention ││           │ │
│ │ Head      ││ Head      ││ ...       │ │
│ ╰───────────╯╰───────────╯╰───────────╯ │
╰─────────────────────────────────────────╯
╭─────────────────────────────────────────╮
│                    ↓                    │
╰─────────────────────────────────────────╯
╭──────────── Attention Layer ────────────╮
│ ╭───────────╮╭───────────╮╭───────────╮ │
│ │ Attention ││ Attention ││           │ │
│ │ Head      ││ Head      ││ ...       │ │
│ ╰───────────╯╰───────────╯╰───────────╯ │
╰─────────────────────────────────────────╯
╭─────────────────────────────────────────╮
│                    ↓                    │
╰─────────────────────────────────────────╯
╭─────────────────────────────────────────╮
│                   ...                   │
╰─────────────────────────────────────────╯
╭─────────────────────────────────────────╮
│                    ↓                    │
╰─────────────────────────────────────────╯
╭─────────────────────────────────────────╮
│                 Output                  │
╰─────────────────────────────────────────╯
</code></pre>
</body>
<p>These three stages are:</p>
<ol>
<li><bullet><em>The input gets converted into vectors or matrics.</em><br>
The first step starts with creating a mathematical representation of our input data. This process can vary based on different types of inputs. It can simply be some sort of look-up tables (text embedding), some matrix transformations of the raw inputs (convolution) etc. <br>
In our T5 mdoel, this corresponds to the <em><code>Embedding(32128, 768)</code></em> shown in our T5 model, or the <em>&lsquo;Embedding&rsquo;</em> in the flow chart.<br></bullet></li>
<li><bullet><em>The raw output from Stage 1 feeds into the multiple different attention layers.</em><br>
Mathematically, each attention layer is doing very much the same mathematical operation, with each layer having its own sets of parameters. Each layer takes a matrix as an input, and outputs another matrix to pass onto the next layer. This process is repeated multiple times. Stage 2 is the core of a transformer model, it <em>transforms</em> our inputs into something else.<br>
In our T5 model, this coressponds to the <em><code>encoder</code></em> and the <em><code>deocoder</code></em>, or the <em>&lsquo;Attention Layers&rsquo;</em> in the flow chart.
<bullet></li>
<li><bullet><em>We convert the matrix output from Stage 2 into task-specific results.</em><br>
This is usually done by another set of simple matrix operations, depending on the task. For example, if we are doing text sentimental analysis task, this operation could be a simple matrix multiplication, resulting in a final score of 0-10.<br>
In our T5 model, this coresspinds to the <em><code>lm_head</code></em> at the end of the T5, or the <em>Output</em> in the flow chart.
</bullet><br></li>
</ol>
<p>In other words, you can concepturally see <strong>Stage 1</strong> as a conversion stage in order to initiate the model, <strong>Stage 2</strong> being the core of a transformer model, and <strong>Stage 3</strong> as a &lsquo;decoding&rsquo; step to convert the output back into human-readable form. The rest of the post will focus on <strong>Stage 2</strong>, the core, and I&rsquo;ll elabrate a lot more on what&rsquo;s hapenning in <strong>Stage 3</strong> in the next post.</p>

<h3 class="relative group">The attention head
    <div id="the-attention-head" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#the-attention-head" aria-label="Anchor">#</a>
    </span>
    
</h3>
<p>Each attention layer consists of multiple attention heads <em>(<code>T5Attention</code>)</em>, each of them operated independent from one another: they take output matrix from the previous layer as the input, do some mathematical calculations, and output the transformed matrix. Outputs from each of the attention heads are then joined together <em>(<code>layer_norm</code>)</em>. The joined matrix is the final output of the current layer. <br></p>
<p>You can think each of the attention head as a mini neural network. It takes some inputs and spits out some outputs.<br></p>
<p>A typical attention head works like this:</p>
<ol>
<li><bullet>The input matrix gets converted into multiple matices through matrix multiplication. Most current transformers converts input matrix into three smaller matrices.</bullet></li>
<li><bullet>Two of the matrix from step (1) gets combined together using some matrix operation, usually dot products.</bullet></li>
<li><bullet>The third matrix from step (1) combines with output from step (2), using some other matrix operation.</bullet></li>
</ol>
<body>
    <pre style="font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><code style="font-family:inherit">╭──────────────────────────────────────────────╮
│              * previous layer *              │
╰──────────────────────────────────────────────╯
     ↓           ↓                              
╭──────────╮╭──────────╮                        
│ Matrix 1 ││ Matrix 2 │           ↓            
╰──────────╯╰──────────╯                        
     ↓           ↓                              
╭──────────────────────╮╭──────────────────────╮
│ Matrix 1 &amp; 2 Merged  ││       Matrix 3       │
╰──────────────────────╯╰──────────────────────╯
           ↓                       ↓            
<p>╭──────────────────────────────────────────────╮
│          Matrix 1 &amp; 2 &amp; 3 Re-Joined          │
╰──────────────────────────────────────────────╯
╭──────────────────────────────────────────────╮
│                    ↓ ↓ ↓                     │
╰──────────────────────────────────────────────╯
╭──────────────────────────────────────────────╮
│              * to be combined *              │
╰──────────────────────────────────────────────╯</code></pre></p>
</body>

<h2 class="relative group">Why are the models so big?
    <div id="why-are-the-models-so-big" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#why-are-the-models-so-big" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>I wanted to point out the (maybe) obvious thing here: almost all operations mentioned contain learnable parameters. Inside individual attention heads, the three matrics convreted by inputs are typically converted by multiplying (&lsquo;dot product&rsquo;) inputs with three <strong>separate</strong> matrices. These matrics are part of the learnable parameters for the attention head. When we combine matrices, the combination operation also has <a
  href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"
    target="_blank"
  >their own learnable parameters</a>. Furthermore, when we combining outputs from each &lsquo;attention heads&rsquo;, this combining opearation also has its own set of trainable parameters, so on and so on&hellip;Almost every stage of the matrix computations are parameterised, resulting the unbelievably <strong>massive</strong> Aİ models as of today.</p>
<p><strong>So, it&rsquo;s just a huge stack of matirx calculation?</strong><br>
In sense yes, it is. The three matrics in the attention head are commonly named as &lsquo;key&rsquo;. &lsquo;query&rsquo; and &lsquo;value&rsquo;, the idea behind these names are: &lsquo;user queries something, program looks for keys (i.e., keywords) to match with query, and value is whatever gets matched with&rsquo;. Honstly I found this explaination very confusing, although by design, it does (sort of) work in such way. My main skeptcisim is that, just because we vaguely designed it this way does not mean it is really what&rsquo;s happening underneath. As shown in the demo earlier, a model that&rsquo;s not trained for English-Spanish translation did have some ability to do English-Spanish translation, innit. We gave names to these matrics, we don&rsquo;t know much about their behaviour.<br></p>

<h2 class="relative group">What&rsquo;s up next?<br>
    <div id="whats-up-next" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#whats-up-next" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>I think this is a rather good place to conclude this post, so I&rsquo;ll leave it here for now. I hope you find it helpfull.<br></p>
<p>There are still a bit more stuffs that I&rsquo;d like to share, like how text-generation works and what&rsquo;s really happenning when we are training a generative model. We&rsquo;ve heard of the same old things over and over: <em>&lsquo;generative LLM is just a very massive auto-complete!&rsquo;</em>. Whilst I do aggree with it, I also find it not helpful if one wants to <em>understand</em> text generation, for both model training and model inference. In the next post, we&rsquo;ll have a look at the <strong>Stage 3</strong> for text generation.</p>

<h2 class="relative group">Some Other Resources
    <div id="some-other-resources" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#some-other-resources" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>I hope whoever come accross this post would find it useful. Here are some extra reading materials that İ found particularly useful:<br></p>
<ol>
<li><a
  href="https://docs.pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html"
    target="_blank"
  >Pytorch&rsquo;s step-byp-step guide on creating a generative LLM is prob one of the best out there that teaches you all the fundenmentals.</a></li>
<li><a
  href="https://github.com/jessevig/bertviz"
    target="_blank"
  >BertViz, a very good visualisation tool for looking at attention heads layer by layer. You can run it interactively in a jupyter notebook.</a></li>
<li><a
  href="https://huggingface.co/learn/llm-course/en/chapter0/1"
    target="_blank"
  >Huggingface&rsquo;s LLM cources. Although they tends to focus on the programming &amp; practical applications, I found many of their conceptual guides very good for a beginner.</a></li>
</ol>

          
          
          
        </div>
        
          
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
    
    
      
    
    
      
      
        
      
      <img
        class="!mt-0 !mb-0 h-24 w-24 rounded-full me-4"
        width="96"
        height="96"
        alt="@Yiheng Yu"
        src="/doggo_hu_866cab9ed3fa5abb.jpg"
        data-zoom-src="/doggo.jpg">
    
  
  <div class="place-self-center">
    
      <div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">
        Author
      </div>
      <div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">
        @Yiheng Yu
      </div>
    
    
      <div class="text-sm text-neutral-700 dark:text-neutral-400">The the industrial revolution and its consequence is auto-threading sewing machines (they are very epic)</div>
    
    <div class="text-2xl sm:text-lg">
  <div class="flex flex-wrap text-neutral-400 dark:text-neutral-500">
    
      
        <a
          class="px-1 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://github.com/Yiheng-Yu"
          target="_blank"
          aria-label="Github"
          title="Github"
          rel="me noopener noreferrer"
          ><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a
        >
      
    
  </div>

</div>
  </div>
</div>

  

  

  
    <div class="mb-10"></div>
  

        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_posts/transformers-pt-1/index.md"
          data-oid-likes="likes_posts/transformers-pt-1/index.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  


      
    </footer>
  </article>

        


  






<div
  id="scroll-to-top"
  class="fixed bottom-6 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
          
          
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 ">
        <ul class="flex list-none flex-col sm:flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 ">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href="/tags/"
                title="Tags">
                
                Tags
              </a>
            </li>
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 ">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href="/categories/"
                title="Categories">
                
                Categories
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          @Yiheng Yu
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>

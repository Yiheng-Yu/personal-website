<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="dark"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title>ELI5 Transformers (Part 1): Attention Mechanism  &middot; Myxiniformes Moment</title>
    <meta name="title" content="ELI5 Transformers (Part 1): Attention Mechanism  &middot; Myxiniformes Moment">
  

  
  
    <meta name="description" content="(it&#39;s glorified linear algebra)">
  
  
    <meta name="keywords" content="AI,Machine Learning,ELI5,">
  
  
  
  <link rel="canonical" href="http://localhost:1313/posts/transformers-pt-1/">
  

  
  
    <meta name="author" content="@Yiheng Yu">
  
  
    
      
        
          <link href="https://github.com/Yiheng-Yu" rel="me">
        
      
    
  

  
  <meta property="og:url" content="http://localhost:1313/posts/transformers-pt-1/">
  <meta property="og:site_name" content="Myxiniformes Moment">
  <meta property="og:title" content="ELI5 Transformers (Part 1): Attention Mechanism ">
  <meta property="og:description" content="(it’s glorified linear algebra)">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-31T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-31T00:00:00+00:00">
    <meta property="article:tag" content="AI">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="ELI5">
    <meta property="og:image" content="http://localhost:1313/posts/transformers-pt-1/featured.jpg">

  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/posts/transformers-pt-1/featured.jpg">
  <meta name="twitter:title" content="ELI5 Transformers (Part 1): Attention Mechanism ">
  <meta name="twitter:description" content="(it’s glorified linear algebra)">

  
  
  
  
    
      
    
  
    
  
    
  
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
    
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.bec4af756dcf5b3898dd42614136c2600b18525d856358b8e491de1db63e490a262c45c9aa46252bc50d70436f2d60843c1072ddcb79a8397eb00f1c41e2c667.css"
    integrity="sha512-vsSvdW3PWziY3UJhQTbCYAsYUl2FY1i45JHeHbY&#43;SQomLEXJqkYlK8UNcENvLWCEPBBy3ct5qDl&#43;sA8cQeLGZw==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
    
    <script src="/js/a11y.min.7eeed9f7a6fe7c8aa3b999966484369eb1b5380455116fc15c2b407140f3c2e3b174fc00201797dba34e4d4013213736ae375c9bcf2d7ad1fadb224355d8d54d.js" integrity="sha512-fu7Z96b&#43;fIqjuZmWZIQ2nrG1OARVEW/BXCtAcUDzwuOxdPwAIBeX26NOTUATITc2rjdcm88tetH62yJDVdjVTQ=="></script>
  
  
  
  
    
    <script
      type="text/javascript"
      src="/js/zen-mode.min.9814dee9614d32aeb56239d118edfe20acd6231424f9b19c2dd48835038414130a5e946c0d1c9087d60e60e31840c9babfd217e3d4b95643dc8d651a71ccdf4a.js"
      integrity="sha512-mBTe6WFNMq61YjnRGO3&#43;IKzWIxQk&#43;bGcLdSINQOEFBMKXpRsDRyQh9YOYOMYQMm6v9IX49S5VkPcjWUacczfSg=="></script>
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
    
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9cc802d09f28c6af56ceee7bc6e320a39251fdae98243f2a9942f221ac57a9f49c51609699a91794a7b2580ee1deaa8e4d794a68ffa94aa317c66e893ce51e02.js"
      integrity="sha512-nMgC0J8oxq9Wzu57xuMgo5JR/a6YJD8qmULyIaxXqfScUWCWmakXlKeyWA7h3qqOTXlKaP&#43;pSqMXxm6JPOUeAg=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>



  
  
  
  
  <script
    defer
    type="text/javascript"
    src="/js/mermaid.bundle.434182ec73aab0e02e11611f98689ca73dc14816078abc05abf71756ee2d87f489797a2bc72983d2a85588b63e5d29de4a6b5340a2ee9d7415dea86cb4f2c575.js"
    integrity="sha512-Q0GC7HOqsOAuEWEfmGicpz3BSBYHirwFq/cXVu4th/SJeXorxymD0qhViLY&#43;XSneSmtTQKLunXQV3qhstPLFdQ=="></script>







  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/lib/katex/katex.min.9e578d7ac3b5270b37ae32edae7835a744cda7734036e6e2f1ccf16f382b056ccd51c0df6b7ef3470991357a026507840aa22c1740ea2fe6bbce9b4223c5c182.css"
    integrity="sha512-nleNesO1Jws3rjLtrng1p0TNp3NANubi8czxbzgrBWzNUcDfa37zRwmRNXoCZQeECqIsF0DqL&#43;a7zptCI8XBgg==">
  
  
  
  <script
    defer
    type="text/javascript"
    src="/js/katex.bundle.a10a4a0655682a978b0422e48a40f3515d4d96c8e600a48e3dad34029a55cc00b128b89a908855227d1c07897ce09cdfe9f0d57decb148cb73f17dbb8a0a715c.js"
    integrity="sha512-oQpKBlVoKpeLBCLkikDzUV1NlsjmAKSOPa00AppVzACxKLiakIhVIn0cB4l84Jzf6fDVfeyxSMtz8X27igpxXA=="
    id="katex-render"></script>
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  










  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/components/carousel.8e65f2c27285218feed8ca2a6e12a80ec2fb4be8f774b7c74d9642f1bc528a3352288a700a64a0e1e3c38a40b74c978b5dbbeb38346202b5f7613d8fc023e3c9.css"
    integrity="sha512-jmXywnKFIY/u2MoqbhKoDsL7S&#43;j3dLfHTZZC8bxSijNSKIpwCmSg4ePDikC3TJeLXbvrODRiArX3YT2PwCPjyQ==">
  
  
  <script
    defer
    type="text/javascript"
    src="/lib/tw-elements/index.min.5ffccf337b773ee831515cdde152c8039e972dd479f7dffb0532a79de9a6de5926154ae2587f181ed6c06a99aee25667096bcf6495da5aa86e235d98baee5943.js"
    integrity="sha512-X/zPM3t3PugxUVzd4VLIA56XLdR599/7BTKnnemm3lkmFUriWH8YHtbAapmu4lZnCWvPZJXaWqhuI12Yuu5ZQw=="></script>









  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Blog",
    "name": "ELI5 Transformers (Part 1): Attention Mechanism ",
    "headline": "ELI5 Transformers (Part 1): Attention Mechanism ",
    
    "abstract": "(it\u0026rsquo;s glorified linear algebra)",
    "inLanguage": "en",
    "url" : "http://localhost:1313/posts/transformers-pt-1/",
    "author" : {
      "@type": "Person",
      "name": "@Yiheng Yu"
    },
    "copyrightYear": "2025",
    "dateCreated": "2025-10-31T00:00:00\u002b00:00",
    "datePublished": "2025-10-31T00:00:00\u002b00:00",
    
    "dateModified": "2025-10-31T00:00:00\u002b00:00",
    
    "keywords": ["AI","Machine Learning","ELI5"],
    
    "mainEntityOfPage": "true",
    "wordCount": "2617"
  }]
  </script>



  
  

  
  

  
  

  
  
    
      <script src="https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js"></script>
      <script src="https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js"></script>
      <script src="https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js"></script>
      <script>

    const firebaseConfig = {
      apiKey: "AIzaSyC7c0UZ3xECttjMuZWh5AE41qOMLl5bAcg",
      authDomain: "yiheng-blog-stats.firebaseapp.com",
      projectId: "yiheng-blog-stats",
      storageBucket: "yiheng-blog-stats.firebasestorage.app",
      messagingSenderId: "674513555332",
      appId: "1:674513555332:web:fcc2cd0ba68d0adc0017d1",
      measurementId: "G-4X2W77Z3VD"
    };

        var app = firebase.initializeApp(firebaseConfig);
        var db = firebase.firestore();
        var auth = firebase.auth();

      </script>
    
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      <div class="min-h-[148px]"></div>
<div class="fixed inset-x-0 bg-neutral dark:bg-neutral-800 z-100">
  <div class="relative m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32">
    













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  
    
    

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Myxiniformes Moment
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/posts/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="Blog"
  title="Blog">
  
  
    <p class="text-base font-medium">
      Blog
    </p>
  
</a>



      
        
  <div>
  <div class="cursor-pointer flex items-center nested-menu">
    
    <a
      
        href="/topics/"
        
      
      aria-label="Topics"
      class="text-base font-medium hover:text-primary-600 dark:hover:text-primary-400"
      title="Topics">
      <p>
        Topics
      </p>
    </a>
    <span>
      <span class="relative block icon"><svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>
</span>
    </span>
  </div>
  <div class="absolute menuhide">
    <div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl">
      <div class="flex flex-col space-y-3">
        
          <a
            href="/topics/transformers/"
            
            aria-label="Transformer"
            class="flex items-center hover:text-primary-600 dark:hover:text-primary-400">
            
            <p class="text-sm font-sm" title="Transformer">
              Transformer
            </p>
          </a>
        
      </div>
    </div>
  </div>
</div>



      
    

    

    
      <div class="flex items-center">
    <button
      id="desktop-a11y-toggle"
      aria-label="Open accessibility panel"
      aria-expanded="false"
      type="button"
      class="text-base hover:text-primary-600 dark:hover:text-primary-400"
      role="button"
      aria-pressed="false">
      <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M0 256a256 256 0 1 1 512 0A256 256 0 1 1 0 256zm161.5-86.1c-12.2-5.2-26.3 .4-31.5 12.6s.4 26.3 12.6 31.5l11.9 5.1c17.3 7.4 35.2 12.9 53.6 16.3l0 50.1c0 4.3-.7 8.6-2.1 12.6l-28.7 86.1c-4.2 12.6 2.6 26.2 15.2 30.4s26.2-2.6 30.4-15.2l24.4-73.2c1.3-3.8 4.8-6.4 8.8-6.4s7.6 2.6 8.8 6.4l24.4 73.2c4.2 12.6 17.8 19.4 30.4 15.2s19.4-17.8 15.2-30.4l-28.7-86.1c-1.4-4.1-2.1-8.3-2.1-12.6l0-50.1c18.4-3.5 36.3-8.9 53.6-16.3l11.9-5.1c12.2-5.2 17.8-19.3 12.6-31.5s-19.3-17.8-31.5-12.6L338.7 175c-26.1 11.2-54.2 17-82.7 17s-56.5-5.8-82.7-17l-11.9-5.1zM256 160a40 40 0 1 0 0-80 40 40 0 1 0 0 80z"/></svg>
</span>
    </button>

    <div id="desktop-a11y-overlay" class="fixed inset-0 z-500 hidden"></div>

    <div
      id="desktop-a11y-panel"
      role="dialog"
      aria-labelledby="desktop-a11y-panel-title"
      class="a11y-panel-enter fixed hidden z-500 p-6 top-1/2 left-1/2 -translate-x-1/2 -translate-y-1/2 w-80 rounded-lg shadow-xl bg-neutral-50 dark:bg-neutral-800 border border-neutral-200 dark:border-neutral-700"
      style="min-width: 20rem;">
      <div class="flex items-center justify-between mb-6">
        <h3
          id="desktop-a11y-panel-title"
          class="text-lg font-semibold text-neutral-900 dark:text-neutral-100">
          Accessibility settings
        </h3>
        <button
          id="desktop-a11y-close"
          class="text-neutral-500 hover:text-neutral-700 dark:text-neutral-400 dark:hover:text-neutral-200"
          aria-label="Close a11y panel">
          <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
          </svg>
        </button>
      </div>

      <div class="space-y-5">
        
        
        
        
          
        
        
          <div class="flex items-center justify-between">
            <label for="desktop-disable-blur" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Disable blur
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="desktop-disable-blur">
            </div>
          </div>
          <div class="flex items-center justify-between">
            <label for="desktop-underline-links" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Show link underline
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="desktop-underline-links">
            </div>
          </div>
          <div class="flex items-center justify-between">
            <label for="desktop-zen-mode" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Enable zen mode
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="desktop-zen-mode">
            </div>
          </div>


        <div class="flex items-center justify-between">
          <label
            for="desktop-font-size-select"
            class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
            Font size
          </label>
          <select
            id="desktop-font-size-select"
            class="border rounded-lg px-3 py-1.5 pr-8 text-neutral-900 text-sm dark:bg-neutral-700 dark:text-neutral-200 focus:ring-primary-500 focus:border-primary-500">
            
            
              <option value="default">default</option>
            
              <option value="12px">12px</option>
            
              <option value="14px">14px</option>
            
              <option value="16px">16px</option>
            
              <option value="18px">18px</option>
            
              <option value="20px">20px</option>
            
              <option value="22px">22px</option>
            
              <option value="24px">24px</option>
            
          </select>
        </div>
      </div>
    </div>
  </div>

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    
      <div class="flex items-center">
    <button
      id="mobile-a11y-toggle"
      aria-label="Open accessibility panel"
      aria-expanded="false"
      type="button"
      class="text-base hover:text-primary-600 dark:hover:text-primary-400"
      role="button"
      aria-pressed="false">
      <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M0 256a256 256 0 1 1 512 0A256 256 0 1 1 0 256zm161.5-86.1c-12.2-5.2-26.3 .4-31.5 12.6s.4 26.3 12.6 31.5l11.9 5.1c17.3 7.4 35.2 12.9 53.6 16.3l0 50.1c0 4.3-.7 8.6-2.1 12.6l-28.7 86.1c-4.2 12.6 2.6 26.2 15.2 30.4s26.2-2.6 30.4-15.2l24.4-73.2c1.3-3.8 4.8-6.4 8.8-6.4s7.6 2.6 8.8 6.4l24.4 73.2c4.2 12.6 17.8 19.4 30.4 15.2s19.4-17.8 15.2-30.4l-28.7-86.1c-1.4-4.1-2.1-8.3-2.1-12.6l0-50.1c18.4-3.5 36.3-8.9 53.6-16.3l11.9-5.1c12.2-5.2 17.8-19.3 12.6-31.5s-19.3-17.8-31.5-12.6L338.7 175c-26.1 11.2-54.2 17-82.7 17s-56.5-5.8-82.7-17l-11.9-5.1zM256 160a40 40 0 1 0 0-80 40 40 0 1 0 0 80z"/></svg>
</span>
    </button>

    <div id="mobile-a11y-overlay" class="fixed inset-0 z-500 hidden"></div>

    <div
      id="mobile-a11y-panel"
      role="dialog"
      aria-labelledby="mobile-a11y-panel-title"
      class="a11y-panel-enter fixed hidden z-500 p-6 top-1/2 left-1/2 -translate-x-1/2 -translate-y-1/2 w-80 rounded-lg shadow-xl bg-neutral-50 dark:bg-neutral-800 border border-neutral-200 dark:border-neutral-700"
      style="min-width: 20rem;">
      <div class="flex items-center justify-between mb-6">
        <h3
          id="mobile-a11y-panel-title"
          class="text-lg font-semibold text-neutral-900 dark:text-neutral-100">
          Accessibility settings
        </h3>
        <button
          id="mobile-a11y-close"
          class="text-neutral-500 hover:text-neutral-700 dark:text-neutral-400 dark:hover:text-neutral-200"
          aria-label="Close a11y panel">
          <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
          </svg>
        </button>
      </div>

      <div class="space-y-5">
        
        
        
        
          
        
        
          <div class="flex items-center justify-between">
            <label for="mobile-disable-blur" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Disable blur
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="mobile-disable-blur">
            </div>
          </div>
          <div class="flex items-center justify-between">
            <label for="mobile-underline-links" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Show link underline
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="mobile-underline-links">
            </div>
          </div>
          <div class="flex items-center justify-between">
            <label for="mobile-zen-mode" class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
              Enable zen mode
            </label>
            <div class="ios-toggle">
              <input type="checkbox" id="mobile-zen-mode">
            </div>
          </div>


        <div class="flex items-center justify-between">
          <label
            for="mobile-font-size-select"
            class="text-sm font-medium text-neutral-700 dark:text-neutral-300">
            Font size
          </label>
          <select
            id="mobile-font-size-select"
            class="border rounded-lg px-3 py-1.5 pr-8 text-neutral-900 text-sm dark:bg-neutral-700 dark:text-neutral-200 focus:ring-primary-500 focus:border-primary-500">
            
            
              <option value="default">default</option>
            
              <option value="12px">12px</option>
            
              <option value="14px">14px</option>
            
              <option value="16px">16px</option>
            
              <option value="18px">18px</option>
            
              <option value="20px">20px</option>
            
              <option value="22px">22px</option>
            
              <option value="24px">24px</option>
            
          </select>
        </div>
      </div>
    </div>
  </div>

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/posts/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="Blog"
    title="Blog">
    
    
      <p class="text-bg font-bg">
        Blog
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/topics/"
    aria-label="Topics"
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-bg font-bg" title="Topics">
      Topics
    </p>
    <span>
      <span class="relative block icon"><svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>
</span>
    </span>
  </a>
</li>

  <li class="mt-1">
    <a
      href="/topics/transformers/"
      
      aria-label="Transformer"
      class="flex items-center hover:text-primary-600 dark:hover:text-primary-400">
      
      <p class="text-sm font-small" title="Transformer">
        Transformer
      </p>
    </a>
  </li>

<li class="mb-2"></li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>




  <script>
    (function () {
      var $mainmenu = $(".main-menu");
      var path = window.location.pathname;
      $mainmenu.find('a[href="' + path + '"]').each(function (i, e) {
        $(e).children("p").addClass("active");
      });
    })();
  </script>


  </div>
</div>

    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
        <ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden">
  
  
    
  
    
  
  <li class="hidden">
    <a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href="/"
      >Myxiniformes Moment</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class="inline">
    <a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href="/posts/"
      >Blog</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class="hidden">
    <a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href="/posts/transformers-pt-1/"
      >ELI5 Transformers (Part 1): Attention Mechanism </a
    ><span class="px-1 text-primary-500">/</span>
  </li>

</ol>


      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        ELI5 Transformers (Part 1): Attention Mechanism 
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  
    
  

  

  

  

  
    
  

  

  

  

  

  

  
    
  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <span>2617 words</span><span class="px-2 text-primary-500">&middot;</span><span class="mb-[2px]">
  <span
    id="zen-mode-button"
    class="text-lg hover:text-primary-500"
    title="Enable zen mode"
    data-title-i18n-disable="Enable zen mode"
    data-title-i18n-enable="Disable zen mode">
    <span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="50px" height="50px">
    <path fill="currentColor" d="M 12.980469 4 C 9.1204688 4 5.9804688 7.14 5.9804688 11 L 6 26 L 9.9804688 26 L 9.9804688 11 C 9.9804688 9.35 11.320469 8 12.980469 8 L 40.019531 8 C 41.679531 8 43.019531 9.35 43.019531 11 L 43.019531 39 C 43.019531 40.65 41.679531 42 40.019531 42 L 29 42 C 29 43.54 28.420938 44.94 27.460938 46 L 40.019531 46 C 43.879531 46 47.019531 42.86 47.019531 39 L 47.019531 11 C 47.019531 7.14 43.879531 4 40.019531 4 L 12.980469 4 z M 7 28 C 4.794 28 3 29.794 3 32 L 3 42 C 3 44.206 4.794 46 7 46 L 23 46 C 25.206 46 27 44.206 27 42 L 27 32 C 27 29.794 25.206 28 23 28 L 7 28 z M 7 32 L 23 32 L 23.001953 42 L 7 42 L 7 32 z"/>
</svg></span></span>
  </span>
</span>

    

    
    
  </div>

  
    <div class="flex flex-row flex-wrap items-center">
      
        
      
        
      
        
      
    </div>
  

  
  
    <div class="flex flex-row flex-wrap items-center">
      
        
      
        
          
            
              <a class="relative mt-[0.5rem] me-2" href="/tags/ai/">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    AI
  </span>
</span>

              </a>
            
              <a class="relative mt-[0.5rem] me-2" href="/tags/machine-learning/">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Machine Learning
  </span>
</span>

              </a>
            
              <a class="relative mt-[0.5rem] me-2" href="/tags/eli5/">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    ELI5
  </span>
</span>

              </a>
            
          
        
      
        
          
            
              <a class="relative mt-[0.5rem] me-2" href="/topics/transformers/">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Transformer
  </span>
</span>

              </a>
            
          
        
      
    </div>
  

  
  



      </div>
      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      
        <div class="order-first lg:ml-auto px-0 lg:order-last lg:ps-8 lg:max-w-2xs">
          <div class="toc ps-5 print:hidden lg:sticky lg:top-[140px]">
            
              <details
  open
  id="TOCView"
  class="toc-right mt-0 overflow-y-auto overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg -ms-5 ps-5 pe-2 hidden lg:block">
  <summary
    class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 -ms-5 ps-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">
    Table of Contents
  </summary>
  <div
    class="min-w-[220px] py-2 border-dotted border-s-1 -ms-5 ps-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#model-only-needs-to-be-useful">Model only needs to be useful</a></li>
    <li><a href="#transformer-model-architecture">Transformer Model Architecture</a>
      <ul>
        <li><a href="#overview">Overview</a></li>
        <li><a href="#the-transformer-itself">The Transformer Itself</a></li>
        <li><a href="#the-attention-head">The attention head</a></li>
      </ul>
    </li>
    <li><a href="#example-qwen3">Example: Qwen3</a>
      <ul>
        <li><a href="#prep">Prep</a></li>
        <li><a href="#get-the-model">Get the model</a></li>
        <li><a href="#model-architecture">Model Architecture</a></li>
        <li><a href="#have-some-fun">Have some fun!</a></li>
      </ul>
    </li>
    <li><a href="#final-thoughts">Final thoughts</a></li>
    <li><a href="#some-other-resources">Some Other Resources</a></li>
  </ul>
</nav>
  </div>
</details>
<details class="toc-inside mt-0 overflow-hidden rounded-lg -ms-5 ps-5 lg:hidden">
  <summary
    class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 -ms-5 ps-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">
    Table of Contents
  </summary>
  <div
    class="py-2 border-dotted border-neutral-300 border-s-1 -ms-5 ps-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#model-only-needs-to-be-useful">Model only needs to be useful</a></li>
    <li><a href="#transformer-model-architecture">Transformer Model Architecture</a>
      <ul>
        <li><a href="#overview">Overview</a></li>
        <li><a href="#the-transformer-itself">The Transformer Itself</a></li>
        <li><a href="#the-attention-head">The attention head</a></li>
      </ul>
    </li>
    <li><a href="#example-qwen3">Example: Qwen3</a>
      <ul>
        <li><a href="#prep">Prep</a></li>
        <li><a href="#get-the-model">Get the model</a></li>
        <li><a href="#model-architecture">Model Architecture</a></li>
        <li><a href="#have-some-fun">Have some fun!</a></li>
      </ul>
    </li>
    <li><a href="#final-thoughts">Final thoughts</a></li>
    <li><a href="#some-other-resources">Some Other Resources</a></li>
  </ul>
</nav>
  </div>
</details>


<script>
  (function () {
    'use strict'

    const SCROLL_OFFSET_RATIO = 0.33
    const TOC_SELECTOR = '#TableOfContents'
    const ANCHOR_SELECTOR = '.anchor'
    const TOC_LINK_SELECTOR = 'a[href^="#"]'
    const NESTED_LIST_SELECTOR = 'li ul'
    const ACTIVE_CLASS = 'active'
    let isJumpingToAnchor = false

    function getActiveAnchorId(anchors, offsetRatio) {
      const threshold = window.scrollY + window.innerHeight * offsetRatio
      const tocLinks = [...document.querySelectorAll('#TableOfContents a[href^="#"]')]
      const tocIds = new Set(tocLinks.map(link => link.getAttribute('href').substring(1)))

      if (isJumpingToAnchor) {
        for (let i = 0; i < anchors.length; i++) {
          const anchor = anchors[i]
          if (!tocIds.has(anchor.id)) continue
          const top = anchor.getBoundingClientRect().top + window.scrollY
          if (Math.abs(window.scrollY - top) < 100) {
            return anchor.id
          }
        }
      }

      for (let i = anchors.length - 1; i >= 0; i--) {
        const top = anchors[i].getBoundingClientRect().top + window.scrollY
        if (top <= threshold && tocIds.has(anchors[i].id)) {
          return anchors[i].id
        }
      }
      return anchors.find(anchor => tocIds.has(anchor.id))?.id || ''
    }

    function updateTOC({ toc, anchors, links, scrollOffset, collapseInactive }) {
      const activeId = getActiveAnchorId(anchors, scrollOffset)
      if (!activeId) return

      links.forEach(link => {
        const isActive = link.getAttribute('href') === `#${activeId}`
        link.classList.toggle(ACTIVE_CLASS, isActive)

        if (collapseInactive) {
          const ul = link.closest('li')?.querySelector('ul')
          if (ul) ul.style.display = isActive ? '' : 'none'
        }
      })

      if (collapseInactive) {
        const activeLink = toc.querySelector(`a[href="#${CSS.escape(activeId)}"]`)
        let el = activeLink
        while (el && el !== toc) {
          if (el.tagName === 'UL') el.style.display = ''
          if (el.tagName === 'LI') el.querySelector('ul')?.style.setProperty('display', '')
          el = el.parentElement
        }
      }
    }

    function initTOC() {
      const toc = document.querySelector(TOC_SELECTOR)
      if (!toc) return

      const collapseInactive = false
      const anchors = [...document.querySelectorAll(ANCHOR_SELECTOR)]
      const links = [...toc.querySelectorAll(TOC_LINK_SELECTOR)]

      if (collapseInactive) {
        toc.querySelectorAll(NESTED_LIST_SELECTOR).forEach(ul => ul.style.display = 'none')
      }

      links.forEach(link => {
        link.addEventListener('click', () => {
          isJumpingToAnchor = true
        })
      })

      const config = {
        toc,
        anchors,
        links,
        scrollOffset: SCROLL_OFFSET_RATIO,
        collapseInactive
      }

      window.addEventListener('scroll', () => updateTOC(config), { passive: true })
      window.addEventListener('hashchange', () => updateTOC(config), { passive: true })

      updateTOC(config)
    }

    document.readyState === 'loading'
      ? document.addEventListener('DOMContentLoaded', initTOC)
      : initTOC()
  })()
</script>


            
          </div>
        </div>
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          

<p>In this particular post, I would like to do a very brief overview of the transformer model architecture, specifically on the attention mechanism. I won&rsquo;t go metion too much math and there won&rsquo;t be any mathenathical formulas. However, I would assume readers of this silly little post already have some okay-ish background of math/ datascience. (i.e., matrix computations embeddings, tokens, model fitting etc.). I am not going to list out all the implemention details for transformers, since there are a lot of very good materials out there and they are doing fantastic jobs. Instead, in this (maybe series of?) post, İ would like to draw out a general framework on transformers to help one understand the detailed math behind.<br></p>
<p>Today, I&rsquo;ll very quickly go through some very basics on neural network model, just enough to cover what needed for this post, accompied by demo of transformer model as a proof of concept. In this section, there will be some codes that you can copy and paste into an interactive python session to fiddle around for a bit. And lastly, I&rsquo;ll do a quick sketch on the general architecture of transformers, and a overview of the attention mechasm.<br></p>

<h2 class="relative group">Model only needs to be useful
    <div id="model-only-needs-to-be-useful" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#model-only-needs-to-be-useful" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In order to make things easier to understand, I would wish to start with an inaccuate premise: we can view neural network models as functions that takes some sort of matrix as inputs, do some sort of matrix computations, and output another matrix as the final result. What makes one neural network different from others is how the computation is carried out. It&rsquo;s like \(y=a \times x^2\) is a different function from \(y=a \times sin(x)\), only that in the case of neural network, both x and y are matrices, and the math is much complicated. When it comes to model training, we are essentially trying to find values gives best fit to the data.<br></p>
<p>There&rsquo;s an important assumption here: just because model fits the data well does not mean the model describes mechanisms behind the data. For example, we <i>definitely</i> can fit \(y=a \times sin(x) + b\) to a normal distribution data (like distribution of customer spendings in McDonald&rsquo;s), and it&rsquo;s prob going to be a pretty good fit, but this does not mean the sine function has anything to do with explaining the normal distribution. A good model does not always need to be description, a good model just needs to be useful for its purpose.<br></p>
<p>Transformers are preciesly these kinds of models: they are, surprisingly good at fitting into all sorts of data whilst the math behind the model probably doesn&rsquo;t have much to do with the mechanisms behind. We don&rsquo;t know how transformers works so well for text-based tasks. At least not yet. Originally, transformer was designed as an add-on to the text-processing neural network models in order to tackle with some tricky problems (these problems are not the main forcus of the current blogpost so I&rsquo;m skipping them, but <a
  href="https://towardsdatascience.com/beautifully-illustrated-nlp-models-from-rnn-to-transformer-80d69faf2109/"
    target="_blank"
  >here&rsquo;s a good article if you were interested</a>). We just happened to discover that transformers alone is good enough to solve these problems, we just need to make the transformers much bigger. So that&rsquo;s where the Aİ bloom started: GPT2 solved issues in GPT1 by simply being 10 times bigger; the most-recently open-sourced <a
  href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1"
    target="_blank"
  >pretrained GPT-Oss</a>, is 200 times bigger than <a
  href="https://huggingface.co/openai-community/gpt2"
    target="_blank"
  >the previous openpsourced model, GPT2</a> <i>(note: GPT-OSS is structurlly different from the original GPT2 but the fundamental ideas are the same.)</i>. There are even speculations suggesting transformer neural network models can be seen as some sort of universal function approximator. That is, it&rsquo;s capacable of &lsquo;approximate&rsquo; other formulas/ functions with certain degree of accuracy, providing the model itself is big enough (<a
  href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"
    target="_blank"
  >&lsquo;universal approximation theorem&rsquo;</a>). <br></p>

<h2 class="relative group">Transformer Model Architecture
    <div id="transformer-model-architecture" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#transformer-model-architecture" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h3 class="relative group">Overview
    <div id="overview" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#overview" aria-label="Anchor">#</a>
    </span>
    
</h3>
<p>At conceptual level, the general idea behind transformer models are is actually pretty intuitive. We can roughly divide the model calclations into three stages:
<ol class="border-s-2 border-primary-500 dark:border-primary-300 list-none">






<li>
  <div class="flex">
    <div
      class="bg-primary-500 dark:bg-primary-300 text-neutral-50 dark:text-neutral-700 min-w-[30px] h-8 text-2xl flex items-center justify-center rounded-full ltr:-ml-12 rtl:-mr-[79px] mt-5">
      <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512">
<path fill="currentColor"  d="M392.8 1.2c-17-4.9-34.7 5-39.6 22l-128 448c-4.9 17 5 34.7 22 39.6s34.7-5 39.6-22l128-448c4.9-17-5-34.7-22-39.6zm80.6 120.1c-12.5 12.5-12.5 32.8 0 45.3L562.7 256l-89.4 89.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l112-112c12.5-12.5 12.5-32.8 0-45.3l-112-112c-12.5-12.5-32.8-12.5-45.3 0zm-306.7 0c-12.5-12.5-32.8-12.5-45.3 0l-112 112c-12.5 12.5-12.5 32.8 0 45.3l112 112c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L77.3 256l89.4-89.4c12.5-12.5 12.5-32.8 0-45.3z"/></svg></span>
    </div>
    <div class="block p-6 rounded-lg shadow-2xl flex-1 ms-6 mb-10 break-words">
      <div class="flex justify-between">
        
          <h2 class="mt-0">Stage 1</h2>
        
        
          <h3 class="">
            <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Embedding
  </span>
</span>

          </h3>
        
      </div>
      
        <h4 class="mt-0">
          The input gets converted into vectors or matrices.
        </h4>
      
      <div class="mb-6">
        
  The first step starts with creating a mathematical representation of our input data. This process can vary based on different types of inputs. It can simply be some sort of look-up tables (text embedding), some matrix transformations of the raw inputs (convolution) etc.

      </div>
    </div>
  </div>
</li>






<li>
  <div class="flex">
    <div
      class="bg-primary-500 dark:bg-primary-300 text-neutral-50 dark:text-neutral-700 min-w-[30px] h-8 text-2xl flex items-center justify-center rounded-full ltr:-ml-12 rtl:-mr-[79px] mt-5">
      <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512">
<path fill="currentColor"  d="M392.8 1.2c-17-4.9-34.7 5-39.6 22l-128 448c-4.9 17 5 34.7 22 39.6s34.7-5 39.6-22l128-448c4.9-17-5-34.7-22-39.6zm80.6 120.1c-12.5 12.5-12.5 32.8 0 45.3L562.7 256l-89.4 89.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l112-112c12.5-12.5 12.5-32.8 0-45.3l-112-112c-12.5-12.5-32.8-12.5-45.3 0zm-306.7 0c-12.5-12.5-32.8-12.5-45.3 0l-112 112c-12.5 12.5-12.5 32.8 0 45.3l112 112c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L77.3 256l89.4-89.4c12.5-12.5 12.5-32.8 0-45.3z"/></svg></span>
    </div>
    <div class="block p-6 rounded-lg shadow-2xl flex-1 ms-6 mb-10 break-words">
      <div class="flex justify-between">
        
          <h2 class="mt-0">Stage 2</h2>
        
        
          <h3 class="">
            <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Transformer
  </span>
</span>

          </h3>
        
      </div>
      
        <h4 class="mt-0">
          The raw output from Stage 1 feeds into the multiple different attention layers.
        </h4>
      
      <div class="mb-6">
        
  Mathematically, each attention layer is doing very much the same mathematical operation, with each layer having its own sets of parameters. Each layer takes a matrix as an input, and outputs another matrix to pass onto the next layer. This process is repeated multiple times. Stage 2 is the core of a transformer model, it *transforms* our inputs into something else.

      </div>
    </div>
  </div>
</li>






<li>
  <div class="flex">
    <div
      class="bg-primary-500 dark:bg-primary-300 text-neutral-50 dark:text-neutral-700 min-w-[30px] h-8 text-2xl flex items-center justify-center rounded-full ltr:-ml-12 rtl:-mr-[79px] mt-5">
      <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512">
<path fill="currentColor"  d="M392.8 1.2c-17-4.9-34.7 5-39.6 22l-128 448c-4.9 17 5 34.7 22 39.6s34.7-5 39.6-22l128-448c4.9-17-5-34.7-22-39.6zm80.6 120.1c-12.5 12.5-12.5 32.8 0 45.3L562.7 256l-89.4 89.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l112-112c12.5-12.5 12.5-32.8 0-45.3l-112-112c-12.5-12.5-32.8-12.5-45.3 0zm-306.7 0c-12.5-12.5-32.8-12.5-45.3 0l-112 112c-12.5 12.5-12.5 32.8 0 45.3l112 112c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L77.3 256l89.4-89.4c12.5-12.5 12.5-32.8 0-45.3z"/></svg></span>
    </div>
    <div class="block p-6 rounded-lg shadow-2xl flex-1 ms-6 mb-10 break-words">
      <div class="flex justify-between">
        
          <h2 class="mt-0">Stage 3</h2>
        
        
          <h3 class="">
            <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Output
  </span>
</span>

          </h3>
        
      </div>
      
        <h4 class="mt-0">
          We convert the matrix output from Stage 2 into task-specific results.
        </h4>
      
      <div class="mb-6">
        
  This is usually done by another set of simple matrix operations, depending on the task. For example, if we are doing text sentimental analysis task, this operation could be a simple matrix multiplication, resulting in a final score of 0-10.

      </div>
    </div>
  </div>
</li>

</ol>
</p>
<p>In other words, you can concepturally see <strong>Stage 1</strong> as a conversion stage in order to initiate the model, <strong>Stage 2</strong> being the core of a transformer model, and <strong>Stage 3</strong> as a &lsquo;decoding&rsquo; step to convert the output back into human-readable form. When we talk about transformer models, we are mostly referring to  <strong>Stage 2</strong>, which is the focus of the current post. I&rsquo;ll elabrate a lot more on what&rsquo;s hapenning in <strong>Stage 3</strong> in the next post.<br></p>

<h3 class="relative group">The Transformer Itself
    <div id="the-transformer-itself" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#the-transformer-itself" aria-label="Anchor">#</a>
    </span>
    
</h3>
<p>The transformer itself is pretty straightforward: it consists of stacks of multiple attention layers that often share exactly the same, or very similar structure:
<pre class="not-prose mermaid">
---
config:
  layout: dagre
---
flowchart LR

    n1["Input"] --> n2["Attention<br>Layer"]
    n2 --> n3["Attention<br>Layer"]
    n3 --> n6["Repeat"]
    n6 --> n5["Output"]

    n2@{ shape: rounded}
    n3@{ shape: rounded}
    n6@{ shape: text}
    classDef default fill: transparent, bg-color: transparent

</pre>
</p>
<p>Asttention layer consists of multiple &lsquo;attention heads&rsquo; that works in parallel: each attention head processes the inputs independently, and the output of all attention heads are merged back together. The joined matrix is the final output of the current layer: <br></p>
<pre class="not-prose mermaid">
flowchart LR
 subgraph s1["Attention Heads"]

    direction LR
        n14["Attention Head"]
        n16["Attention Head"]
        n17["Attention Head"]

  end

    s1 --> n10["Merge"]
    n10 --> n18["Next Layer"]
    n19["Previous Layer"] --> s1

    n18@{ shape: text}
    n19@{ shape: text}
    classDef default fill: transparent
    style s1 fill:transparent

</pre>


<h3 class="relative group">The attention head
    <div id="the-attention-head" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#the-attention-head" aria-label="Anchor">#</a>
    </span>
    
</h3>
<p>You can think each of the attention head as a mini neural network. A typical attention head works like this:</p>
<p><pre class="not-prose mermaid">
flowchart TB

    IN["Previous Layer"] --> Q["Matrix 1"] & K["Matrix 2"] & V["Matrix 3"]
    Q ---> SDPA["Matrix 1 & 2"]
    K ---> SDPA
    SDPA --> n1["Output"]
    V ---> n1
    classDef default fill: transparent, bg-color: transparent

</pre>

<br></p>
<ol>
<li><bullet> Step 1: The input matrix gets converted into multiple matices through matrix multiplication. Most current transformers converts input matrix into three smaller matrices.</bullet></li>
<li><bullet>Step 2: Two of the matrix from step (1) gets combined together using some matrix operation, usually dot products.</bullet></li>
<li><bullet>Step 3: The third matrix from step (1) combines with output from step (2), using some other matrix operation.</bullet></li>
</ol>

<h2 class="relative group">Example: Qwen3
    <div id="example-qwen3" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#example-qwen3" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We&rsquo;ll now take a look at an actual transformer model and see how it works in action.
It&rsquo;s very suprising is that, transformers are able to produce pretty impressive results for tasks model that are not specifically trained for. Here, we&rsquo;ll use <a
  href="https://qwen.ai/blog?id=qwen3"
    target="_blank"
  >Qwen3, a small-sized text generation model</a> as a demo. It&rsquo;s tiny (~1.5GB) but the performance is VERY impressive for its size.<br>
Here&rsquo;s huggingface&rsquo;s link to the model: <br>
<div class="huggingface-card-wrapper">
    <a id="huggingface-61480284aec1dd272883911c77809285" target="_blank" href="https://huggingface.co/Qwen/Qwen3-0.6B" class="cursor-pointer">
      <div
        class="w-full md:w-auto p-0 m-0 border border-neutral-200 dark:border-neutral-700 border rounded-md shadow-2xl">
        <div class="w-full md:w-auto pt-3 p-5">
          <div class="flex items-center">
            <span class="text-2xl text-neutral-800 dark:text-neutral me-2">
              <span class="relative inline-block align-text-bottom icon">
                
  
  <svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 475 439"><path d="M235.793 396.126a187.281 187.281 0 00187.285-187.284A187.283 187.283 0 00235.793 21.558 187.287 187.287 0 0048.509 208.842a187.286 187.286 0 00187.284 187.284z" fill="#FFD21E"/><path d="M423.078 208.842A187.283 187.283 0 00235.793 21.558 187.283 187.283 0 0048.509 208.842a187.283 187.283 0 00319.714 132.43 187.284 187.284 0 0054.855-132.43zm-396.127 0a208.842 208.842 0 11417.685 0 208.842 208.842 0 01-417.685 0z" fill="#FF9D0B"/><path d="M296.641 157.912c6.898 2.371 9.593 16.491 16.545 12.827a26.946 26.946 0 008.24-40.841 26.952 26.952 0 00-28.632-8.767 26.942 26.942 0 00-19.055 23.099 26.943 26.943 0 003.014 15.352c3.288 6.198 13.744-3.88 19.941-1.724l-.053.054zm-126.923 0c-6.898 2.371-9.647 16.491-16.545 12.827a26.946 26.946 0 01-8.24-40.841 26.952 26.952 0 0128.632-8.767 26.944 26.944 0 0116.041 38.451c-3.288 6.198-13.797-3.88-19.941-1.724l.053.054z" fill="#3A3B45"/><path d="M234.446 287.205c52.979 0 70.063-47.212 70.063-71.464 0-12.612-8.461-8.624-22.043-1.941-12.557 6.198-29.426 14.768-47.966 14.768-38.75 0-70.063-37.08-70.063-12.827 0 24.252 17.031 71.464 70.063 71.464h-.054z" fill="#FF323D"/><path fill-rule="evenodd" clip-rule="evenodd" d="M193.863 274.863a46.895 46.895 0 0128.565-24.199c2.155-.646 4.365 3.072 6.682 6.899 2.156 3.665 4.42 7.384 6.683 7.384 2.426 0 4.851-3.665 7.168-7.276 2.426-3.773 4.797-7.438 7.115-6.737a46.403 46.403 0 0126.947 22.474c20.103-15.845 27.486-41.715 27.486-57.667 0-12.612-8.461-8.624-22.043-1.941l-.754.378c-12.45 6.198-29.05 14.39-47.266 14.39-18.216 0-34.762-8.192-47.266-14.39-14.012-6.953-22.797-11.318-22.797 1.563 0 16.438 7.869 43.439 29.48 59.122z" fill="#3A3B45"/><path d="M362.446 183.242a17.515 17.515 0 10.002-35.03 17.515 17.515 0 00-.002 35.03zm-250.61 0a17.515 17.515 0 100-35.03 17.515 17.515 0 000 35.03zM75.78 242.526c-8.731 0-16.492 3.557-21.935 10.079a32.173 32.173 0 00-7.168 20.264 38.275 38.275 0 00-10.456-1.617c-8.353 0-15.899 3.18-21.234 8.947a31.257 31.257 0 00-4.312 37.726 28.566 28.566 0 00-9.647 15.198 31.512 31.512 0 004.312 25.546 28.136 28.136 0 00-1.995 27.056c5.498 12.503 19.24 22.312 45.919 32.875 16.545 6.576 31.744 10.779 31.851 10.833a238.92 238.92 0 0058.907 8.623c31.583 0 54.165-9.701 67.153-28.779 20.911-30.666 17.947-58.746-9.162-85.801-14.929-14.983-24.899-37.025-26.947-41.876-4.204-14.336-15.306-30.289-33.684-30.289a30.716 30.716 0 00-24.792 13.258c-5.389-6.79-10.671-12.126-15.414-15.198a39.885 39.885 0 00-21.396-6.845zm0 21.558c2.749 0 6.144 1.186 9.809 3.503 11.533 7.33 33.684 45.434 41.822 60.255 2.695 4.958 7.384 7.06 11.534 7.06 8.353 0 14.821-8.246.808-18.755-21.127-15.792-13.743-41.607-3.665-43.17.431-.108.916-.108 1.294-.108 9.162 0 13.204 15.791 13.204 15.791s11.857 29.75 32.229 50.122c20.318 20.319 21.396 36.649 6.575 58.368-10.132 14.821-29.48 19.295-49.368 19.295-20.533 0-41.66-4.851-53.463-7.869-.593-.162-72.489-20.48-63.38-37.726 1.509-2.911 4.042-4.096 7.222-4.096 12.826 0 36.11 19.078 46.187 19.078 2.21 0 3.773-.916 4.474-3.233 4.257-15.36-64.997-21.828-59.177-44.032 1.078-3.935 3.827-5.498 7.761-5.498 16.923 0 54.973 29.804 62.95 29.804.592 0 1.077-.161 1.293-.539 3.988-6.467 1.778-10.994-26.409-28.025-28.079-17.031-47.858-27.271-36.648-39.505 1.293-1.401 3.126-2.048 5.39-2.048 17.084 0 57.451 36.756 57.451 36.756s10.887 11.318 17.516 11.318c1.509 0 2.802-.539 3.665-2.048 4.635-7.868-43.44-44.301-46.134-59.338-1.833-10.24 1.293-15.36 7.06-15.36z" fill="#FF9D0B"/><path d="M189.39 397.15c14.821-21.773 13.743-38.103-6.575-58.422-20.372-20.318-32.229-50.122-32.229-50.122s-4.419-17.246-14.498-15.629c-10.078 1.617-17.462 27.378 3.665 43.169 21.073 15.792-4.204 26.517-12.342 11.696-8.084-14.821-30.289-52.925-41.822-60.255-11.48-7.276-19.564-3.233-16.87 11.857 2.696 15.037 50.824 51.47 46.135 59.284-4.689 7.923-21.181-9.216-21.181-9.216s-51.577-46.942-62.841-34.708c-11.21 12.234 8.569 22.474 36.648 39.505 28.187 17.031 30.397 21.558 26.409 28.025-4.042 6.468-66.183-45.972-72.004-23.713-5.82 22.15 63.434 28.564 59.177 43.924-4.312 15.36-48.829-28.996-57.883-11.749-9.162 17.3 62.787 37.618 63.38 37.78 23.175 6.036 82.189 18.809 102.831-11.426z" fill="#FFD21E"/><path d="M398.502 242.526c8.731 0 16.545 3.557 21.935 10.079a32.173 32.173 0 017.168 20.264 38.272 38.272 0 0110.509-1.617c8.354 0 15.899 3.18 21.235 8.947a31.257 31.257 0 014.311 37.726 28.564 28.564 0 019.594 15.198 31.513 31.513 0 01-4.312 25.546 28.142 28.142 0 011.994 27.056c-5.497 12.503-19.24 22.312-45.864 32.875-16.6 6.576-31.798 10.779-31.906 10.833a238.914 238.914 0 01-58.907 8.623c-31.582 0-54.164-9.701-67.153-28.779-20.911-30.667-17.947-58.746 9.162-85.801 14.983-14.983 24.954-37.026 27.002-41.876 4.203-14.336 15.252-30.289 33.63-30.289a30.716 30.716 0 0124.792 13.258c5.389-6.791 10.671-12.126 15.467-15.198a39.888 39.888 0 0121.343-6.845zm0 21.558c-2.749 0-6.09 1.186-9.809 3.503-11.48 7.33-33.684 45.434-41.822 60.255a13.106 13.106 0 01-11.534 7.06c-8.3 0-14.821-8.246-.754-18.756 21.072-15.791 13.689-41.606 3.61-43.169a8.233 8.233 0 00-1.293-.108c-9.162 0-13.204 15.791-13.204 15.791s-11.857 29.75-32.175 50.122c-20.373 20.319-21.45 36.649-6.576 58.368 10.079 14.821 29.481 19.295 49.314 19.295 20.588 0 41.661-4.851 53.518-7.869.539-.162 72.488-20.48 63.38-37.726-1.563-2.911-4.042-4.096-7.222-4.096-12.827 0-36.163 19.078-46.188 19.078-2.263 0-3.826-.916-4.473-3.233-4.312-15.36 64.943-21.828 59.122-44.032-1.024-3.935-3.772-5.498-7.76-5.498-16.923 0-54.973 29.804-62.949 29.804-.539 0-1.024-.161-1.24-.539-3.988-6.467-1.832-10.994 26.301-28.025 28.187-17.031 47.966-27.271 36.648-39.505-1.24-1.401-3.072-2.048-5.282-2.048-17.138 0-57.505 36.756-57.505 36.756s-10.887 11.318-17.462 11.318a3.99 3.99 0 01-3.665-2.048c-4.689-7.868 43.385-44.301 46.08-59.338 1.832-10.24-1.294-15.36-7.06-15.36z" fill="#FF9D0B"/><path d="M284.945 397.15c-14.821-21.773-13.797-38.103 6.576-58.422 20.318-20.318 32.175-50.122 32.175-50.122s4.419-17.246 14.551-15.629c10.025 1.617 17.408 27.378-3.664 43.169-21.127 15.792 4.203 26.517 12.287 11.696 8.139-14.821 30.343-52.925 41.823-60.255 11.479-7.276 19.617-3.233 16.869 11.857-2.695 15.037-50.769 51.47-46.08 59.284 4.635 7.923 21.127-9.216 21.127-9.216s51.631-46.942 62.841-34.708c11.21 12.234-8.515 22.474-36.649 39.505-28.186 17.031-30.342 21.558-26.408 28.025 4.042 6.468 66.183-45.972 72.003-23.713 5.821 22.15-63.38 28.564-59.122 43.924 4.311 15.36 48.775-28.996 57.883-11.749 9.108 17.3-62.788 37.618-63.38 37.78-23.229 6.036-82.244 18.809-102.832-11.426z" fill="#FFD21E"/></svg>
  

              </span>
            </span>
            <div
              id="huggingface-61480284aec1dd272883911c77809285-id"
              class="m-0 font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral">
              Qwen/Qwen3-0.6B
            </div>
          </div><div class="m-0 mt-2 flex items-center">
            <span class="mr-1 inline-block h-3 w-3 rounded-full language-dot" data-language="model"></span>
            <div class="m-0 mr-5 text-md text-neutral-800 dark:text-neutral">
              
                text-generation
              
            </div>

            <span class="text-md mr-1 text-neutral-800 dark:text-neutral">
              <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M244 84L255.1 96L267.1 84.02C300.6 51.37 347 36.51 392.6 44.1C461.5 55.58 512 115.2 512 185.1V190.9C512 232.4 494.8 272.1 464.4 300.4L283.7 469.1C276.2 476.1 266.3 480 256 480C245.7 480 235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1 0 232.4 0 190.9V185.1C0 115.2 50.52 55.58 119.4 44.1C164.1 36.51 211.4 51.37 244 84C243.1 84 244 84.01 244 84L244 84zM255.1 163.9L210.1 117.1C188.4 96.28 157.6 86.4 127.3 91.44C81.55 99.07 48 138.7 48 185.1V190.9C48 219.1 59.71 246.1 80.34 265.3L256 429.3L431.7 265.3C452.3 246.1 464 219.1 464 190.9V185.1C464 138.7 430.4 99.07 384.7 91.44C354.4 86.4 323.6 96.28 301.9 117.1L255.1 163.9z"/></svg></span>
            </span>
            <div id="huggingface-61480284aec1dd272883911c77809285-likes" class="m-0 mr-5 text-md text-neutral-800 dark:text-neutral">
              802
            </div>

            <span class="text-md mr-1 text-neutral-800 dark:text-neutral">
              <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M288 32c0-17.7-14.3-32-32-32s-32 14.3-32 32V274.7l-73.4-73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l128 128c12.5 12.5 32.8 12.5 45.3 0l128-128c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L288 274.7V32zM64 352c-35.3 0-64 28.7-64 64v32c0 35.3 28.7 64 64 64H448c35.3 0 64-28.7 64-64V416c0-35.3-28.7-64-64-64H346.5l-45.3 45.3c-25 25-65.5 25-90.5 0L165.5 352H64zM432 456c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
            </span>
            <div id="huggingface-61480284aec1dd272883911c77809285-downloads" class="m-0 mr-5 text-md text-neutral-800 dark:text-neutral">
              7.059495e&#43;06
            </div>
          </div>
        </div>
      </div>
      
      
      <script
        async
        type="text/javascript"
        src="/js/fetch-repo.min.dc5533c50cefd50405344b235937142271f26229fe39cbee27fd4960e8bb897a0beebfad77a1091ca91cd0d1fb14e70fc37cc114dd9674fb2c32e0ab512ec8a4.js"
        integrity="sha512-3FUzxQzv1QQFNEsjWTcUInHyYin&#43;OcvuJ/1JYOi7iXoL7r&#43;td6EJHKkc0NH7FOcPw3zBFN2WdPssMuCrUS7IpA=="
        data-repo-url="https://huggingface.co/api/models/Qwen/Qwen3-0.6B"
        data-repo-id="huggingface-61480284aec1dd272883911c77809285"></script>
    </a>
  </div>
</p>

<h3 class="relative group">Prep
    <div id="prep" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#prep" aria-label="Anchor">#</a>
    </span>
    
</h3>
<p>Make sure you have python pre-installed. If you were using windows, python can be downloaded from the Microsoft Store. If you were Mac/ Linux Mint/ Ubuntu/ Debian etc., your computer should already come with python by default. For this demo, we would need python version <a style="font-size:75%">\( \geqslant \)</a> 3.12.<br></p>
<p>We first need to install some dependencies. Open terminal/ command prompt, type this command to install required dependencies:<br></p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>pip install torch transformers
</span></span></code></pre></div><p>And then type:<br></p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>python
</span></span></code></pre></div><p>To open python and start an interactive python session.<br><br>
<em>Alternatively, if you installed <a
  href="https://ipython.org/install.html"
    target="_blank"
  >ipython</a>, which should already be installed if you have installed <a
  href="https://jupyter.org/install"
    target="_blank"
  >jupyter notebook</a> previously, you can open ipython instead:</em></p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>ipython
</span></span></code></pre></div><p>Once python was opened, copy and paste these lines into python to import required packages:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#94e2d5">from</span> <span style="color:#fab387">pprint</span> <span style="color:#94e2d5">import</span> pprint
</span></span><span style="display:flex;"><span><span style="color:#94e2d5">import</span> <span style="color:#fab387">re</span>
</span></span><span style="display:flex;"><span><span style="color:#94e2d5">from</span> <span style="color:#fab387">transformers</span> <span style="color:#94e2d5">import</span> AutoModelForCausalLM, AutoTokenizer, BatchEncoding
</span></span></code></pre></div><div class="aSeparator"></div>

<h3 class="relative group">Get the model
    <div id="get-the-model" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#get-the-model" aria-label="Anchor">#</a>
    </span>
    
</h3>
<p>Models from <code>transformers</code> library takes matrices as inputs, and outputs matrices. Thus, text generation models needs to be paired with <code>tokenizer</code> in order to convert inputs into model-readable formant, and convert model outputs into human-readble format. <code>transformers</code> library has a special class called <code>TextGenerationPipeline</code> to handle this conversion class, but would be a bit too complicated for our purpose. Here, I modified demo code from <a href='https://huggingface.co/Qwen/Qwen3-0.6B'>Qwen3&rsquo;s demo code</a> that simply combines tokenizers and models together. Here&rsquo;s the code for you to copy and paste into the currently running python session:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#cba6f7">class</span> <span style="color:#f9e2af">DemoChatbot</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#a6e3a1">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a6e3a1">    A simple demo chatbot, code modified from https://huggingface.co/Qwen/Qwen3-0.6B
</span></span></span><span style="display:flex;"><span><span style="color:#a6e3a1">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#cba6f7">def</span> <span style="color:#89b4fa">__init__</span>(<span style="color:#89dceb">self</span>, model_name:<span style="color:#89dceb">str</span><span style="color:#89dceb;font-weight:bold">=</span><span style="color:#a6e3a1">&#34;Qwen/Qwen3-0.6B&#34;</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#89dceb">self</span><span style="color:#89dceb;font-weight:bold">.</span>tokenizer <span style="color:#89dceb;font-weight:bold">=</span> AutoTokenizer<span style="color:#89dceb;font-weight:bold">.</span>from_pretrained(model_name)
</span></span><span style="display:flex;"><span>        <span style="color:#89dceb">self</span><span style="color:#89dceb;font-weight:bold">.</span>enable_thinking <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#fab387">False</span>
</span></span><span style="display:flex;"><span>        <span style="color:#89dceb">self</span><span style="color:#89dceb;font-weight:bold">.</span>_thiniking_regex <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#f38ba8">r</span><span style="color:#a6e3a1">&#34;&lt;think&gt;(.*?)&lt;/think&gt;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#89dceb">self</span><span style="color:#89dceb;font-weight:bold">.</span>model <span style="color:#89dceb;font-weight:bold">=</span> AutoModelForCausalLM<span style="color:#89dceb;font-weight:bold">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>            model_name,
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>        <span style="color:#89dceb">self</span><span style="color:#89dceb;font-weight:bold">.</span>history <span style="color:#89dceb;font-weight:bold">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#cba6f7">def</span> <span style="color:#89b4fa">clear_history</span>(<span style="color:#89dceb">self</span>) <span style="color:#89dceb;font-weight:bold">-&gt;</span> <span style="color:#fab387">None</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#89dceb">self</span><span style="color:#89dceb;font-weight:bold">.</span>history <span style="color:#89dceb;font-weight:bold">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#cba6f7">def</span> <span style="color:#89b4fa">tokenise</span>(<span style="color:#89dceb">self</span>, user_input:<span style="color:#89dceb">str</span>, enable_thinking<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">None</span>) <span style="color:#89dceb;font-weight:bold">-&gt;</span> BatchEncoding:
</span></span><span style="display:flex;"><span>        <span style="color:#cba6f7">if</span> enable_thinking <span style="color:#89dceb;font-weight:bold">is</span> <span style="color:#fab387">None</span>:
</span></span><span style="display:flex;"><span>          enable_thinking <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb">self</span><span style="color:#89dceb;font-weight:bold">.</span>enable_thinking  <span style="color:#6c7086;font-style:italic"># default value</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        messages <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb">self</span><span style="color:#89dceb;font-weight:bold">.</span>history <span style="color:#89dceb;font-weight:bold">+</span> [{<span style="color:#a6e3a1">&#34;role&#34;</span>: <span style="color:#a6e3a1">&#34;user&#34;</span>, <span style="color:#a6e3a1">&#34;content&#34;</span>: user_input}]
</span></span><span style="display:flex;"><span>        tokens <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb">self</span><span style="color:#89dceb;font-weight:bold">.</span>tokenizer<span style="color:#89dceb;font-weight:bold">.</span>apply_chat_template(
</span></span><span style="display:flex;"><span>            messages,
</span></span><span style="display:flex;"><span>            tokenize<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">True</span>,
</span></span><span style="display:flex;"><span>            enable_thinking<span style="color:#89dceb;font-weight:bold">=</span>enable_thinking, 
</span></span><span style="display:flex;"><span>            add_generation_prompt<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">True</span>,
</span></span><span style="display:flex;"><span>            return_tensors<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#a6e3a1">&#34;pt&#34;</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        <span style="color:#cba6f7">return</span> tokens
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#cba6f7">def</span> <span style="color:#89b4fa">__call__</span>(<span style="color:#89dceb">self</span>, user_input, enable_thinking<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">None</span>) <span style="color:#89dceb;font-weight:bold">-&gt;</span> <span style="color:#fab387">None</span>:
</span></span><span style="display:flex;"><span>        inputs <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb">self</span><span style="color:#89dceb;font-weight:bold">.</span>tokenise(user_input, enable_thinking<span style="color:#89dceb;font-weight:bold">=</span>enable_thinking)
</span></span><span style="display:flex;"><span>        response_ids <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb">self</span><span style="color:#89dceb;font-weight:bold">.</span>model<span style="color:#89dceb;font-weight:bold">.</span>generate(
</span></span><span style="display:flex;"><span>            <span style="color:#89dceb;font-weight:bold">**</span>inputs,
</span></span><span style="display:flex;"><span>            max_new_tokens<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>,
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>        response_ids <span style="color:#89dceb;font-weight:bold">=</span> response_ids[<span style="color:#fab387">0</span>][<span style="color:#89dceb">len</span>(inputs<span style="color:#89dceb;font-weight:bold">.</span>input_ids[<span style="color:#fab387">0</span>]):]<span style="color:#89dceb;font-weight:bold">.</span>tolist()
</span></span><span style="display:flex;"><span>        response <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb">self</span><span style="color:#89dceb;font-weight:bold">.</span>tokenizer<span style="color:#89dceb;font-weight:bold">.</span>decode(response_ids, skip_special_tokens<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">True</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#6c7086;font-style:italic"># since we aren&#39;t going to do anything with the output, </span>
</span></span><span style="display:flex;"><span>        <span style="color:#6c7086;font-style:italic"># just prints out the response and save it to the chat history.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#89dceb">print</span>(response)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#6c7086;font-style:italic"># Update history</span>
</span></span><span style="display:flex;"><span>        response_history <span style="color:#89dceb;font-weight:bold">=</span> re<span style="color:#89dceb;font-weight:bold">.</span>sub(<span style="color:#89dceb">self</span><span style="color:#89dceb;font-weight:bold">.</span>_thiniking_regex, <span style="color:#a6e3a1">&#34;&#34;</span>, response, flags<span style="color:#89dceb;font-weight:bold">=</span>re<span style="color:#89dceb;font-weight:bold">.</span>DOTALL)<span style="color:#89dceb;font-weight:bold">.</span>strip()
</span></span><span style="display:flex;"><span>        <span style="color:#89dceb">self</span><span style="color:#89dceb;font-weight:bold">.</span>history<span style="color:#89dceb;font-weight:bold">.</span>append({<span style="color:#a6e3a1">&#34;role&#34;</span>: <span style="color:#a6e3a1">&#34;user&#34;</span>, <span style="color:#a6e3a1">&#34;content&#34;</span>: user_input})
</span></span><span style="display:flex;"><span>        <span style="color:#89dceb">self</span><span style="color:#89dceb;font-weight:bold">.</span>history<span style="color:#89dceb;font-weight:bold">.</span>append({<span style="color:#a6e3a1">&#34;role&#34;</span>: <span style="color:#a6e3a1">&#34;assistant&#34;</span>, <span style="color:#a6e3a1">&#34;content&#34;</span>: response_history})
</span></span><span style="display:flex;"><span>        
</span></span></code></pre></div><p>Then, create a new chat instance with the code below:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>chat <span style="color:#89dceb;font-weight:bold">=</span> DemoChatbot()
</span></span></code></pre></div><p>You will see a progress bar showing download status. Once completed, type the following to have a look at the QWen3 model structure:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pprint(chat<span style="color:#89dceb;font-weight:bold">.</span>model)
</span></span></code></pre></div><p>…which in term will give you this output:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Qwen3ForCausalLM(
</span></span><span style="display:flex;"><span>  (model): Qwen3Model(
</span></span><span style="display:flex;"><span>    (embed_tokens): Embedding(<span style="color:#fab387">151936</span>, <span style="color:#fab387">1024</span>)
</span></span><span style="display:flex;"><span>    (layers): ModuleList(
</span></span><span style="display:flex;"><span>      (<span style="color:#fab387">0</span><span style="color:#89dceb;font-weight:bold">-</span><span style="color:#fab387">27</span>): <span style="color:#fab387">28</span> x Qwen3DecoderLayer(
</span></span><span style="display:flex;"><span>        (self_attn): Qwen3Attention(
</span></span><span style="display:flex;"><span>          (q_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          (k_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          (v_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          (o_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          (q_norm): Qwen3RMSNorm((<span style="color:#fab387">128</span>,), eps<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1e-06</span>)
</span></span><span style="display:flex;"><span>          (k_norm): Qwen3RMSNorm((<span style="color:#fab387">128</span>,), eps<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1e-06</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (mlp): Qwen3MLP(
</span></span><span style="display:flex;"><span>          (gate_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">3072</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          (up_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">3072</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          (down_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">3072</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>          (act_fn): SiLUActivation()
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (input_layernorm): Qwen3RMSNorm((<span style="color:#fab387">1024</span>,), eps<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1e-06</span>)
</span></span><span style="display:flex;"><span>        (post_attention_layernorm): Qwen3RMSNorm((<span style="color:#fab387">1024</span>,), eps<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1e-06</span>)
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    (norm): Qwen3RMSNorm((<span style="color:#fab387">1024</span>,), eps<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1e-06</span>)
</span></span><span style="display:flex;"><span>    (rotary_emb): Qwen3RotaryEmbedding()
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (lm_head): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">151936</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div>
<h3 class="relative group">Model Architecture
    <div id="model-architecture" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#model-architecture" aria-label="Anchor">#</a>
    </span>
    
</h3>
<p>The very core of a transformer model can be checked by:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pprint(chat<span style="color:#89dceb;font-weight:bold">.</span>model<span style="color:#89dceb;font-weight:bold">.</span>model)
</span></span></code></pre></div><p>And you will get an output similar to the previous output, but without the <code>embed_tokens</code> at the beginning and the <code>lm_head</code> at the end:<br></p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Qwen3Model(
</span></span><span style="display:flex;"><span>  (embed_tokens): Embedding(<span style="color:#fab387">151936</span>, <span style="color:#fab387">1024</span>)
</span></span><span style="display:flex;"><span>  (layers): ModuleList(
</span></span><span style="display:flex;"><span>    (<span style="color:#fab387">0</span><span style="color:#89dceb;font-weight:bold">-</span><span style="color:#fab387">27</span>): <span style="color:#fab387">28</span> x Qwen3DecoderLayer(
</span></span><span style="display:flex;"><span>      (self_attn): Qwen3Attention(
</span></span><span style="display:flex;"><span>        (q_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>        (k_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>        (v_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>        (o_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>        (q_norm): Qwen3RMSNorm((<span style="color:#fab387">128</span>,), eps<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1e-06</span>)
</span></span><span style="display:flex;"><span>        (k_norm): Qwen3RMSNorm((<span style="color:#fab387">128</span>,), eps<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1e-06</span>)
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (mlp): Qwen3MLP(
</span></span><span style="display:flex;"><span>        (gate_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">3072</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>        (up_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">3072</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>        (down_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">3072</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>        (act_fn): SiLUActivation()
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (input_layernorm): Qwen3RMSNorm((<span style="color:#fab387">1024</span>,), eps<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1e-06</span>)
</span></span><span style="display:flex;"><span>      (post_attention_layernorm): Qwen3RMSNorm((<span style="color:#fab387">1024</span>,), eps<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1e-06</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (norm): Qwen3RMSNorm((<span style="color:#fab387">1024</span>,), eps<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1e-06</span>)
</span></span><span style="display:flex;"><span>  (rotary_emb): Qwen3RotaryEmbedding()
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>The <code>embed_tokens</code> is the <a
  href="#overview"><strong>Stage 1</strong></a> mentioned earlier, and the <code>lm_head</code> is the <a
  href="#overview"><strong>Stage 3</strong></a>. The core of a transformer, is what we&rsquo;ve been discussing today.<br></p>
<p>Let&rsquo;s have a look at one of the layers inside the transformer:<br></p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>layer <span style="color:#89dceb;font-weight:bold">=</span> chat<span style="color:#89dceb;font-weight:bold">.</span>model<span style="color:#89dceb;font-weight:bold">.</span>model<span style="color:#89dceb;font-weight:bold">.</span>layers[<span style="color:#fab387">0</span>]
</span></span><span style="display:flex;"><span>pprint(layer)
</span></span></code></pre></div><p>You will see something like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Qwen3DecoderLayer(
</span></span><span style="display:flex;"><span>  (self_attn): Qwen3Attention(
</span></span><span style="display:flex;"><span>    (q_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>    (k_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>    (v_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>    (o_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>    (q_norm): Qwen3RMSNorm((<span style="color:#fab387">128</span>,), eps<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1e-06</span>)
</span></span><span style="display:flex;"><span>    (k_norm): Qwen3RMSNorm((<span style="color:#fab387">128</span>,), eps<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1e-06</span>)
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (mlp): Qwen3MLP(
</span></span><span style="display:flex;"><span>    (gate_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">3072</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>    (up_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">3072</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>    (down_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">3072</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>    (act_fn): SiLUActivation()
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (input_layernorm): Qwen3RMSNorm((<span style="color:#fab387">1024</span>,), eps<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1e-06</span>)
</span></span><span style="display:flex;"><span>  (post_attention_layernorm): Qwen3RMSNorm((<span style="color:#fab387">1024</span>,), eps<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1e-06</span>)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>Layer is divided into 2 major sections, Just like like we <a
  href="#the-transformer-itself">discussed previously</a>:</p>
<ul>
<li><code>self_attn</code> is where attention gets calculatd.</li>
<li>All the rest (i.e., <code>mlp</code>, <code>input_layernorm</code> and <code>post_attention_layernorm</code>) are step-by-step computations of combining and averaging the attention heads.</li>
</ul>
<p>Run the following code to see the structure of an attention head:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>attention <span style="color:#89dceb;font-weight:bold">=</span> layer<span style="color:#89dceb;font-weight:bold">.</span>self_attn
</span></span><span style="display:flex;"><span>pprint(attention)
</span></span></code></pre></div><p>The <code>q_proj</code>, <code>k_proj</code> and <code>v_proj</code> are the <a
  href="#the-attention-head">three matrices</a> mentioned earlier:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Qwen3Attention(
</span></span><span style="display:flex;"><span>  (q_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>  (k_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>  (v_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>  (o_proj): Linear(in_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">2048</span>, out_features<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1024</span>, bias<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span>  (q_norm): Qwen3RMSNorm((<span style="color:#fab387">128</span>,), eps<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1e-06</span>)
</span></span><span style="display:flex;"><span>  (k_norm): Qwen3RMSNorm((<span style="color:#fab387">128</span>,), eps<span style="color:#89dceb;font-weight:bold">=</span><span style="color:#fab387">1e-06</span>)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>The actual impelmentations of attention heads are different from model to model, but the general principle behind should be roughly the same. I&rsquo;ll dive depper into it in the future.</p>

<h3 class="relative group">Have some fun!
    <div id="have-some-fun" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#have-some-fun" aria-label="Anchor">#</a>
    </span>
    
</h3>
<p>Meanwhile, since we&rsquo;ve got a generative model already, we might as well test out some text generations:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>chat(<span style="color:#a6e3a1">&#34;Why is the content, which was held to be true in perceiving, in fact only belongs to the form, and it dissolves into the form’s unity 🥺🥺🥺🥺🥺??&#34;</span>)
</span></span></code></pre></div><p>Or, you can turn on the &rsquo;thinking&rsquo; mode to enable chain-of-thought:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>chat<span style="color:#89dceb;font-weight:bold">.</span>enable_thinking <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#fab387">True</span>
</span></span><span style="display:flex;"><span>chat(<span style="color:#a6e3a1">&#34;But...but the phenomenology Φ147 says the inner, essential is essentially the truth of appearance😠😠😠 I&#39;m absolutely fewming&#34;</span>)
</span></span></code></pre></div><p>If it takes too long to run, you can clear chat history to remove cached chats:</p>
<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>chat<span style="color:#89dceb;font-weight:bold">.</span>clear_history()
</span></span></code></pre></div><figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="llm is magic text"
    src="./featured.jpg"
    ><figcaption>&lsquo;LLM is magic&rsquo;</figcaption></figure>
<p>I wanted to point out the (maybe) obvious thing here: almost all operations mentioned contain learnable parameters. Inside individual attention heads, the three matrices convreted by inputs are typically converted by multiplying (&lsquo;dot product&rsquo;) inputs with three <strong>separate</strong> matrices. These matrices are part of the learnable parameters for the attention head. When we combine matrices, the combination operation also has <a
  href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"
    target="_blank"
  >their own learnable parameters</a>. Furthermore, when we combining outputs from each &lsquo;attention heads&rsquo;, this combining opearation also has its own set of trainable parameters, so on and so on&hellip; Almost every stage of the matrix computations are parameterised, resulting the unbelievably <strong>massive</strong> Aİ models as of today. However, the model used in the demo today, despite it&rsquo;s size, is still able to produce long and very coherent responses. Yes, in a sense, transformers are just a huge stack of matirx calculation? Howver, we know very little about the reason behind We gave names to these matrices, we don&rsquo;t know much about their behaviour.<br></p>

<h2 class="relative group">Final thoughts
    <div id="final-thoughts" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#final-thoughts" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Transformer neural network models aren&rsquo;t as mysterious as one think it would be. It has no difference compared with any other functions. At the end of the day it&rsquo;s just another mathematical function, but takes <em>matrix</em> as inputs, does <em>matrix</em> calculations, and outputs  <em>matrices</em>. Just like any other neural network models, it takes multiple steps to do the calculation. Within each computational step, the inputs gets processed by three mini-neural networks, and outputs are re-combined together as the output of the current step. In this sense, transformers are like nested neural networks: they are bigger neural network that contains lots of mini neural networks.<br></p>
<p><strong>What&rsquo;s up next?</strong><br>
There are still a bit more stuffs that I&rsquo;d like to share, like how text-generation works and what&rsquo;s really happenning when we are training a generative model. We&rsquo;ve heard of the same old things over and over: <em>&lsquo;generative LLM is just a very massive auto-complete!&rsquo;</em>. Whilst I do aggree with it, I also find it not helpful if one wants to <em>understand</em> text generation, for both model training and model inference. In the next post, we&rsquo;ll have a look at the <strong>Stage 3</strong> for text generation.</p>

<h2 class="relative group">Some Other Resources
    <div id="some-other-resources" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#some-other-resources" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>I hope whoever come accross this post would find it useful. Here are some extra reading materials that İ found particularly useful:<br></p>
<ol>
<li><a
  href="https://docs.pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html"
    target="_blank"
  >Pytorch&rsquo;s step-byp-step guide</a> on creating a generative LLM is prob one of the best out there that teaches you all the fundenmentals.</li>
<li><a
  href="https://github.com/jessevig/bertviz"
    target="_blank"
  >BertViz, a very good visualisation tool</a> for looking at attention heads layer by layer. You can run it interactively in a jupyter notebook.</li>
<li><a
  href="https://huggingface.co/learn/llm-course/en/chapter0/1"
    target="_blank"
  >Huggingface&rsquo;s LLM cources.</a> Although they tends to focus on the side of programming &amp; practical applications, I found many of their conceptual guides very good for a beginner.</li>
</ol>

          
          
          
        </div>
        
          
  
  
  
  
  
  

  

  

  

  

        
        

        
  
  <section class="flex flex-row flex-wrap justify-center pt-4 text-xl">
    
      
        <a
          target="_blank"
          class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://bsky.app/intent/compose?text=ELI5%20Transformers%20%28Part%201%29:%20Attention%20Mechanism%20&#43;http://localhost:1313/posts/transformers-pt-1/"
          title="Post on Bluesky"
          aria-label="Post on Bluesky">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256,232.562c-21.183,-41.196 -78.868,-117.97 -132.503,-155.834c-51.378,-36.272 -70.978,-29.987 -83.828,-24.181c-14.872,6.72 -17.577,29.554 -17.577,42.988c0,13.433 7.365,110.138 12.169,126.281c15.873,53.336 72.376,71.358 124.413,65.574c2.66,-0.395 5.357,-0.759 8.089,-1.097c-2.68,0.429 -5.379,0.796 -8.089,1.097c-76.259,11.294 -143.984,39.085 -55.158,137.972c97.708,101.165 133.908,-21.692 152.484,-83.983c18.576,62.291 39.972,180.718 150.734,83.983c83.174,-83.983 22.851,-126.674 -53.408,-137.969c-2.71,-0.302 -5.409,-0.667 -8.089,-1.096c2.732,0.337 5.429,0.702 8.089,1.096c52.037,5.785 108.54,-12.239 124.413,-65.574c4.804,-16.142 12.169,-112.847 12.169,-126.281c-0,-13.434 -2.705,-36.267 -17.577,-42.988c-12.85,-5.806 -32.45,-12.09 -83.829,24.181c-53.634,37.864 -111.319,114.635 -132.502,155.831Z"/></svg></span>
        </a>
      
    
      
        <a
          target="_blank"
          class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:1313/posts/transformers-pt-1/&amp;title=ELI5%20Transformers%20%28Part%201%29:%20Attention%20Mechanism%20"
          title="Share on LinkedIn"
          aria-label="Share on LinkedIn">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
</span>
        </a>
      
    
      
        <a
          target="_blank"
          class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://s2f.kytta.dev/?text=ELI5%20Transformers%20%28Part%201%29:%20Attention%20Mechanism%20%20http://localhost:1313/posts/transformers-pt-1/"
          title="Share via Mastodon"
          aria-label="Share via Mastodon">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.54 102.54 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5zm-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg>
</span>
        </a>
      
    
  </section>


        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_posts/transformers-pt-1/index.md"
          data-oid-likes="likes_posts/transformers-pt-1/index.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/posts/transformers-pt-2/">
              <span class="leading-6">
                ELI5 Transformers (Part 2) - Generation&ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        


  






<div
  id="scroll-to-top"
  class="fixed bottom-6 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
          
          
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 ">
        <ul class="flex list-none flex-col sm:flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 ">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href="/tags/"
                title="Tags">
                
                Tags
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          @Yiheng Yu
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
